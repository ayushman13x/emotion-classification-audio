{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feacf179-b451-4e18-a481-31088c87380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\user\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\user\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\user\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0b4)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile numpy pandas matplotlib scikit-learn tensorflow streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fb3fd6-a636-489f-bfa9-5319a70ae4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ audio_speech_actors_01-24 exists with 24 actor folders.\n",
      "Sample actor folders: ['Actor_01', 'Actor_02', 'Actor_03']\n",
      "✅ audio_song_actors_01-24 exists with 24 actor folders.\n",
      "Sample actor folders: ['Actor_01', 'Actor_02', 'Actor_03']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to your folders (change if needed)\n",
    "speech_path = 'audio_speech_actors_01-24'\n",
    "song_path = 'audio_song_actors_01-24'\n",
    "\n",
    "# Function to check extracted folders\n",
    "def check_actor_folders(path):\n",
    "    if os.path.exists(path):\n",
    "        actor_dirs = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "        print(f\"✅ {path} exists with {len(actor_dirs)} actor folders.\")\n",
    "        print(f\"Sample actor folders: {actor_dirs[:3]}\")\n",
    "    else:\n",
    "        print(f\"❌ {path} not found. Please check.\")\n",
    "\n",
    "# Check both\n",
    "check_actor_folders(speech_path)\n",
    "check_actor_folders(song_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298662c5-08d6-4e52-868b-2a8e11df3cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ audio_speech_actors_01-24 exists with 24 actor folders.\n",
      "Sample actor folders: ['Actor_01', 'Actor_02', 'Actor_03']\n"
     ]
    }
   ],
   "source": [
    "check_actor_folders('audio_speech_actors_01-24')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1599f7b-e8b9-4ecc-9c9a-71bbe1096e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total files loaded: 2452\n",
      "🎭 Emotions found: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>emotion</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-02-01...</td>\n",
       "      <td>calm</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  emotion  source\n",
       "0  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "1  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "2  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "3  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "4  audio_speech_actors_01-24\\Actor_01\\03-01-02-01...     calm  speech"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Root paths\n",
    "speech_path = 'audio_speech_actors_01-24'\n",
    "song_path = 'audio_song_actors_01-24'\n",
    "\n",
    "# Emotion code to label\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Initialize empty list\n",
    "data = []\n",
    "\n",
    "# Loop for both datasets\n",
    "for source_type, path in [('speech', speech_path), ('song', song_path)]:\n",
    "    for actor in os.listdir(path):\n",
    "        actor_folder = os.path.join(path, actor)\n",
    "        if os.path.isdir(actor_folder):\n",
    "            for file in os.listdir(actor_folder):\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(actor_folder, file)\n",
    "                    parts = file.split('-')\n",
    "                    if len(parts) >= 3:\n",
    "                        emotion_code = parts[2]\n",
    "                        emotion = emotion_map.get(emotion_code, 'unknown')\n",
    "                        data.append({\n",
    "                            'file_path': file_path,\n",
    "                            'emotion': emotion,\n",
    "                            'source': source_type\n",
    "                        })\n",
    "\n",
    "# Build DataFrame\n",
    "full_data = pd.DataFrame(data)\n",
    "\n",
    "# Show result\n",
    "print(\"✅ Total files loaded:\", len(full_data))\n",
    "print(\"🎭 Emotions found:\", full_data['emotion'].unique())\n",
    "full_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94b55d5-f610-4cdd-8288-ddd0d276597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Sample path: audio_speech_actors_01-24\\Actor_01\\03-01-01-01-01-01-01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded successfully. Length: 72838 Sample Rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Pick a sample file path from your DataFrame\n",
    "sample_path = full_data['file_path'].iloc[0]\n",
    "print(\"🎧 Sample path:\", sample_path)\n",
    "\n",
    "# Try to load it\n",
    "try:\n",
    "    audio, sr = librosa.load(sample_path, res_type='kaiser_fast')\n",
    "    print(\"✅ Loaded successfully. Length:\", len(audio), \"Sample Rate:\", sr)\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to load:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7abbf812-938c-439c-aa8e-6b976b24c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rows in full_data: 2452\n",
      "🎭 Emotions found: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>emotion</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-01-01...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_speech_actors_01-24\\Actor_01\\03-01-02-01...</td>\n",
       "      <td>calm</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  emotion  source\n",
       "0  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "1  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "2  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "3  audio_speech_actors_01-24\\Actor_01\\03-01-01-01...  neutral  speech\n",
       "4  audio_speech_actors_01-24\\Actor_01\\03-01-02-01...     calm  speech"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Rebuild paths\n",
    "speech_path = 'audio_speech_actors_01-24'\n",
    "song_path = 'audio_song_actors_01-24'\n",
    "\n",
    "# Emotion code to label\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Collect files\n",
    "data = []\n",
    "\n",
    "for source_type, path in [('speech', speech_path), ('song', song_path)]:\n",
    "    for actor in os.listdir(path):\n",
    "        actor_folder = os.path.join(path, actor)\n",
    "        if os.path.isdir(actor_folder):\n",
    "            for file in os.listdir(actor_folder):\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(actor_folder, file)\n",
    "                    parts = file.split('-')\n",
    "                    if len(parts) >= 3:\n",
    "                        emotion_code = parts[2]\n",
    "                        emotion = emotion_map.get(emotion_code, 'unknown')\n",
    "                        data.append({\n",
    "                            'file_path': file_path,\n",
    "                            'emotion': emotion,\n",
    "                            'source': source_type\n",
    "                        })\n",
    "\n",
    "# Create DataFrame\n",
    "full_data = pd.DataFrame(data)\n",
    "\n",
    "# Show results\n",
    "print(\"✅ Rows in full_data:\", len(full_data))\n",
    "print(\"🎭 Emotions found:\", full_data['emotion'].unique())\n",
    "full_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55661827-6533-4404-85c5-e55c9b000aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    audio_speech_actors_01-24/Actor_01/03-01-01-01...\n",
       "1    audio_speech_actors_01-24/Actor_01/03-01-01-01...\n",
       "2    audio_speech_actors_01-24/Actor_01/03-01-01-01...\n",
       "3    audio_speech_actors_01-24/Actor_01/03-01-01-01...\n",
       "4    audio_speech_actors_01-24/Actor_01/03-01-02-01...\n",
       "Name: file_path, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix Windows-style paths to work universally\n",
    "full_data['file_path'] = full_data['file_path'].apply(lambda x: x.replace('\\\\', '/'))\n",
    "\n",
    "# Preview first 5 fixed paths\n",
    "full_data['file_path'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb376aa9-0802-45fa-ac1a-866331c85a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resampy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from resampy) (1.26.4)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\user\\anaconda3\\lib\\site-packages (from resampy) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.53->resampy) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install resampy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c30b1f-a641-4d45-8815-1a8764ab6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(file_path, n_mfcc=40):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "\n",
    "        # Extract MFCCs (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Mean of each coefficient across time\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "        return mfccs_processed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73af79f3-75eb-4d8c-9868-261b10e8c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successful samples: 10\n",
      "✅ Feature vector shape (should be 40): (40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-run after installing resampy\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enables progress bar for pandas .apply()\n",
    "\n",
    "test_data = full_data.head(10).copy()\n",
    "test_data['features'] = test_data['file_path'].progress_apply(extract_features)\n",
    "test_data.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "print(\"✅ Successful samples:\", len(test_data))\n",
    "print(\"✅ Feature vector shape (should be 40):\", test_data['features'].iloc[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c3e8b-614e-46d7-a99a-a50ec2524651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2452/2452 [04:41<00:00,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total successful samples: 2452\n",
      "✅ Example feature shape: (40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enables progress bar in .progress_apply()\n",
    "\n",
    "# Apply feature extraction to full dataset\n",
    "full_data['features'] = full_data['file_path'].progress_apply(extract_features)\n",
    "\n",
    "# Drop any rows where extraction failed\n",
    "full_data.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "# Check how many were successful\n",
    "print(\"✅ Total successful samples:\", len(full_data))\n",
    "print(\"✅ Example feature shape:\", full_data['features'].iloc[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7775f3b-68f5-4275-abb9-bc5b42e8fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ X shape (features): (2452, 40)\n",
      "✅ y shape (labels): (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stack features vertically into one big array\n",
    "X = np.vstack(full_data['features'].values)\n",
    "\n",
    "# Get emotion labels\n",
    "y = full_data['emotion'].values\n",
    "\n",
    "print(\"✅ X shape (features):\", X.shape)\n",
    "print(\"✅ y shape (labels):\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c33982-d028-4702-b8be-22d0ca10df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "✅ X_train: (1961, 40)  | y_train: (1961,)\n",
      "✅ X_test: (491, 40)  | y_test: (491,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save class names for reference\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Split the data (80% train, 20% test), stratified by label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"✅ Classes:\", list(class_names))\n",
    "print(\"✅ X_train:\", X_train.shape, \" | y_train:\", y_train.shape)\n",
    "print(\"✅ X_test:\", X_test.shape, \" | y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7258adb-1084-4d67-b976-1234ea8baf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot emotion distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=full_data, x='emotion', order=full_data['emotion'].value_counts().index)\n",
    "plt.title(\"Emotion Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8d1192-89e9-406b-9f78-59791f7b10d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=full_data, x='emotion', hue='source',\n",
    "              order=full_data['emotion'].value_counts().index)\n",
    "plt.title(\"Emotion Distribution by Source (Speech vs Song)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12a8d9ad-debe-4b09-801c-df01de048659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New X shape after adding source: (2452, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'source' column to numeric: 0 for speech, 1 for song\n",
    "source_map = {'speech': 0, 'song': 1}\n",
    "full_data['source_code'] = full_data['source'].map(source_map)\n",
    "\n",
    "# Add this to features\n",
    "X = np.hstack((X, full_data['source_code'].values.reshape(-1, 1)))\n",
    "\n",
    "print(\"✅ New X shape after adding source:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3954ac34-c81f-49bd-8d77-467d4923b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoded emotions: {'angry': 0, 'calm': 1, 'disgust': 2, 'fearful': 3, 'happy': 4, 'neutral': 5, 'sad': 6, 'surprised': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"✅ Encoded emotions:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a3b15e-276a-4870-9ee8-3f35f2e67b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train samples: 1961\n",
      "✅ Test samples: 491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"✅ Train samples:\", X_train.shape[0])\n",
    "print(\"✅ Test samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ea3e52-3ff6-4aa7-be49-3dd803693f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Class weights: {0: 0.8143687707641196, 1: 0.8143687707641196, 2: 1.602124183006536, 3: 0.8143687707641196, 4: 0.8143687707641196, 5: 1.6341666666666668, 6: 0.8143687707641196, 7: 1.602124183006536}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Convert to dict: required by many classifiers\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "print(\"✅ Class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b0bcd03-4275-4f74-8071-ea3b6d7ab0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.79      0.81      0.80        75\n",
      "        calm       0.72      0.84      0.77        75\n",
      "     disgust       0.53      0.59      0.56        39\n",
      "     fearful       0.74      0.68      0.71        75\n",
      "       happy       0.84      0.68      0.75        75\n",
      "     neutral       0.86      0.79      0.82        38\n",
      "         sad       0.68      0.71      0.69        75\n",
      "   surprised       0.62      0.64      0.63        39\n",
      "\n",
      "    accuracy                           0.73       491\n",
      "   macro avg       0.72      0.72      0.72       491\n",
      "weighted avg       0.73      0.73      0.73       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"🎯 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ef0217-ca36-4422-a734-86b743ae8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Accuracy for class '0': 0.81\n",
      "🎯 Accuracy for class '1': 0.84\n",
      "🎯 Accuracy for class '2': 0.59\n",
      "🎯 Accuracy for class '3': 0.68\n",
      "🎯 Accuracy for class '4': 0.68\n",
      "🎯 Accuracy for class '5': 0.79\n",
      "🎯 Accuracy for class '6': 0.71\n",
      "🎯 Accuracy for class '7': 0.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classes = sorted(set(y_test))  # or use `model.classes_` if needed\n",
    "for cls in classes:\n",
    "    idx = np.where(y_test == cls)\n",
    "    acc = accuracy_score(y_test[idx], y_pred[idx])\n",
    "    print(f\"🎯 Accuracy for class '{cls}': {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09050498-0ca2-491d-be55-ad7027a601b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(full_data['emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f577ece-bad9-4595-8496-2154a4de3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(file_path, sr=22050, duration=3, offset=0.5):\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, sample_rate = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
    "\n",
    "        # 1. MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "        # 2. Chroma STFT\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "        # 3. Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "\n",
    "        # 4. Tonnetz\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sample_rate)\n",
    "        tonnetz_mean = np.mean(tonnetz, axis=1)\n",
    "\n",
    "        # 5. ZCR\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "\n",
    "        # 6. RMS\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        rms_mean = np.mean(rms)\n",
    "\n",
    "        # 7. Tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=audio, sr=sample_rate)\n",
    "\n",
    "        # Concatenate all\n",
    "        features = np.hstack([\n",
    "            mfcc_mean,\n",
    "            chroma_mean,\n",
    "            contrast_mean,\n",
    "            tonnetz_mean,\n",
    "            zcr_mean,\n",
    "            rms_mean,\n",
    "            tempo\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting features from {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9da6e1db-292b-4d60-b0f6-8dec34a3e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=978\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=955\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=920\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=989\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1001\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=932\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=943\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=897\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=886\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=851\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=863\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=874\n",
      "  warnings.warn(\n",
      "100%|██████████| 2452/2452 [26:34<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total usable samples: 2452\n",
      "✅ Example feature shape: (41,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply new feature extraction\n",
    "full_data['features'] = full_data['file_path'].progress_apply(extract_features)\n",
    "\n",
    "# Drop any rows where extraction failed\n",
    "full_data.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "print(\"✅ Total usable samples:\", len(full_data))\n",
    "print(\"✅ Example feature shape:\", full_data['features'].iloc[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b331f5fa-5033-4cea-98f8-a927d23d4bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Pitch Stats:\n",
      "Minimum pitch observed: 50.0\n",
      "Maximum pitch observed: 2004.5454545454545\n",
      "Mean pitch range: 50.01 Hz – 2004.55 Hz\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_pitch_ranges(file_paths, sample_size=20):\n",
    "    min_pitches = []\n",
    "    max_pitches = []\n",
    "    \n",
    "    for path in file_paths[:sample_size]:\n",
    "        try:\n",
    "            y, sr = librosa.load(path, sr=None)\n",
    "            pitches = librosa.yin(y, fmin=50, fmax=2000)\n",
    "            pitches = pitches[pitches > 0]\n",
    "            if len(pitches) > 0:\n",
    "                min_pitches.append(np.min(pitches))\n",
    "                max_pitches.append(np.max(pitches))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {path}: {e}\")\n",
    "    \n",
    "    print(\"🔎 Pitch Stats:\")\n",
    "    print(\"Minimum pitch observed:\", np.min(min_pitches))\n",
    "    print(\"Maximum pitch observed:\", np.max(max_pitches))\n",
    "    print(\"Mean pitch range: {:.2f} Hz – {:.2f} Hz\".format(np.mean(min_pitches), np.mean(max_pitches)))\n",
    "\n",
    "    # Plot pitch ranges\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(min_pitches, bins=15, alpha=0.7, label='Min Pitch')\n",
    "    plt.hist(max_pitches, bins=15, alpha=0.7, label='Max Pitch')\n",
    "    plt.xlabel(\"Pitch (Hz)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Pitch Distribution in Sample Audio\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "analyze_pitch_ranges(full_data['file_path'].tolist(), sample_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa447b4-0af3-4f94-b5a9-493accb59f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa.display\n",
    "import random\n",
    "\n",
    "def plot_spectrum_sample(file_paths):\n",
    "    sample_file = random.choice(file_paths)\n",
    "    y, sr = librosa.load(sample_file)\n",
    "    S = np.abs(librosa.stft(y, n_fft=2048))**2\n",
    "    freqs = librosa.fft_frequencies(sr=sr)\n",
    "\n",
    "    mean_spectrum = np.mean(S, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(freqs, mean_spectrum)\n",
    "    plt.title(f\"Mean Spectral Energy - Sample File\\n{sample_file}\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.grid(True)\n",
    "    plt.axvspan(0, 500, color='green', alpha=0.2, label=\"Low Freq (0–500 Hz)\")\n",
    "    plt.axvspan(1000, 2000, color='purple', alpha=0.2, label=\"Nasal Freq (1k–2k Hz)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_spectrum_sample(full_data['file_path'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "054085dc-9166-4357-b404-20d62966a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_combined(file_path, emotion_label=None, sr=22050, duration=3, offset=0.5):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
    "\n",
    "        # ----- BASE FEATURES -----\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "        base_features = np.hstack([\n",
    "            np.mean(mfcc, axis=1),\n",
    "            np.mean(chroma, axis=1),\n",
    "            np.mean(contrast, axis=1),\n",
    "            np.mean(tonnetz, axis=1),\n",
    "            np.mean(zcr),\n",
    "            np.mean(rms),\n",
    "            tempo\n",
    "        ])\n",
    "\n",
    "        # ----- EMOTION-SPECIFIC FEATURES -----\n",
    "        emotion_features = []\n",
    "\n",
    "        # ---- DISGUST ----\n",
    "        if emotion_label == 'disgust':\n",
    "            S = np.abs(librosa.stft(y))\n",
    "            low_freq_energy = np.mean(S[:10, :])  # 0–500 Hz\n",
    "            \n",
    "            f_low, f_high = 1000, 2000\n",
    "            bin_low = int(f_low / (sr / 2) * S.shape[0])\n",
    "            bin_high = int(f_high / (sr / 2) * S.shape[0])\n",
    "            nasal_energy = np.mean(S[bin_low:bin_high, :])\n",
    "\n",
    "            harmonic = librosa.effects.harmonic(y)\n",
    "            hnr = np.mean(harmonic) / (np.mean(np.abs(y)) - np.mean(harmonic) + 1e-6)\n",
    "\n",
    "            emotion_features.extend([low_freq_energy, nasal_energy, hnr])\n",
    "\n",
    "        # ---- SURPRISED ----\n",
    "        elif emotion_label == 'surprised':\n",
    "            pitches = librosa.yin(y, fmin=50, fmax=2000)\n",
    "            valid_pitches = pitches[pitches > 0]\n",
    "            if len(valid_pitches) > 2:\n",
    "                pitch_deltas = np.diff(valid_pitches)\n",
    "                max_pitch_excursion = np.max(pitch_deltas)\n",
    "                pitch_var = np.var(pitch_deltas)\n",
    "            else:\n",
    "                max_pitch_excursion = 0\n",
    "                pitch_var = 0\n",
    "\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            onset_sharpness = np.max(onset_env)\n",
    "\n",
    "            emotion_features.extend([max_pitch_excursion, pitch_var, onset_sharpness])\n",
    "\n",
    "        # ---- NEUTRAL ----\n",
    "        elif emotion_label == 'neutral':\n",
    "            pitches = librosa.yin(y, fmin=50, fmax=2000)\n",
    "            valid_pitches = pitches[pitches > 0]\n",
    "            pitch_entropy = np.std(valid_pitches) / np.mean(valid_pitches) if len(valid_pitches) > 0 else 0\n",
    "\n",
    "            rms = librosa.feature.rms(y=y)\n",
    "            energy_entropy = np.std(rms) / np.mean(rms)\n",
    "\n",
    "            non_silent = librosa.effects.split(y, top_db=25)\n",
    "            if len(non_silent) > 2:\n",
    "                pause_durations = np.diff([x[1] for x in non_silent]) / sr\n",
    "                pause_regularity = np.std(pause_durations)\n",
    "            else:\n",
    "                pause_regularity = 0\n",
    "\n",
    "            emotion_features.extend([pitch_entropy, energy_entropy, pause_regularity])\n",
    "\n",
    "        # FINAL FEATURE VECTOR\n",
    "        full_feature_vector = np.hstack([base_features, emotion_features])\n",
    "        return full_feature_vector\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43e75d69-3f8a-4740-806c-e32a3f2fd161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=978\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=955\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=920\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=989\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1001\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=932\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=943\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=897\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=886\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=851\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=863\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=874\n",
      "  warnings.warn(\n",
      "100%|██████████| 2452/2452 [25:09<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "full_data['features'] = full_data.progress_apply(\n",
    "    lambda row: extract_features_combined(row['file_path'], row['emotion']),\n",
    "    axis=1\n",
    ")\n",
    "full_data.dropna(subset=['features'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3ff33a9-bc97-491f-a8c7-42ba0a016a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rrrr\n"
     ]
    }
   ],
   "source": [
    "print(\"rrrr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b52b6-7f7c-4194-b9f7-b27afc326e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ad421b-8c36-4624-a510-5a7c07b5e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_len\n",
      "41    1880\n",
      "44     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "full_data['feature_len'] = full_data['features'].apply(lambda x: len(x) if x is not None else -1)\n",
    "print(full_data['feature_len'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb04cc04-4d84-4039-93b7-1a0cd2ae94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = full_data['features'].apply(lambda x: len(x)).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db5e3511-7f29-4562-80af-50419eb75e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(feat, length=max_len):\n",
    "    if feat is None:\n",
    "        return np.zeros(length)\n",
    "    if len(feat) < length:\n",
    "        return np.pad(feat, (0, length - len(feat)))\n",
    "    else:\n",
    "        return feat\n",
    "\n",
    "full_data['features'] = full_data['features'].apply(pad_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1af57908-6ccd-43c5-8671-530e01ba1c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training set size: (1961, 44)\n",
      "✅ Test set size: (491, 44)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack(full_data['features'].values)\n",
    "y = full_data['emotion'].values\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Training set size:\", X_train.shape)\n",
    "print(\"✅ Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a37ce-88f5-4072-a64d-7ed556ba7b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fd95a2a-082a-4f56-8eae-270968036b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode the emotion labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Train/test split (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07699bf6-2ce3-431a-8955-ecebc661ac82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_estimators=200,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_estimators=200,\n",
       "                       random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "897c3bed-d9c7-49f0-8765-acf32087ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.74      0.81      0.78        75\n",
      "        calm       0.82      0.87      0.84        75\n",
      "     disgust       1.00      1.00      1.00        39\n",
      "     fearful       0.72      0.57      0.64        75\n",
      "       happy       0.77      0.73      0.75        75\n",
      "     neutral       1.00      1.00      1.00        38\n",
      "         sad       0.61      0.68      0.65        75\n",
      "   surprised       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           0.80       491\n",
      "   macro avg       0.83      0.83      0.83       491\n",
      "weighted avg       0.80      0.80      0.79       491\n",
      "\n",
      "✅ Overall Accuracy: 0.7963340122199593\n",
      "\n",
      "✅ Per-Class Accuracy:\n",
      "     angry: 0.81\n",
      "      calm: 0.87\n",
      "   disgust: 1.00\n",
      "   fearful: 0.57\n",
      "     happy: 0.73\n",
      "   neutral: 1.00\n",
      "       sad: 0.68\n",
      " surprised: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Label names for display\n",
    "emotion_labels = le.inverse_transform(sorted(np.unique(y_test)))\n",
    "\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=emotion_labels))\n",
    "\n",
    "print(\"✅ Overall Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Per-class accuracy\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n✅ Per-Class Accuracy:\")\n",
    "for i, label in enumerate(emotion_labels):\n",
    "    acc = cm[i, i] / cm[i].sum()\n",
    "    print(f\"{label:>10}: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30002d2b-53d7-4cc3-a856-22d2c31516d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a5a5536-9ffe-47f4-b204-900504e04efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1f8739f-d942-46a2-b792-3a25f4f7ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Original Class Distribution: Counter({3: 301, 0: 301, 1: 301, 6: 301, 4: 301, 7: 153, 2: 153, 5: 150})\n"
     ]
    }
   ],
   "source": [
    "# Check current distribution\n",
    "print(\"🔍 Original Class Distribution:\", Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08572607-6281-449b-a395-1d83771dce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique classes in y_train:\", np.unique(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c7a68f7-ed33-4209-9e7b-c1567598f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final class distribution: Counter({0: 301, 1: 301, 6: 301, 4: 301, 3: 301, 5: 301, 2: 301, 7: 153})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set target underperforming classes\n",
    "target_classes = [2, 3, 5]  # happy, sad, fearful\n",
    "\n",
    "# Separate\n",
    "X_minority = X_train[np.isin(y_train, target_classes)]\n",
    "y_minority = y_train[np.isin(y_train, target_classes)]\n",
    "\n",
    "X_majority = X_train[~np.isin(y_train, target_classes)]\n",
    "y_majority = y_train[~np.isin(y_train, target_classes)]\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_minority_res, y_minority_res = smote.fit_resample(X_minority, y_minority)\n",
    "\n",
    "# Combine back\n",
    "X_train_balanced = np.vstack((X_majority, X_minority_res))\n",
    "y_train_balanced = np.concatenate((y_majority, y_minority_res))\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"✅ Final class distribution:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6c51a62-27be-42a1-af4e-b0cc20b4cfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 0.8143687707641196,\n",
       "                                     1: 0.8143687707641196,\n",
       "                                     2: 1.602124183006536,\n",
       "                                     3: 0.8143687707641196,\n",
       "                                     4: 0.8143687707641196,\n",
       "                                     5: 1.6341666666666668,\n",
       "                                     6: 0.8143687707641196,\n",
       "                                     7: 1.602124183006536},\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight={0: 0.8143687707641196,\n",
       "                                     1: 0.8143687707641196,\n",
       "                                     2: 1.602124183006536,\n",
       "                                     3: 0.8143687707641196,\n",
       "                                     4: 0.8143687707641196,\n",
       "                                     5: 1.6341666666666668,\n",
       "                                     6: 0.8143687707641196,\n",
       "                                     7: 1.602124183006536},\n",
       "                       random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 0.8143687707641196,\n",
       "                                     1: 0.8143687707641196,\n",
       "                                     2: 1.602124183006536,\n",
       "                                     3: 0.8143687707641196,\n",
       "                                     4: 0.8143687707641196,\n",
       "                                     5: 1.6341666666666668,\n",
       "                                     6: 0.8143687707641196,\n",
       "                                     7: 1.602124183006536},\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_balanced, y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f895870e-db40-4b1f-b873-4387a5077502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.79      0.77        75\n",
      "        calm       0.82      0.88      0.85        75\n",
      "     disgust       1.00      1.00      1.00        39\n",
      "     fearful       0.74      0.60      0.66        75\n",
      "       happy       0.71      0.76      0.74        75\n",
      "     neutral       1.00      1.00      1.00        38\n",
      "         sad       0.65      0.65      0.65        75\n",
      "   surprised       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           0.80       491\n",
      "   macro avg       0.83      0.83      0.83       491\n",
      "weighted avg       0.80      0.80      0.80       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5683341-5830-4943-bbbd-85962431ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.79      0.77        75\n",
      "        calm       0.82      0.88      0.85        75\n",
      "     disgust       1.00      1.00      1.00        39\n",
      "     fearful       0.74      0.60      0.66        75\n",
      "       happy       0.71      0.76      0.74        75\n",
      "     neutral       1.00      1.00      1.00        38\n",
      "         sad       0.65      0.65      0.65        75\n",
      "   surprised       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           0.80       491\n",
      "   macro avg       0.83      0.83      0.83       491\n",
      "weighted avg       0.80      0.80      0.80       491\n",
      "\n",
      "\n",
      "✅ Per-Class Accuracy:\n",
      "     angry: 0.79\n",
      "      calm: 0.88\n",
      "   disgust: 1.00\n",
      "   fearful: 0.60\n",
      "     happy: 0.76\n",
      "   neutral: 1.00\n",
      "       sad: 0.65\n",
      " surprised: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Standard classification report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# 2. Per-class accuracy\n",
    "import numpy as np\n",
    "\n",
    "unique_labels = np.unique(y_test)\n",
    "print(\"\\n✅ Per-Class Accuracy:\")\n",
    "for label in unique_labels:\n",
    "    label_name = le.inverse_transform([label])[0]\n",
    "    mask = (y_test == label)\n",
    "    acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "    print(f\"{label_name:>10}: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f408bf6a-0572-467c-8058-d3df3b2ca91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17208\\3087857897.py:31: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Feature name list\n",
    "base_feature_names = (\n",
    "    [f\"mfcc_{i}\" for i in range(13)] +\n",
    "    [f\"chroma_{i}\" for i in range(12)] +\n",
    "    [f\"contrast_{i}\" for i in range(7)] +\n",
    "    [f\"tonnetz_{i}\" for i in range(6)] +\n",
    "    ['zcr', 'rms', 'tempo']\n",
    ")\n",
    "\n",
    "# Extra emotion-specific features\n",
    "extra_features = ['low_freq_energy', 'hnr_disgust',  # disgust\n",
    "                  'max_pitch_excursion', 'pitch_var', 'onset_sharpness',  # surprised\n",
    "                  'pitch_entropy', 'energy_entropy', 'pause_regularity']  # neutral\n",
    "\n",
    "feature_names = base_feature_names + extra_features\n",
    "\n",
    "# Plot top N features\n",
    "N = 15\n",
    "top_indices = np.argsort(importances)[-N:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(N), importances[top_indices], align='center')\n",
    "plt.yticks(range(N), [feature_names[i] for i in top_indices])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"🎯 Top Feature Importances in RandomForest\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ca41490-c130-486a-8a8b-78a3534c8671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One feature vector shape: (44,)\n",
      "\n",
      "Unique feature dimensions in your data: [44]\n"
     ]
    }
   ],
   "source": [
    "# Check shape of one feature vector\n",
    "print(\"One feature vector shape:\", full_data['features'].iloc[0].shape)\n",
    "\n",
    "# Sanity check: Are all shapes same?\n",
    "shapes = full_data['features'].apply(lambda x: x.shape[0])\n",
    "print(\"\\nUnique feature dimensions in your data:\", shapes.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3818a5d2-e4d3-4a37-892f-5d4b938d9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Mismatch: X has 44 features, but feature_names has 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature names in the order you added them (adjust according to what you've extracted)\n",
    "feature_names = [\n",
    "    # MFCCs\n",
    "    *['mfcc_' + str(i) for i in range(13)],\n",
    "    # Chroma\n",
    "    *['chroma_' + str(i) for i in range(12)],\n",
    "    # Contrast\n",
    "    *['contrast_' + str(i) for i in range(7)],\n",
    "    # Tonnetz\n",
    "    *['tonnetz_' + str(i) for i in range(6)],\n",
    "    'zcr', 'rms', 'tempo',\n",
    "    # Optional emotion-specific (only if added)\n",
    "    'low_freq_energy', 'hnr_disgust',\n",
    "    'max_pitch_excursion', 'pitch_var', 'onset_sharpness',\n",
    "    'pitch_entropy', 'energy_entropy', 'pause_regularity',\n",
    "    # If added\n",
    "    'source_code'\n",
    "]\n",
    "\n",
    "# Match the length of feature_names to your actual feature vector shape\n",
    "if X.shape[1] != len(feature_names):\n",
    "    print(f\"❌ Mismatch: X has {X.shape[1]} features, but feature_names has {len(feature_names)}\")\n",
    "else:\n",
    "    # Feature importance plot\n",
    "    importances = rf.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_importance_df['feature'], feature_importance_df['importance'])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"🎯 Feature Importance from Random Forest\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0932fbb2-36be-4ee5-b45c-292043f863d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature vector length: 44\n"
     ]
    }
   ],
   "source": [
    "sample_feature = full_data['features'].iloc[0]\n",
    "print(\"Total feature vector length:\", len(sample_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "514bc7d6-9a7a-4ce2-931f-c7eedacafa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample feature vector values:\n",
      " [-6.70195435e+02  6.50638504e+01  8.88954341e-01  1.47159786e+01\n",
      "  9.18216515e+00  6.60574794e-01 -3.84683585e+00 -3.58394599e+00\n",
      " -1.29590063e+01 -3.30013299e+00  9.10779595e-01 -3.59703588e+00\n",
      "  2.37627435e+00  6.25734925e-01  5.94129920e-01  5.20973027e-01\n",
      "  5.15360773e-01  5.28446078e-01  5.05175412e-01  5.47890604e-01]\n",
      "Next values:\n",
      " [ 6.11973763e-01  6.39426827e-01  6.05721235e-01  6.00476801e-01\n",
      "  5.51562428e-01  2.09453247e+01  1.18614319e+01  1.57830801e+01\n",
      "  1.43947067e+01  1.53847683e+01  1.74785937e+01  4.59629738e+01\n",
      " -4.95951333e-02  2.56191153e-02 -4.41293368e-02 -8.04766070e-02\n",
      "  2.37735057e-02  9.14605229e-03  3.50109762e-01  2.66674696e-03]\n",
      "Last few values:\n",
      " [-4.41293368e-02 -8.04766070e-02  2.37735057e-02  9.14605229e-03\n",
      "  3.50109762e-01  2.66674696e-03  8.07495117e+01  1.73697072e+00\n",
      "  1.28190911e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Print first few values to guess the layout\n",
    "print(\"Sample feature vector values:\\n\", sample_feature[:20])  # first 20 values\n",
    "print(\"Next values:\\n\", sample_feature[20:40])  # MFCCs and chroma?\n",
    "print(\"Last few values:\\n\", sample_feature[-10:])  # This will likely show if emotional features were added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "badee871-274c-460c-8a97-f4fa07f7f298",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "X_train.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "217b084b-3761-4bd0-bd55-f34673b47831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddd\n"
     ]
    }
   ],
   "source": [
    "print(\"ddd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b856e15a-ba43-4542-a0ce-a727f1a500d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Feature vector sizes: features\n",
      "44    2452\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming X is a NumPy array and full_data has a 'features' column with arrays\n",
    "sample_shapes = full_data['features'].apply(lambda x: len(x) if x is not None else -1)\n",
    "print(\"📊 Feature vector sizes:\", sample_shapes.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c27d0a2-08c1-41e0-b990-051f7f8c1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_final(file_path, source=None, sr=22050, duration=3, offset=0.5):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
    "        \n",
    "        # --- Base Features ---\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        tonnetz_mean = np.mean(tonnetz, axis=1)\n",
    "\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "        rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "        spec_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        spec_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "        spec_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        spec_flatness = np.mean(librosa.feature.spectral_flatness(y=y))\n",
    "\n",
    "        # --- Duration as feature ---\n",
    "        clip_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        # --- Source code (0 = speech, 1 = song) ---\n",
    "        source_code = 0 if source == 'speech' else 1\n",
    "\n",
    "        # --- Combine all ---\n",
    "        full_features = np.hstack([\n",
    "            mfcc_mean, mfcc_delta_mean, chroma_mean,\n",
    "            contrast_mean, tonnetz_mean,\n",
    "            [zcr, rms, tempo, spec_centroid, spec_bandwidth, spec_rolloff, spec_flatness],\n",
    "            [clip_duration, source_code]\n",
    "        ])\n",
    "\n",
    "        return full_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eaacb29f-1b70-4b70-adeb-d55fe00eb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def extract_all_audio_features(file_path, source=None, sr=22050, duration=3, offset=0.5):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
    "\n",
    "        # --- Base MFCCs and Delta ---\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "\n",
    "        # --- Chroma, Contrast, Tonnetz ---\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        tonnetz_mean = np.mean(tonnetz, axis=1)\n",
    "\n",
    "        # --- Spectral Features ---\n",
    "        spec_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        spec_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "        spec_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        spec_flatness = np.mean(librosa.feature.spectral_flatness(y=y))\n",
    "\n",
    "        # --- Zero Crossing Rate & RMS ---\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "        rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "        # --- Tempo ---\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "        # --- Pitch Features ---\n",
    "        pitches = librosa.yin(y, fmin=50, fmax=2000)\n",
    "        valid_pitches = pitches[pitches > 0]\n",
    "        pitch_mean = np.mean(valid_pitches) if len(valid_pitches) > 0 else 0\n",
    "        pitch_std = np.std(valid_pitches) if len(valid_pitches) > 0 else 0\n",
    "        pitch_entropy = pitch_std / pitch_mean if pitch_mean != 0 else 0\n",
    "        max_pitch_excursion = (np.max(valid_pitches) - np.min(valid_pitches)) if len(valid_pitches) > 0 else 0\n",
    "\n",
    "        # --- Onset Sharpness ---\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        onset_sharpness = np.max(onset_env)\n",
    "\n",
    "        # --- Pause Regularity ---\n",
    "        non_silent = librosa.effects.split(y, top_db=25)\n",
    "        if len(non_silent) > 2:\n",
    "            pauses = np.diff([x[1] for x in non_silent]) / sr\n",
    "            pause_regularity = np.std(pauses)\n",
    "        else:\n",
    "            pause_regularity = 0\n",
    "\n",
    "        # --- Low-Frequency Energy (below 300 Hz) ---\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        freqs = librosa.fft_frequencies(sr=sr)\n",
    "        low_freq_energy = np.sum(stft[freqs < 300]) / np.sum(stft) if np.sum(stft) != 0 else 0\n",
    "\n",
    "        # --- Harmonics-to-Noise Ratio (HNR) approximation ---\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "        hnr = np.mean(y_harmonic ** 2) / (np.mean(y_percussive ** 2) + 1e-6)\n",
    "\n",
    "        # --- Energy Entropy ---\n",
    "        frame_energy = np.sum(librosa.util.frame(y, frame_length=2048, hop_length=512) ** 2, axis=0)\n",
    "        energy_distribution = frame_energy / np.sum(frame_energy) if np.sum(frame_energy) > 0 else np.zeros_like(frame_energy)\n",
    "        energy_entropy = scipy.stats.entropy(energy_distribution)\n",
    "\n",
    "        # --- Source Label ---\n",
    "        source_code = 0 if source == 'speech' else 1\n",
    "\n",
    "        # --- Combine All Features ---\n",
    "        features = np.hstack([\n",
    "            mfcc_mean, mfcc_delta_mean,\n",
    "            chroma_mean, contrast_mean, tonnetz_mean,\n",
    "            [zcr, rms, tempo],\n",
    "            [spec_centroid, spec_bandwidth, spec_rolloff, spec_flatness],\n",
    "            [pitch_std, pitch_mean, pitch_entropy, max_pitch_excursion],\n",
    "            [onset_sharpness, pause_regularity],\n",
    "            [low_freq_energy, hnr, energy_entropy],\n",
    "            [source_code]\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c5bd103-c3d7-4262-a9bf-0a9f866abfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "  0%|          | 2/2452 [00:00<08:34,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=978\n",
      "  warnings.warn(\n",
      "  0%|          | 3/2452 [00:00<10:56,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-01-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=955\n",
      "  warnings.warn(\n",
      "  0%|          | 4/2452 [00:01<12:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-01-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=920\n",
      "  warnings.warn(\n",
      "  0%|          | 5/2452 [00:01<13:01,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-01-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2452 [00:01<13:39,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2452 [00:02<14:12,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2452 [00:02<14:22,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2452 [00:02<14:55,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2452 [00:03<16:57,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/2452 [00:04<18:21,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/2452 [00:04<18:17,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2452 [00:04<17:20,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-02-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/2452 [00:05<16:58,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2452 [00:05<16:02,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2452 [00:05<15:44,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2452 [00:06<15:36,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2452 [00:06<16:16,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2452 [00:07<15:49,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2452 [00:07<16:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2452 [00:07<16:12,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-03-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2452 [00:08<15:49,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/2452 [00:08<15:24,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2452 [00:09<15:10,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=989\n",
      "  warnings.warn(\n",
      "  1%|          | 25/2452 [00:09<14:42,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2452 [00:09<14:47,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/2452 [00:10<14:42,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2452 [00:10<14:28,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/2452 [00:10<14:22,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-04-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/2452 [00:11<15:42,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 31/2452 [00:11<15:52,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 32/2452 [00:12<16:59,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 33/2452 [00:12<17:11,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 34/2452 [00:12<16:25,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 35/2452 [00:13<16:11,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 36/2452 [00:13<16:28,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 37/2452 [00:14<16:20,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-05-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 38/2452 [00:14<16:10,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 39/2452 [00:14<15:53,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "  2%|▏         | 40/2452 [00:15<15:25,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/2452 [00:15<15:08,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 42/2452 [00:16<14:46,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 43/2452 [00:16<14:47,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 44/2452 [00:16<14:26,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 45/2452 [00:17<14:34,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-06-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 46/2452 [00:17<14:10,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 47/2452 [00:17<14:23,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 48/2452 [00:18<14:32,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 49/2452 [00:18<15:32,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 50/2452 [00:19<16:35,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 51/2452 [00:19<16:34,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 52/2452 [00:19<17:12,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 53/2452 [00:20<16:44,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-07-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1001\n",
      "  warnings.warn(\n",
      "  2%|▏         | 54/2452 [00:20<15:33,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-01-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 55/2452 [00:21<14:57,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-01-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 56/2452 [00:21<14:43,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-01-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 57/2452 [00:21<14:29,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-01-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 58/2452 [00:22<14:38,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-02-01-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/2452 [00:22<15:02,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-02-01-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/2452 [00:22<15:35,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-02-02-01-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 61/2452 [00:23<15:11,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_01/03-01-08-02-02-02-01.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 62/2452 [00:23<14:52,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 63/2452 [00:23<14:22,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-01-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 64/2452 [00:24<14:44,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 65/2452 [00:24<14:38,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-01-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 66/2452 [00:25<14:51,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 67/2452 [00:25<15:42,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 68/2452 [00:25<15:30,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 69/2452 [00:26<15:44,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 70/2452 [00:26<15:44,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 71/2452 [00:27<15:49,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 72/2452 [00:27<16:39,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 73/2452 [00:28<17:01,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-02-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 74/2452 [00:28<16:53,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 75/2452 [00:28<16:05,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 76/2452 [00:29<16:08,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 77/2452 [00:29<16:04,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 78/2452 [00:30<16:26,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 79/2452 [00:30<16:47,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/2452 [00:30<16:44,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 81/2452 [00:31<16:48,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-03-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 82/2452 [00:31<16:25,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 83/2452 [00:32<16:42,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 84/2452 [00:32<15:56,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 85/2452 [00:33<15:55,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 86/2452 [00:33<15:52,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 87/2452 [00:33<15:30,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 88/2452 [00:34<15:01,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 89/2452 [00:34<15:03,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-04-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 90/2452 [00:34<16:08,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 91/2452 [00:35<17:46,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 92/2452 [00:36<19:26,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 93/2452 [00:36<19:39,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 94/2452 [00:37<18:03,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 95/2452 [00:37<17:11,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 96/2452 [00:37<16:16,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 97/2452 [00:38<15:39,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-05-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 98/2452 [00:38<15:51,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2452 [00:38<15:34,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2452 [00:39<15:13,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 101/2452 [00:39<14:50,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 102/2452 [00:40<15:38,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 103/2452 [00:40<14:56,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 104/2452 [00:40<16:10,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 105/2452 [00:41<15:49,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-06-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 106/2452 [00:41<15:26,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 107/2452 [00:42<15:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 108/2452 [00:42<14:59,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 109/2452 [00:42<14:52,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 110/2452 [00:43<14:30,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 111/2452 [00:43<14:36,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 112/2452 [00:43<15:18,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 113/2452 [00:44<15:48,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-07-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 114/2452 [00:44<15:45,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 115/2452 [00:45<16:06,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-01-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 116/2452 [00:45<16:44,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-01-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 117/2452 [00:46<17:33,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-01-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 118/2452 [00:46<17:16,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-02-01-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 119/2452 [00:47<18:08,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-02-01-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 120/2452 [00:47<17:53,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-02-02-01-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 121/2452 [00:48<18:16,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_02/03-01-08-02-02-02-02.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 122/2452 [00:48<17:02,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-01-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 123/2452 [00:48<16:59,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-01-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 124/2452 [00:49<17:21,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-01-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 125/2452 [00:49<17:06,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-01-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 126/2452 [00:50<16:49,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 127/2452 [00:50<16:37,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 128/2452 [00:51<18:03,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 129/2452 [00:51<19:10,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 130/2452 [00:52<20:16,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 131/2452 [00:52<19:53,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 132/2452 [00:53<18:08,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 133/2452 [00:53<17:57,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-02-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 134/2452 [00:54<17:25,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 135/2452 [00:54<16:35,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 136/2452 [00:54<17:31,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 137/2452 [00:55<18:15,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 138/2452 [00:55<17:38,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 139/2452 [00:56<17:18,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 140/2452 [00:56<17:06,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 141/2452 [00:57<16:38,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-03-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 142/2452 [00:57<16:49,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 143/2452 [00:57<16:05,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 144/2452 [00:58<16:54,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 145/2452 [00:58<17:47,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 146/2452 [00:59<18:24,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 147/2452 [00:59<18:02,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 148/2452 [01:00<18:31,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 149/2452 [01:00<18:38,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-04-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 150/2452 [01:01<19:08,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 151/2452 [01:01<18:38,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 152/2452 [01:02<18:22,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 153/2452 [01:02<18:35,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 154/2452 [01:03<19:02,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 155/2452 [01:03<19:10,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 156/2452 [01:04<19:09,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 157/2452 [01:04<18:54,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-05-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 158/2452 [01:05<19:01,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 159/2452 [01:05<18:21,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 160/2452 [01:06<18:45,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=932\n",
      "  warnings.warn(\n",
      "  7%|▋         | 161/2452 [01:06<18:30,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 162/2452 [01:07<19:33,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 163/2452 [01:08<20:35,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 164/2452 [01:08<20:55,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 165/2452 [01:09<20:37,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-06-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 166/2452 [01:09<20:19,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 167/2452 [01:10<20:10,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 168/2452 [01:10<18:58,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 169/2452 [01:11<18:41,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 170/2452 [01:11<18:16,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 171/2452 [01:11<18:11,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 172/2452 [01:12<18:13,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 173/2452 [01:12<17:51,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-07-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=943\n",
      "  warnings.warn(\n",
      "  7%|▋         | 174/2452 [01:13<17:53,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-01-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 175/2452 [01:13<17:52,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-01-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 176/2452 [01:14<17:48,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-01-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 177/2452 [01:14<18:07,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-01-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 178/2452 [01:15<18:07,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-02-01-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 179/2452 [01:15<18:46,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-02-01-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 180/2452 [01:16<18:31,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-02-02-01-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 181/2452 [01:16<16:44,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_03/03-01-08-02-02-02-03.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 182/2452 [01:17<16:23,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-01-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 183/2452 [01:17<15:29,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-01-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 184/2452 [01:17<14:41,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-01-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 185/2452 [01:18<14:32,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-01-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 186/2452 [01:18<14:36,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 187/2452 [01:18<14:25,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 188/2452 [01:19<14:27,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 189/2452 [01:19<14:08,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 190/2452 [01:20<15:07,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 191/2452 [01:20<15:19,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 192/2452 [01:20<16:14,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 193/2452 [01:21<16:09,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-02-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 194/2452 [01:21<15:23,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 195/2452 [01:22<15:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 196/2452 [01:22<14:45,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 197/2452 [01:22<15:03,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 198/2452 [01:23<15:16,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 199/2452 [01:23<16:39,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2452 [01:24<17:23,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 201/2452 [01:24<16:45,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-03-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 202/2452 [01:25<15:40,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 203/2452 [01:25<14:53,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 204/2452 [01:25<14:20,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 205/2452 [01:26<14:15,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 206/2452 [01:26<14:16,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 207/2452 [01:26<14:02,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 208/2452 [01:27<14:22,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 209/2452 [01:27<14:17,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-04-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 210/2452 [01:28<14:26,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 211/2452 [01:28<14:07,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 212/2452 [01:28<14:23,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 213/2452 [01:29<14:08,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 214/2452 [01:29<13:52,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 215/2452 [01:30<13:39,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 216/2452 [01:30<14:08,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 217/2452 [01:30<13:52,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-05-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 218/2452 [01:31<13:30,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 219/2452 [01:31<13:15,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 220/2452 [01:31<13:19,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 221/2452 [01:32<13:14,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 222/2452 [01:32<13:18,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 223/2452 [01:32<13:07,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 224/2452 [01:33<14:13,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 225/2452 [01:33<14:05,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-06-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 226/2452 [01:34<13:58,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 227/2452 [01:34<14:10,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 228/2452 [01:34<14:09,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 229/2452 [01:35<14:41,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 230/2452 [01:35<14:29,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 231/2452 [01:36<14:07,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 232/2452 [01:36<13:49,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 233/2452 [01:36<13:43,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-07-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 234/2452 [01:37<13:44,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-01-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 235/2452 [01:37<13:56,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-01-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 236/2452 [01:37<14:31,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-01-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 237/2452 [01:38<14:25,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-01-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 238/2452 [01:38<14:23,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-02-01-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 239/2452 [01:39<16:04,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-02-01-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 240/2452 [01:39<16:32,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-02-02-01-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 241/2452 [01:40<17:19,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_04/03-01-08-02-02-02-04.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 242/2452 [01:40<17:19,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-01-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 243/2452 [01:41<16:11,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-01-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=897\n",
      "  warnings.warn(\n",
      " 10%|▉         | 244/2452 [01:41<15:10,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-01-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 245/2452 [01:41<15:57,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-01-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 246/2452 [01:42<15:28,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 247/2452 [01:42<14:56,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 248/2452 [01:43<14:19,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 249/2452 [01:43<14:08,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 250/2452 [01:43<13:52,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 251/2452 [01:44<14:06,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 252/2452 [01:44<14:23,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 253/2452 [01:44<13:58,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-02-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 254/2452 [01:45<14:34,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 255/2452 [01:45<14:55,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 256/2452 [01:46<14:31,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 257/2452 [01:46<14:54,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 258/2452 [01:46<14:45,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 259/2452 [01:47<14:44,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 260/2452 [01:47<14:52,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 261/2452 [01:48<14:45,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-03-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 262/2452 [01:48<14:22,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 263/2452 [01:48<14:09,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 264/2452 [01:49<14:01,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 265/2452 [01:49<14:48,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 266/2452 [01:50<14:38,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 267/2452 [01:50<14:08,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 268/2452 [01:50<14:02,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      " 11%|█         | 269/2452 [01:51<13:40,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-04-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 270/2452 [01:51<13:33,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 271/2452 [01:51<13:17,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 272/2452 [01:52<13:18,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 273/2452 [01:52<13:31,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 274/2452 [01:53<14:02,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 275/2452 [01:53<15:27,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 276/2452 [01:54<16:31,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 277/2452 [01:54<16:09,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-05-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 278/2452 [01:55<17:16,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 279/2452 [01:55<19:15,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 280/2452 [01:56<20:48,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=886\n",
      "  warnings.warn(\n",
      " 11%|█▏        | 281/2452 [01:57<21:39,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 282/2452 [01:57<21:31,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 283/2452 [01:58<19:21,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 284/2452 [01:58<17:42,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 285/2452 [01:58<16:53,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-06-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 286/2452 [01:59<16:57,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 287/2452 [01:59<17:44,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 288/2452 [02:00<17:05,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 289/2452 [02:00<16:36,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 290/2452 [02:01<16:07,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 291/2452 [02:01<16:11,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 292/2452 [02:02<16:45,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 293/2452 [02:02<17:51,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-07-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 294/2452 [02:03<18:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-01-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 295/2452 [02:03<17:28,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-01-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 296/2452 [02:04<16:35,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-01-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 297/2452 [02:04<17:22,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-01-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 298/2452 [02:05<17:57,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-02-01-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 299/2452 [02:05<17:49,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-02-01-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 300/2452 [02:06<18:33,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-02-02-01-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 301/2452 [02:06<18:45,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_05/03-01-08-02-02-02-05.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 302/2452 [02:07<18:40,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-01-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 303/2452 [02:07<17:54,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-01-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 304/2452 [02:08<17:11,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-01-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 305/2452 [02:08<16:20,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-01-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 306/2452 [02:09<16:21,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 307/2452 [02:09<16:49,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 308/2452 [02:10<17:22,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 309/2452 [02:10<18:47,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 310/2452 [02:11<19:52,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 311/2452 [02:11<20:24,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 312/2452 [02:12<21:54,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 313/2452 [02:13<21:42,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-02-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 314/2452 [02:13<20:56,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 315/2452 [02:14<19:38,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 316/2452 [02:14<18:50,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 317/2452 [02:15<18:44,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 318/2452 [02:15<17:50,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 319/2452 [02:16<16:54,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 320/2452 [02:16<16:51,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 321/2452 [02:17<17:53,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-03-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 322/2452 [02:17<18:32,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 323/2452 [02:18<18:06,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 324/2452 [02:18<19:14,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 325/2452 [02:19<19:06,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 326/2452 [02:19<18:14,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 327/2452 [02:20<18:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 328/2452 [02:21<20:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 329/2452 [02:21<21:45,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-04-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 330/2452 [02:22<22:06,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 331/2452 [02:22<21:44,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 332/2452 [02:23<19:44,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 333/2452 [02:23<17:48,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 334/2452 [02:24<16:28,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 335/2452 [02:24<16:14,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 336/2452 [02:25<17:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 337/2452 [02:25<17:01,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-05-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 338/2452 [02:26<17:08,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 339/2452 [02:26<17:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 340/2452 [02:27<17:12,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 341/2452 [02:27<18:41,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 342/2452 [02:28<20:34,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 343/2452 [02:29<22:07,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 344/2452 [02:29<21:04,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 345/2452 [02:30<19:52,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-06-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 346/2452 [02:30<19:27,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 347/2452 [02:31<19:43,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 348/2452 [02:31<18:46,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 349/2452 [02:32<17:28,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 350/2452 [02:32<17:36,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 351/2452 [02:33<17:06,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 352/2452 [02:33<16:55,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 353/2452 [02:34<17:35,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-07-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 354/2452 [02:34<18:47,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-01-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 355/2452 [02:35<18:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-01-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 356/2452 [02:35<17:18,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-01-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 357/2452 [02:36<17:53,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-01-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 358/2452 [02:36<17:13,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-02-01-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 359/2452 [02:37<16:35,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-02-01-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 360/2452 [02:37<16:19,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-02-02-01-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 361/2452 [02:37<15:37,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_06/03-01-08-02-02-02-06.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 362/2452 [02:38<16:06,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-01-01-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 363/2452 [02:38<16:11,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-01-01-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 364/2452 [02:39<15:37,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-01-01-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 365/2452 [02:39<16:19,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-01-01-02-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 366/2452 [02:40<16:04,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-01-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 367/2452 [02:40<16:08,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-01-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 368/2452 [02:41<17:49,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-01-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 369/2452 [02:42<19:48,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-01-02-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 370/2452 [02:42<20:46,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-02-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 371/2452 [02:43<22:39,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-02-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 372/2452 [02:44<23:50,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-02-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 373/2452 [02:45<25:16,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-02-02-02-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 374/2452 [02:45<25:49,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-01-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 375/2452 [02:46<24:19,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-01-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 376/2452 [02:47<22:36,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-01-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 377/2452 [02:47<21:41,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-01-02-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 378/2452 [02:48<20:42,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-02-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 379/2452 [02:48<20:39,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-02-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 380/2452 [02:49<20:36,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-02-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 381/2452 [02:50<21:10,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-03-02-02-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 382/2452 [02:50<21:57,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-04-01-01-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 383/2452 [02:51<21:34,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-04-01-01-02-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 384/2452 [02:52<15:26,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing audio_speech_actors_01-24/Actor_07/03-01-04-01-02-01-07.wav: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: re-extract for full_data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m full_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m full_data\u001b[38;5;241m.\u001b[39mprogress_apply(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: extract_all_audio_features(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m], source\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_code\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: re-extract for full_data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m full_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m full_data\u001b[38;5;241m.\u001b[39mprogress_apply(\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: extract_all_audio_features(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m], source\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_code\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "Cell \u001b[1;32mIn[61], line 22\u001b[0m, in \u001b[0;36mextract_all_audio_features\u001b[1;34m(file_path, source, sr, duration, offset)\u001b[0m\n\u001b[0;32m     19\u001b[0m contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     20\u001b[0m contrast_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(contrast, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m tonnetz \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mtonnetz(y\u001b[38;5;241m=\u001b[39mlibrosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mharmonic(y), sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     23\u001b[0m tonnetz_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(tonnetz, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# --- Spectral Features ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\effects.py:234\u001b[0m, in \u001b[0;36mharmonic\u001b[1;34m(y, kernel_size, power, mask, margin, n_fft, hop_length, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m    224\u001b[0m stft \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mstft(\n\u001b[0;32m    225\u001b[0m     y,\n\u001b[0;32m    226\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39mn_fft,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Remove percussives\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m stft_harm \u001b[38;5;241m=\u001b[39m decompose\u001b[38;5;241m.\u001b[39mhpss(\n\u001b[0;32m    235\u001b[0m     stft, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, power\u001b[38;5;241m=\u001b[39mpower, mask\u001b[38;5;241m=\u001b[39mmask, margin\u001b[38;5;241m=\u001b[39mmargin\n\u001b[0;32m    236\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Invert the STFTs\u001b[39;00m\n\u001b[0;32m    239\u001b[0m y_harm \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mistft(\n\u001b[0;32m    240\u001b[0m     stft_harm,\n\u001b[0;32m    241\u001b[0m     dtype\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m     length\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\decompose.py:390\u001b[0m, in \u001b[0;36mhpss\u001b[1;34m(S, kernel_size, power, mask, margin)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Compute median filters. Pre-allocation here preserves memory layout.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m harm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(S)\n\u001b[1;32m--> 390\u001b[0m harm[:] \u001b[38;5;241m=\u001b[39m median_filter(S, size\u001b[38;5;241m=\u001b[39mharm_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m perc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(S)\n\u001b[0;32m    393\u001b[0m perc[:] \u001b[38;5;241m=\u001b[39m median_filter(S, size\u001b[38;5;241m=\u001b[39mperc_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\ndimage\\_filters.py:1594\u001b[0m, in \u001b[0;36mmedian_filter\u001b[1;34m(input, size, footprint, output, mode, cval, origin, axes)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;129m@_ni_docstrings\u001b[39m\u001b[38;5;241m.\u001b[39mdocfiller\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian_filter\u001b[39m(\u001b[38;5;28minput\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, footprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1549\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m, cval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124;03m    Calculate a multidimensional median filter.\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _rank_filter(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m0\u001b[39m, size, footprint, output, mode, cval,\n\u001b[0;32m   1595\u001b[0m                         origin, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m, axes\u001b[38;5;241m=\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\ndimage\\_filters.py:1495\u001b[0m, in \u001b[0;36m_rank_filter\u001b[1;34m(input, rank, size, footprint, output, mode, cval, origin, operation, axes)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sequence of modes is not supported by non-separable rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1494\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[1;32m-> 1495\u001b[0m _nd_image\u001b[38;5;241m.\u001b[39mrank_filter(\u001b[38;5;28minput\u001b[39m, rank, footprint, output, mode, cval,\n\u001b[0;32m   1496\u001b[0m                       origins)\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temp_needed:\n\u001b[0;32m   1498\u001b[0m     temp[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example: re-extract for full_data\n",
    "full_data['features'] = full_data.progress_apply(\n",
    "    lambda row: extract_all_audio_features(row['file_path'], source=row['source_code']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "266f890a-a8e1-462b-a147-35686ca2b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def extract_all_audio_features(file_path, source=None, sr=22050, duration=3, offset=0.5):\n",
    "    try:\n",
    "        # Load and pad audio if too short\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
    "        if len(y) < 1024:\n",
    "            y = np.pad(y, (0, 1024 - len(y)), mode='constant')\n",
    "\n",
    "        # --- Base MFCCs and Delta ---\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "\n",
    "        # --- Chroma, Contrast, Tonnetz ---\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        tonnetz_mean = np.mean(tonnetz, axis=1)\n",
    "\n",
    "        # --- Spectral Features ---\n",
    "        spec_centroid = float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))\n",
    "        spec_bandwidth = float(np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)))\n",
    "        spec_rolloff = float(np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)))\n",
    "        spec_flatness = float(np.mean(librosa.feature.spectral_flatness(y=y)))\n",
    "\n",
    "        # --- ZCR & RMS ---\n",
    "        zcr = float(np.mean(librosa.feature.zero_crossing_rate(y)))\n",
    "        rms = float(np.mean(librosa.feature.rms(y=y)))\n",
    "\n",
    "        # --- Tempo ---\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        tempo = float(tempo)\n",
    "\n",
    "        # --- Pitch Features ---\n",
    "        pitches = librosa.yin(y, fmin=50, fmax=2000)\n",
    "        valid_pitches = pitches[pitches > 0]\n",
    "        pitch_mean = float(np.mean(valid_pitches)) if len(valid_pitches) > 0 else 0.0\n",
    "        pitch_std = float(np.std(valid_pitches)) if len(valid_pitches) > 0 else 0.0\n",
    "        pitch_entropy = float(pitch_std / pitch_mean) if pitch_mean != 0 else 0.0\n",
    "        max_pitch_excursion = float(np.max(valid_pitches) - np.min(valid_pitches)) if len(valid_pitches) > 0 else 0.0\n",
    "\n",
    "        # --- Onset Sharpness ---\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        onset_sharpness = float(np.max(onset_env)) if len(onset_env) > 0 else 0.0\n",
    "\n",
    "        # --- Pause Regularity ---\n",
    "        non_silent = librosa.effects.split(y, top_db=25)\n",
    "        if len(non_silent) > 2:\n",
    "            pauses = np.diff([x[1] for x in non_silent]) / sr\n",
    "            pause_regularity = float(np.std(pauses))\n",
    "        else:\n",
    "            pause_regularity = 0.0\n",
    "\n",
    "        # --- Low-Frequency Energy (below 300 Hz) ---\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        freqs = librosa.fft_frequencies(sr=sr)\n",
    "        low_freq_energy = float(np.sum(stft[freqs < 300]) / np.sum(stft)) if np.sum(stft) != 0 else 0.0\n",
    "\n",
    "        # --- Harmonics-to-Noise Ratio (HNR) approximation ---\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "        hnr = float(np.mean(y_harmonic ** 2) / (np.mean(y_percussive ** 2) + 1e-6))\n",
    "\n",
    "        # --- Energy Entropy ---\n",
    "        frame_energy = np.sum(librosa.util.frame(y, frame_length=2048, hop_length=512) ** 2, axis=0)\n",
    "        energy_distribution = frame_energy / np.sum(frame_energy) if np.sum(frame_energy) > 0 else np.zeros_like(frame_energy)\n",
    "        energy_entropy = float(scipy.stats.entropy(energy_distribution))\n",
    "\n",
    "        # --- Source Label ---\n",
    "        source_code = float(0 if source == 'speech' else 1)\n",
    "\n",
    "        # --- Combine All Features ---\n",
    "        features = np.hstack([\n",
    "            mfcc_mean, mfcc_delta_mean,\n",
    "            chroma_mean, contrast_mean, tonnetz_mean,\n",
    "            [zcr, rms, tempo],\n",
    "            [spec_centroid, spec_bandwidth, spec_rolloff, spec_flatness],\n",
    "            [pitch_std, pitch_mean, pitch_entropy, max_pitch_excursion],\n",
    "            [onset_sharpness, pause_regularity],\n",
    "            [low_freq_energy, hnr, energy_entropy],\n",
    "            [source_code]\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20a48efd-a2e6-4891-8b31-a7101eebf373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17208\\3950322558.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tempo = float(tempo)\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=978\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=955\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=920\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=989\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1001\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=932\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=943\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=897\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=886\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=851\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=863\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=874\n",
      "  warnings.warn(\n",
      "100%|██████████| 2452/2452 [18:14<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply safe, full-feature extractor\n",
    "full_data['features'] = full_data.progress_apply(\n",
    "    lambda row: extract_all_audio_features(row['file_path'], source=row['source_code']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b887819-208e-45e2-aa5a-d9da4b3283fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Drop rows where feature extraction failed\n",
    "full_data = full_data[full_data['features'].notnull()].copy()\n",
    "\n",
    "# Step 1: Features (X)\n",
    "X = np.vstack(full_data['features'].values)\n",
    "\n",
    "# Step 2: Labels (y)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(full_data['emotion'])  # Or whatever column has the emotion label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5be90bce-0d7f-4f83-99fd-380664b09d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a34d1744-c601-4e64-b05f-dfa1e7c1d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', random_state=42)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f9422f2-c706-4ab8-b05d-1047995f74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.69      0.79      0.74        75\n",
      "        calm       0.77      0.88      0.82        75\n",
      "     disgust       0.48      0.51      0.49        39\n",
      "     fearful       0.68      0.48      0.56        75\n",
      "       happy       0.72      0.68      0.70        75\n",
      "     neutral       0.85      0.76      0.81        38\n",
      "         sad       0.60      0.65      0.63        75\n",
      "   surprised       0.62      0.62      0.62        39\n",
      "\n",
      "    accuracy                           0.68       491\n",
      "   macro avg       0.68      0.67      0.67       491\n",
      "weighted avg       0.68      0.68      0.68       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd94ae05-1103-453a-809b-e680c17aefe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = model.feature_importances_\n",
    "feature_names = [f'feat_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# Plot top 20 features\n",
    "indices = np.argsort(importances)[-20:][::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.barh(range(20), importances[indices], align='center')\n",
    "plt.yticks(range(20), [feature_names[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "757c9673-bb08-4070-b6a6-50d772412bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAkZ9JREFUeJzs3XlcVdX+//H3AeHI7ISACVGKIibOmZqKQ85jqWVdFce8OeQt07iVipmit0GNq1mamNe+lWnJ7ZpeM+WammOgGRqZBCVITiAOyHB+f/Rw/zoCCgrnKL6ej8d+PNhrr73WZx/7437f37XXNlksFosAAAAAAAAAG3KwdwEAAAAAAAC4+xBKAQAAAAAAwOYIpQAAAAAAAGBzhFIAAAAAAACwOUIpAAAAAAAA2ByhFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAmyOUAgAAAAAAgM0RSgEAAAAAAMDmCKUAAIBNmEymEh3btm0r1zpSU1MVGRmpBx98UFWrVlWNGjUUFhamr776qsj+586d09ixY+Xt7S03Nzd17NhRBw4cKNFcYWFhxT7nkSNHyvKxDIsXL1ZMTEy5jH2rwsLC9MADD9i7jJt24sQJzZw5U/Hx8fYuBQCACqGSvQsAAAB3h1WrVlmdf/DBB9q8eXOh9gYNGpRrHevXr9e8efPUv39/DR8+XHl5efrggw/0yCOP6P3339eIESOMvgUFBerVq5cSEhL0wgsvqEaNGlq8eLHCwsK0f/9+BQUF3XC+2rVra+7cuYXaa9WqVabPddXixYtVo0YNhYeHl8v4d7MTJ04oMjJSgYGBatKkib3LAQDgjkcoBQAAbOIvf/mL1fm3336rzZs3F2ovbx07dlRKSopq1KhhtI0bN05NmjTR9OnTrUKpTz/9VDt37tSaNWs0cOBASdLgwYNVr149zZgxQx9++OEN5/Py8rL5M5Y1i8Wiy5cvy8XFxd6l2EVeXp4KCgrsXQYAABUOr+8BAIDbxoULF/T888/L399fZrNZ9evX1+uvvy6LxWLVz2QyacKECVq9erXq16+vypUrq3nz5vrf//53wzkaNmxoFUhJktlsVs+ePfXrr7/q/PnzRvunn34qHx8fPfroo0abt7e3Bg8erPXr1ysnJ+cWn1jKycnRjBkzVLduXZnNZvn7+2vq1KmFxl6xYoU6deqkmjVrymw2KyQkREuWLLHqExgYqMOHDysuLs54TTAsLEySNHPmTJlMpkLzx8TEyGQyKTk52Wqc3r17a9OmTWrRooVcXFy0dOlSSX+8zjh58mTj36hu3bqaN2/eTYc2V/8t16xZo5CQELm4uKh169Y6dOiQJGnp0qWqW7euKleurLCwMKs6pf//SuD+/fvVpk0bubi46L777tM777xTaK6MjAyNGjVKPj4+qly5sho3bqyVK1da9UlOTpbJZNLrr7+uBQsWqE6dOjKbzVq8eLFatmwpSRoxYoTx+159VXL79u0aNGiQAgICjH/Hv/3tb7p06ZLV+OHh4XJ3d9dvv/2m/v37y93dXd7e3poyZYry8/Ot+hYUFGjhwoVq1KiRKleuLG9vb3Xv3l379u2z6vevf/1LzZs3l4uLi6pVq6YnnnhCqampVn2SkpL02GOPydfXV5UrV1bt2rX1xBNPKDMzs2T/UAAAlANWSgEAgNuCxWJR3759tXXrVo0aNUpNmjTRpk2b9MILL+i3337TW2+9ZdU/Li5OH3/8sSZNmmSEBt27d9eePXtuat+i9PR0ubq6ytXV1Wj77rvv1KxZMzk4WP//8R588EG9++67+vHHH9WoUaPrjpufn69Tp05ZtVWuXFnu7u4qKChQ37599c0332js2LFq0KCBDh06pLfeeks//vijPv/8c+OeJUuWqGHDhurbt68qVaqkf//733rmmWdUUFCg8ePHS5IWLFigiRMnyt3dXS+99JIkycfHp9S/hSQdPXpUQ4YM0dNPP60xY8aofv36unjxojp06KDffvtNTz/9tAICArRz505FREQoLS1NCxYsuKm5tm/frtjYWOM55s6dq969e2vq1KlavHixnnnmGZ09e1bz58/XyJEj9fXXX1vdf/bsWfXs2VODBw/WkCFD9Mknn+ivf/2rnJ2dNXLkSEnSpUuXFBYWpp9++kkTJkzQfffdpzVr1ig8PFznzp3Ts88+azXmihUrdPnyZY0dO1Zms1kDBgzQ+fPnNX36dI0dO1bt2rWTJLVp00aStGbNGl28eFF//etfVb16de3Zs0dvv/22fv31V61Zs8Zq7Pz8fHXr1k2tWrXS66+/rq+++kpvvPGG6tSpo7/+9a9Gv1GjRikmJkY9evTQ6NGjlZeXp+3bt+vbb79VixYtJEmvvfaaXnnlFQ0ePFijR4/W77//rrffflvt27fXd999pypVqujKlSvq1q2bcnJyNHHiRPn6+uq3337TF198oXPnzsnLy+um/t0AALhlFgAAADsYP3685c//U+Tzzz+3SLLMnj3bqt/AgQMtJpPJ8tNPPxltkiySLPv27TPafvnlF0vlypUtAwYMKHUtSUlJlsqVK1uGDh1q1e7m5mYZOXJkof7/+c9/LJIsGzduvO64HTp0MGr98zF8+HCLxWKxrFq1yuLg4GDZvn271X3vvPOORZJlx44dRtvFixcLjd+tWzfL/fffb9XWsGFDS4cOHQr1nTFjhqWo/+m3YsUKiyTL8ePHjbZ77723yOd79dVXLW5ubpYff/zRqv3FF1+0ODo6WlJSUor8Ha7q0KGDpWHDhlZtkixms9lq/qVLl1okWXx9fS1ZWVlGe0RERKFar/7Gb7zxhtGWk5NjadKkiaVmzZqWK1euWCwWi2XBggUWSZZ//etfRr8rV65YWrdubXF3dzfmOX78uEWSxdPT05KRkWFV6969ey2SLCtWrCj0bEX9+8ydO9diMpksv/zyi9E2fPhwiyTLrFmzrPo2bdrU0rx5c+P866+/tkiyTJo0qdC4BQUFFovFYklOTrY4OjpaXnvtNavrhw4dslSqVMlo/+677yySLGvWrCk0FgAA9sTrewAA4LawYcMGOTo6atKkSVbtzz//vCwWi7788kur9tatW6t58+bGeUBAgPr166dNmzYVeg3qei5evKhBgwbJxcVFUVFRVtcuXboks9lc6J7KlSsb128kMDBQmzdvtjqmTp0q6Y/VNQ0aNFBwcLBOnTplHJ06dZIkbd261Rjnz/s5ZWZm6tSpU+rQoYN+/vnncnkF67777lO3bt2s2tasWaN27dqpatWqVvV26dJF+fn5JXp9siidO3dWYGCgcd6qVStJ0mOPPSYPD49C7T///LPV/ZUqVdLTTz9tnDs7O+vpp59WRkaG9u/fL+mP/758fX01ZMgQo5+Tk5MmTZqk7OxsxcXFWY352GOPydvbu8TP8Od/nwsXLujUqVNq06aNLBaLvvvuu0L9x40bZ3Xerl07q+dau3atTCaTZsyYUejeq69hrlu3TgUFBRo8eLDVv4evr6+CgoKM/36uroTatGmTLl68WOJnAgCgvPH6HgAAuC388ssvqlWrllUIIf3/r/H98ssvVu1FffmuXr16unjxon7//Xf5+vrecM78/Hw98cQT+uGHH/Tll18W+iKei4tLkftGXb582bh+I25uburSpUuR15KSkpSYmFhs+JGRkWH8vWPHDs2YMUO7du0qFCxkZmaW+StY9913X5H1Hjx4sET1lkZAQIDV+dVn8ff3L7L97NmzVu21atWSm5ubVVu9evUk/bFH1EMPPaRffvlFQUFBhV7FLO6/r6Ke/3pSUlI0ffp0xcbGFqrv2tDw6v5Qf1a1alWr+44dO6ZatWqpWrVqxc6ZlJQki8VS7FcgnZycjGd57rnn9Oabb2r16tVq166d+vbtq7/85S+8ugcAsCtCKQAAcNcaM2aMvvjiC61evdpYnfRnfn5+SktLK9R+te3aEKu0CgoK1KhRI7355ptFXr8ayhw7dkydO3dWcHCw3nzzTfn7+8vZ2VkbNmzQW2+9VaJNxova5FxSsavKigrcCgoK9Mgjjxgrva51NQgqLUdHx1K1W67Z+L48lOZLg/n5+XrkkUd05swZTZs2TcHBwXJzc9Nvv/2m8PDwQv8+xT1XaRUUFMhkMunLL78sckx3d3fj7zfeeEPh4eFav369/vvf/2rSpEmaO3euvv32W9WuXbtM6gEAoLQIpQAAwG3h3nvv1VdffaXz589brZY6cuSIcf3PkpKSCo3x448/ytXVtUSvXb3wwgtasWKFFixYYPVK1581adJE27dvV0FBgdUKm927d8vV1fWmQ5ir6tSpo4SEBHXu3LnY0EiS/v3vfysnJ0exsbFWq4r+/HrfVcWNU7VqVUl/fD2vSpUqRvu1K4RuVG92dnaxK7/s5cSJE7pw4YLVaqkff/xRkozXAu+9914dPHiw0L9lcf99FaW43/bQoUP68ccftXLlSg0bNsxo37x5c6mf5ao6depo06ZNOnPmTLGrperUqSOLxaL77ruvRP8tNmrUSI0aNdLLL7+snTt3qm3btnrnnXc0e/bsm64TAIBbwZ5SAADgttCzZ0/l5+crOjraqv2tt96SyWRSjx49rNp37dqlAwcOGOepqalav369unbtesOVKP/4xz/0+uuv6+9//3uhr6792cCBA3Xy5EmtW7fOaDt16pTWrFmjPn36FLnfVGkMHjxYv/32m957771C1y5duqQLFy5I+v8ra/68QigzM1MrVqwodJ+bm5vOnTtXqL1OnTqSZLXv04ULF7Ry5cpS1btr1y5t2rSp0LVz584pLy+vxGOVpby8PC1dutQ4v3LlipYuXSpvb29j37GePXsqPT1dH3/8sdV9b7/9ttzd3dWhQ4cbznM19Lr29y3q38disWjhwoU3/UyPPfaYLBaLIiMjC127Os+jjz4qR0dHRUZGFlo9ZrFYdPr0aUlSVlZWoX+bRo0aycHBocjXUwEAsBVWSgEAgNtCnz591LFjR7300ktKTk5W48aN9d///lfr16/X5MmTjVDlqgceeEDdunXTpEmTZDabtXjxYkkq8v+I/7PPPvtMU6dOVVBQkBo0aKB//etfVtcfeeQR+fj4SPojlHrooYc0YsQI/fDDD6pRo4YWL16s/Pz8G85TEkOHDtUnn3yicePGaevWrWrbtq3y8/N15MgRffLJJ9q0aZNatGihrl27ytnZWX369NHTTz+t7Oxsvffee6pZs2ah1wubN2+uJUuWaPbs2apbt65q1qypTp06qWvXrgoICNCoUaP0wgsvyNHRUe+//768vb2VkpJSonpfeOEFxcbGqnfv3goPD1fz5s114cIFHTp0SJ9++qmSk5NVo0aNW/5dSqtWrVqaN2+ekpOTVa9ePX388ceKj4/Xu+++a+yrNHbsWC1dulTh4eHav3+/AgMD9emnn2rHjh1asGBBob3MilKnTh1VqVJF77zzjjw8POTm5qZWrVopODhYderU0ZQpU/Tbb7/J09NTa9euLbS3VGl07NhRQ4cO1aJFi5SUlKTu3buroKBA27dvV8eOHTVhwgTVqVNHs2fPVkREhJKTk9W/f395eHjo+PHj+uyzzzR27FhNmTJFX3/9tSZMmKBBgwapXr16ysvL06pVq+To6KjHHnvspmsEAOCW2eejfwAA4G43fvx4y7X/U+T8+fOWv/3tb5ZatWpZnJycLEFBQZZ//OMfloKCAqt+kizjx4+3/Otf/7IEBQVZzGazpWnTppatW7fecN4ZM2ZYJBV7XDvGmTNnLKNGjbJUr17d4urqaunQoYNl7969JXrGDh06WBo2bHjdPleuXLHMmzfP0rBhQ4vZbLZUrVrV0rx5c0tkZKQlMzPT6BcbG2sJDQ21VK5c2RIYGGiZN2+e5f3337dIshw/ftzol56ebunVq5fFw8PDIsnSoUMH49r+/fstrVq1sjg7O1sCAgIsb775pmXFihWFxrj33nstvXr1KrLe8+fPWyIiIix169a1ODs7W2rUqGFp06aN5fXXX7dcuXKl1L/H1X/LPzt+/LhFkuUf//iHVfvWrVstkixr1qwpNOa+ffssrVu3tlSuXNly7733WqKjowvNf/LkScuIESMsNWrUsDg7O1saNWpkWbFiRYnmvmr9+vWWkJAQS6VKlSySjPt/+OEHS5cuXSzu7u6WGjVqWMaMGWNJSEiw6mOxWCzDhw+3uLm5FRr36n+Xf5aXl2f5xz/+YQkODrY4OztbvL29LT169LDs37/fqt/atWstDz/8sMXNzc3i5uZmCQ4OtowfP95y9OhRi8Visfz888+WkSNHWurUqWOpXLmypVq1apaOHTtavvrqqyKfEQAAWzFZLDbYKRIAAKAMmUwmjR8/vtCrfrj7hIWF6dSpU/r+++/tXQoAACgl9pQCAAAAAACAzRFKAQAAAAAAwOYIpQAAAAAAAGBz7CkFAAAAAAAAm2OlFAAAAAAAAGyOUAoAAAAAAAA2V8neBeD2U1BQoBMnTsjDw0Mmk8ne5QAAAAAAgDuIxWLR+fPnVatWLTk4FL8eilAKhZw4cUL+/v72LgMAAAAAANzBUlNTVbt27WKvE0qhEA8PD0l//Mfj6elp52oAAAAAAMCdJCsrS/7+/ka+UBxCKRRy9ZU9T09PQikAAAAAAHBTbrQlEBudAwAAAAAAwOYIpQAAAAAAAGBzhFIAAAAAAACwOUIpAAAAAAAA2ByhFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAmyOUAgAAAAAAgM0RSgEAAAAAAMDmCKUAAAAAAABgc4RSAAAAAAAAsDlCKQAAAAAAANgcoRQAAAAAAABsjlAKAAAAAAAANkcoBQAAAAAAAJsjlAIAAAAAAIDNEUoBAAAAAADA5gilAAAAAAAAYHOEUgAAAAAAALA5QikAAAAAAADYHKEUAAAAAAAAbK6SvQvA7euBGZvkYHa1dxkAAAAAANw1kqN62bsEm2GlFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAmyOUsoEdO3aoUaNGcnJyUv/+/e1dDgAAAAAAgN0RStnAc889pyZNmuj48eOKiYkp17kuX76s8ePHq3r16nJ3d9djjz2mkydPluucAAAAAAAApUUoZQPHjh1Tp06dVLt2bVWpUqVc5/rb3/6mf//731qzZo3i4uJ04sQJPfroo+U6JwAAAAAAQGkRSpVSWFiYJk6cqMmTJ6tq1ary8fHRe++9pwsXLmjEiBHy8PBQ3bp19eWXXyo5OVkmk0mnT5/WyJEjZTKZjJVShw8fVu/eveXp6SkPDw+1a9dOx44dM+Z5//331bBhQ5nNZvn5+WnChAk3rC0zM1PLly/Xm2++qU6dOql58+ZasWKFdu7cqW+//ba8fhIAAAAAAIBSI5S6CStXrlSNGjW0Z88eTZw4UX/96181aNAgtWnTRgcOHFDXrl01dOhQeXt7Ky0tTZ6enlqwYIHS0tL0+OOP67ffflP79u1lNpv19ddfa//+/Ro5cqTy8vIkSUuWLNH48eM1duxYHTp0SLGxsapbt+4N69q/f79yc3PVpUsXoy04OFgBAQHatWtXuf0eAAAAAAAApVXJ3gXciRo3bqyXX35ZkhQREaGoqCjVqFFDY8aMkSRNnz5dS5Ys0aFDh/TQQw/JZDLJy8tLvr6+kqR//vOf8vLy0kcffSQnJydJUr169YzxZ8+ereeff17PPvus0dayZcsb1pWeni5nZ+dCrwj6+PgoPT292PtycnKUk5NjnGdlZd1wLgAAAAAAgFvBSqmbEBoaavzt6Oio6tWrq1GjRkabj4+PJCkjI6PI++Pj49WuXTsjkPqzjIwMnThxQp07dy7jqos3d+5ceXl5GYe/v7/N5gYAAAAAAHcnQqmbcG2YZDKZrNpMJpMkqaCgoMj7XVxcih37etduxNfXV1euXNG5c+es2k+ePGms0ipKRESEMjMzjSM1NfWmawAAAAAAACgJQik7CA0N1fbt25Wbm1vomoeHhwIDA7Vly5ZSj9u8eXM5OTlZ3Xv06FGlpKSodevWxd5nNpvl6elpdQAAAAAAAJQnQik7mDBhgrKysvTEE09o3759SkpK0qpVq3T06FFJ0syZM/XGG29o0aJFSkpK0oEDB/T222/fcFwvLy+NGjVKzz33nLZu3ar9+/drxIgRat26tR566KHyfiwAAAAAAIASY6NzO6hevbq+/vprvfDCC+rQoYMcHR3VpEkTtW3bVpI0fPhwXb58WW+99ZamTJmiGjVqaODAgSUa+6233pKDg4Mee+wx5eTkqFu3blq8eHF5Pg4AAAAAAECpmSwWi8XeReD2kpWV9ceG55M/kYPZ1d7lAAAAAABw10iO6mXvEm7Z1VwhMzPzulsE8foeAAAAAAAAbI5Q6g6yevVqubu7F3k0bNjQ3uUBAAAAAACUGHtK3UH69u2rVq1aFXnNycnJxtUAAAAAAADcPPaUQiElffcTAAAAAADgWuwpBQAAAAAAgNsWoRQAAAAAAABsjlAKAAAAAAAANkcoBQAAAAAAAJvj63so1gMzNsnB7GrvMgAAAAAAtyA5qpe9SwCKxEopAAAAAAAA2ByhFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAmyOUAgAAAAAAgM0RSt3Grly5Yu8SAAAAAAAAygWh1G0kLCxMEyZM0OTJk1WjRg2ZzWaZTCZt2rRJTZs2lYuLizp16qSMjAx9+eWXatCggTw9PfXkk0/q4sWLxjiffvqpGjVqJBcXF1WvXl1dunTRhQsX7PhkAAAAAAAA1gilbjMrV66Us7OzduzYoXfeeUeSNHPmTEVHR2vnzp1KTU3V4MGDtWDBAn344Yf6z3/+o//+9796++23JUlpaWkaMmSIRo4cqcTERG3btk2PPvqoLBaLPR8LAAAAAADASiV7FwBrQUFBmj9/vqQ/AiZJmj17ttq2bStJGjVqlCIiInTs2DHdf//9kqSBAwdq69atmjZtmtLS0pSXl6dHH31U9957rySpUaNG150zJydHOTk5xnlWVlaZPxcAAAAAAMCfsVLqNtO8efNCbaGhocbfPj4+cnV1NQKpq20ZGRmSpMaNG6tz585q1KiRBg0apPfee09nz5697pxz586Vl5eXcfj7+5fR0wAAAAAAABSNUOo24+bmVqjNycnJ+NtkMlmdX20rKCiQJDk6Omrz5s368ssvFRISorffflv169fX8ePHi50zIiJCmZmZxpGamlpGTwMAAAAAAFA0QqkKyGQyqW3btoqMjNR3330nZ2dnffbZZ8X2N5vN8vT0tDoAAAAAAADKE3tKVTC7d+/Wli1b1LVrV9WsWVO7d+/W77//rgYNGti7NAAAAAAAAAOhVAXj6emp//3vf1qwYIGysrJ077336o033lCPHj3sXRoAAAAAAIDBZLFYLPYuAreXrKysPzY8n/yJHMyu9i4HAAAAAHALkqN62bsE3GWu5gqZmZnX3SKIPaUAAAAAAABgc4RSAAAAAAAAsDlCKQAAAAAAANgcG52jWN9Hdrvuu58AAAAAAAA3i5VSAAAAAAAAsDlCKQAAAAAAANgcoRQAAAAAAABsjlAKAAAAAAAANsdG5yjWAzM2ycHsau8yAAAAYAfJUb3sXQIAoIJjpRQAAAAAAABsjlAKAAAAAAAANkcoBQAAAAAAAJsjlAIAAAAAAIDN3dWhVHJyskwmk+Lj4+1dCgAAAAAAwF3lrg6lKqItW7aoTZs28vDwkK+vr6ZNm6a8vDx7lwUAAAAAAGCFUKqU8vPzVVBQYO8yipSQkKCePXuqe/fu+u677/Txxx8rNjZWL774or1LAwAAAAAAsHJXhFIFBQWaP3++6tatK7PZrICAAL322mvG9Z9//lkdO3aUq6urGjdurF27dhnXYmJiVKVKFcXGxiokJERms1kpKSk6e/ashg0bpqpVq8rV1VU9evRQUlJSofu++OIL1a9fX66urho4cKAuXryolStXKjAwUFWrVtWkSZOUn59v3Ldq1Sq1aNHCWOn05JNPKiMjo0TP+fHHHys0NFTTp09X3bp11aFDB82fP1///Oc/df78+TL4JQEAAAAAAMrGXRFKRUREKCoqSq+88op++OEHffjhh/Lx8TGuv/TSS5oyZYri4+NVr149DRkyxOqVt4sXL2revHlatmyZDh8+rJo1ayo8PFz79u1TbGysdu3aJYvFop49eyo3N9fqvkWLFumjjz7Sxo0btW3bNg0YMEAbNmzQhg0btGrVKi1dulSffvqpcU9ubq5effVVJSQk6PPPP1dycrLCw8NL9Jw5OTmqXLmyVZuLi4suX76s/fv3X/e+rKwsqwMAAAAAAKA8VbJ3AeXt/PnzWrhwoaKjozV8+HBJUp06dfTwww8rOTlZkjRlyhT16tVLkhQZGamGDRvqp59+UnBwsKQ/gqLFixercePGkqSkpCTFxsZqx44datOmjSRp9erV8vf31+eff65BgwYZ9y1ZskR16tSRJA0cOFCrVq3SyZMn5e7urpCQEHXs2FFbt27V448/LkkaOXKkUfv999+vRYsWqWXLlsrOzpa7u/t1n7Vbt25asGCB/u///k+DBw9Wenq6Zs2aJUlKS0sr9r65c+cqMjKy5D8qAAAAAADALarwK6USExOVk5Ojzp07F9snNDTU+NvPz0+SrF6Zc3Z2tuqTmJioSpUqqVWrVkZb9erVVb9+fSUmJhptrq6uRiAlST4+PgoMDLQKl3x8fKzm2r9/v/r06aOAgAB5eHioQ4cOkqSUlJQbPmvXrl31j3/8Q+PGjZPZbFa9evXUs2dPSZKDQ/H/1BEREcrMzDSO1NTUG84FAAAAAABwKyp8KOXi4nLDPk5OTsbfJpNJkqw2M3dxcTHaS+PP414du6i2q3NduHBB3bp1k6enp1avXq29e/fqs88+kyRduXKlRHM+99xzOnfunFJSUnTq1Cn169dP0h+rropjNpvl6elpdQAAAAAAAJSnCh9KBQUFycXFRVu2bCmzMRs0aKC8vDzt3r3baDt9+rSOHj2qkJCQmx73yJEjOn36tKKiotSuXTsFBweXeJPzPzOZTKpVq5ZcXFz0f//3f/L391ezZs1uui4AAAAAAICyVuH3lKpcubKmTZumqVOnytnZWW3bttXvv/+uw4cPX/eVvusJCgpSv379NGbMGC1dulQeHh568cUXdc899xgrk25GQECAnJ2d9fbbb2vcuHH6/vvv9eqrr5ZqjH/84x/q3r27HBwctG7dOkVFRemTTz6Ro6PjTdcFAAAAAABQ1ir8SilJeuWVV/T8889r+vTpatCggR5//PGbWoH0ZytWrFDz5s3Vu3dvtW7dWhaLRRs2bCj0el5peHt7KyYmRmvWrFFISIiioqL0+uuvl2qML7/8Uu3atVOLFi30n//8R+vXr1f//v1vuiYAAAAAAIDyYLJYLBZ7F4HbS1ZWlry8vOQ/+RM5mF3tXQ4AAADsIDmql71LAADcoa7mCpmZmdfdt/quWCkFAAAAAACA2wuh1B1k3Lhxcnd3L/IYN26cvcsDAAAAAAAosQq/0XlFMmvWLE2ZMqXIa9dbDgcAAAAAAHC7YU8pFFLSdz8BAAAAAACuxZ5SAAAAAAAAuG0RSgEAAAAAAMDmCKUAAAAAAABgc2x0jmI9MGOTHMyu9i4DAACgzCVH9bJ3CQAA3PVYKQUAAAAAAACbI5QCAAAAAACAzRFKAQAAAAAAwOYIpW5TYWFhmjx5sr3LAAAAAAAAKBeEUgAAAAAAALA5Qqm7hMViUV5enr3LAAAAAAAAkEQodVsrKCjQ1KlTVa1aNfn6+mrmzJnGNZPJpGXLlmnAgAFydXVVUFCQYmNjjevbtm2TyWTSl19+qebNm8tsNuubb76xw1MAAAAAAAAURih1G1u5cqXc3Ny0e/duzZ8/X7NmzdLmzZuN65GRkRo8eLAOHjyonj176qmnntKZM2esxnjxxRcVFRWlxMREhYaG2voRAAAAAAAAikQodRsLDQ3VjBkzFBQUpGHDhqlFixbasmWLcT08PFxDhgxR3bp1NWfOHGVnZ2vPnj1WY8yaNUuPPPKI6tSpo2rVqhU5T05OjrKysqwOAAAAAACA8kQodRu7dmWTn5+fMjIyirzu5uYmT09Pq+uS1KJFixvOM3fuXHl5eRmHv7//LVYOAAAAAABwfYRStzEnJyerc5PJpIKCghJfl/4Iq24kIiJCmZmZxpGamnoLVQMAAAAAANxYJXsXAPszm80ym832LgMAAAAAANxFWCkFAAAAAAAAmyOUAgAAAAAAgM3x+t5tatu2bYXaPv/8c+Nvi8VS6Pq5c+eMv8PCworsAwAAAAAAcDtgpRQAAAAAAABsjlAKAAAAAAAANkcoBQAAAAAAAJsjlAIAAAAAAIDNsdE5ivV9ZDd5enrauwwAAAAAAFABsVIKAAAAAAAANkcoBQAAAAAAAJsjlAIAAAAAAIDNsacUivXAjE1yMLvauwwAAHCHSI7qZe8SAADAHYSVUgAAAAAAALA5QikAAAAAAADYHKEUAAAAAAAAbI5QCgAAAAAAADZHKAUAAAAAAACbuytCqbCwME2ePNneZRSprGtbt26dunbtqurVq8tkMik+Pr7MxgYAAAAAACgrd0UodTe5cOGCHn74Yc2bN8/epQAAAAAAABSrwodS4eHhiouL08KFC2UymWQymZScnKy4uDg9+OCDMpvN8vPz04svvqi8vDzjvrCwME2aNElTp05VtWrV5Ovrq5kzZ1qNbTKZtGzZMg0YMECurq4KCgpSbGysVZ/vv/9ePXr0kLu7u3x8fDR06FCdOnXqurWFh4cb538+tm3bdsPnHTp0qKZPn64uXbrc8m8HAAAAAABQXip8KLVw4UK1bt1aY8aMUVpamtLS0uTk5KSePXuqZcuWSkhI0JIlS7R8+XLNnj3b6t6VK1fKzc1Nu3fv1vz58zVr1ixt3rzZqk9kZKQGDx6sgwcPqmfPnnrqqad05swZSdK5c+fUqVMnNW3aVPv27dPGjRt18uRJDR48uNja/P39tXDhQuM8LS1Nzz77rGrWrKng4OBy+Y1ycnKUlZVldQAAAAAAAJSnCh9KeXl5ydnZWa6urvL19ZWvr68WL14sf39/RUdHKzg4WP3791dkZKTeeOMNFRQUGPeGhoZqxowZCgoK0rBhw9SiRQtt2bLFavzw8HANGTJEdevW1Zw5c5Sdna09e/ZIkqKjo9W0aVPNmTNHwcHBatq0qd5//31t3bpVP/74Y5G1OTo6ysvLyzjfuXOnli5dqnXr1snX17dcfqO5c+fKy8vLOPz9/ctlHgAAAAAAgKsqfChVlMTERLVu3Vomk8loa9u2rbKzs/Xrr78abaGhoVb3+fn5KSMjw6rtz33c3Nzk6elp9ElISNDWrVvl7u5uHFdXOx07duyGdX733XcaOnSooqOj1bZt29I/aAlFREQoMzPTOFJTU8ttLgAAAAAAAEmqZO8CbmdOTk5W5yaTyWol1Y36ZGdnq0+fPkVuOu7n53fdudPT09W3b1+NHj1ao0aNupnyS8xsNstsNpfrHAAAAAAAAH92V4RSzs7Oys/PN84bNGigtWvXymKxGKulduzYIQ8PD9WuXbvM5m3WrJnWrl2rwMBAVapU9E99bW2SdPnyZfXr10/BwcF68803y6weAAAAAACA28Vd8fpeYGCgdu/ereTkZJ06dUrPPPOMUlNTNXHiRB05ckTr16/XjBkz9Nxzz8nBoex+kvHjx+vMmTMaMmSI9u7dq2PHjmnTpk0aMWKEEURdW1tBQYGefvpppaamatGiRfr999+Vnp6u9PR0Xbly5YZznjlzRvHx8frhhx8kSUePHlV8fLzS09PL7LkAAAAAAABu1V0RSk2ZMkWOjo4KCQmRt7e3cnNztWHDBu3Zs0eNGzfWuHHjNGrUKL388stlOm+tWrW0Y8cO5efnq2vXrmrUqJEmT56sKlWqGOHXtbWlpKQoLi5OaWlpCgkJkZ+fn3Hs3LnzhnPGxsaqadOm6tWrlyTpiSeeUNOmTfXOO++U6bMBAAAAAADcCpPFYrHYuwjcXrKysv74Ct/kT+RgdrV3OQAA4A6RHNXL3iUAAIDbwNVcITMzU56ensX2uytWSgEAAAAAAOD2Qih1B9m+fbvc3d2LPQAAAAAAAO4Ud8XX9yqKFi1aKD4+3t5lAAAAAAAA3DL2lEIhJX33EwAAAAAA4FrsKQUAAAAAAIDbFqEUAAAAAAAAbI5QCgAAAAAAADbHRuco1gMzNsnB7GrvMgAAQBlLjupl7xIAAABYKQUAAAAAAADbI5QCAAAAAACAzRFKAQAAAAAAwOYIpQAAAAAAAGBzd3UolZycLJPJpPj4eHuXAgAAAAAAcFe5q0OpiiYmJkYmk6nIIyMjw97lAQAAAAAAGCrZu4A7TX5+vkwmkxwcbr887/HHH1f37t2t2sLDw3X58mXVrFnTTlUBAAAAAAAUdvslK+WgoKBA8+fPV926dWU2mxUQEKDXXnvNuP7zzz+rY8eOcnV1VePGjbVr1y7jWkxMjKpUqaLY2FiFhITIbDYrJSVFZ8+e1bBhw1S1alW5urqqR48eSkpKKnTfF198ofr168vV1VUDBw7UxYsXtXLlSgUGBqpq1aqaNGmS8vPzjftWrVqlFi1ayMPDQ76+vnryySdLvMrJxcVFvr6+xuHo6Kivv/5ao0aNKoNfEQAAAAAAoOzcFaFURESEoqKi9Morr+iHH37Qhx9+KB8fH+P6Sy+9pClTpig+Pl716tXTkCFDlJeXZ1y/ePGi5s2bp2XLlunw4cOqWbOmwsPDtW/fPsXGxmrXrl2yWCzq2bOncnNzre5btGiRPvroI23cuFHbtm3TgAEDtGHDBm3YsEGrVq3S0qVL9emnnxr35Obm6tVXX1VCQoI+//xzJScnKzw8/Kae+4MPPjDCsOvJyclRVlaW1QEAAAAAAFCeTBaLxWLvIsrT+fPn5e3trejoaI0ePdrqWnJysu677z4tW7bMWE30ww8/qGHDhkpMTFRwcLBiYmI0YsQIxcfHq3HjxpKkpKQk1atXTzt27FCbNm0kSadPn5a/v79WrlypQYMGGff99NNPqlOnjiRp3LhxWrVqlU6ePCl3d3dJUvfu3RUYGKh33nmnyPr37dunli1b6vz588Y9JRUSEqKwsDAtXrz4uv1mzpypyMjIQu3+kz+Rg9m1VHMCAIDbX3JUL3uXAAAAKrCsrCx5eXkpMzNTnp6exfar8CulEhMTlZOTo86dOxfbJzQ01Pjbz89PkqxemXN2drbqk5iYqEqVKqlVq1ZGW/Xq1VW/fn0lJiYaba6urkYgJUk+Pj4KDAy0Cpd8fHys5tq/f7/69OmjgIAAeXh4qEOHDpKklJSUUj33rl27lJiYWKJX9yIiIpSZmWkcqamppZoLAAAAAACgtCr8RucuLi437OPk5GT8bTKZJP2xD9Wfx7jaXhp/Hvfq2EW1XZ3rwoUL6tatm7p166bVq1fL29tbKSkp6tatm65cuVKquZctW6YmTZqoefPmN+xrNptlNptLNT4AAAAAAMCtqPArpYKCguTi4qItW7aU2ZgNGjRQXl6edu/ebbSdPn1aR48eVUhIyE2Pe+TIEZ0+fVpRUVFq166dgoODS7zJ+Z9lZ2frk08+YYNzAAAAAABw26rwK6UqV66sadOmaerUqXJ2dlbbtm31+++/6/Dhw9d9pe96goKC1K9fP40ZM0ZLly6Vh4eHXnzxRd1zzz3q16/fTdcaEBAgZ2dnvf322xo3bpy+//57vfrqq6Ue5+OPP1ZeXp7+8pe/3HQtAAAAAAAA5anCr5SSpFdeeUXPP/+8pk+frgYNGujxxx+/qRVIf7ZixQo1b95cvXv3VuvWrWWxWLRhw4ZCr+eVhre3t2JiYrRmzRqFhIQoKipKr7/+eqnHWb58uR599FFVqVLlpmsBAAAAAAAoTxX+63sovau75PP1PQAAKia+vgcAAMoTX98DAAAAAADAbYtQ6g4ybtw4ubu7F3mMGzfO3uUBAAAAAACUWIXf6LwimTVrlqZMmVLktesthwMAAAAAALjdsKcUCinpu58AAAAAAADXYk8pAAAAAAAA3LYIpQAAAAAAAGBzhFIAAAAAAACwOTY6R7EemLFJDmZXe5cBAABuUXJUL3uXAAAAUAgrpQAAAAAAAGBzhFIAAAAAAACwOUIpAAAAAAAA2ByhFAAAAAAAAGyOUAoAAAAAAAA2Ryh1EwIDA7VgwYLbcq5t27apX79+8vPzk5ubm5o0aaLVq1eXX4EAAAAAAAA3gVCqnOTn56ugoMDm8+7cuVOhoaFau3atDh48qBEjRmjYsGH64osvbF4LAAAAAABAcSpkKFVQUKD58+erbt26MpvNCggI0GuvvSZJOnTokDp16iQXFxdVr15dY8eOVXZ2tnFveHi4+vfvr9dff11+fn6qXr26xo8fr9zcXElSWFiYfvnlF/3tb3+TyWSSyWSSJMXExKhKlSqKjY1VSEiIzGazUlJStHfvXj3yyCOqUaOGvLy81KFDBx04cMCYz2KxaObMmQoICJDZbFatWrU0adKk6851PX//+9/16quvqk2bNqpTp46effZZde/eXevWrSuz3xcAAAAAAOBWVchQKiIiQlFRUXrllVf0ww8/6MMPP5SPj48uXLigbt26qWrVqtq7d6/WrFmjr776ShMmTLC6f+vWrTp27Ji2bt2qlStXKiYmRjExMZKkdevWqXbt2po1a5bS0tKUlpZm3Hfx4kXNmzdPy5Yt0+HDh1WzZk2dP39ew4cP1zfffKNvv/1WQUFB6tmzp86fPy9JWrt2rd566y0tXbpUSUlJ+vzzz9WoUaMbzlUamZmZqlatWrHXc3JylJWVZXUAAAAAAACUp0r2LqCsnT9/XgsXLlR0dLSGDx8uSapTp44efvhhvffee7p8+bI++OADubm5SZKio6PVp08fzZs3Tz4+PpKkqlWrKjo6Wo6OjgoODlavXr20ZcsWjRkzRtWqVZOjo6M8PDzk6+trNXdubq4WL16sxo0bG22dOnWy6vPuu++qSpUqiouLU+/evZWSkiJfX1916dJFTk5OCggI0IMPPihJ152rpD755BPt3btXS5cuLbbP3LlzFRkZeVPjAwAAAAAA3IwKt1IqMTFROTk56ty5c5HXGjdubARSktS2bVsVFBTo6NGjRlvDhg3l6OhonPv5+SkjI+OGczs7Oys0NNSq7eTJkxozZoyCgoLk5eUlT09PZWdnKyUlRZI0aNAgXbp0Sffff7/GjBmjzz77THl5eaV+7qJs3bpVI0aM0HvvvaeGDRsW2y8iIkKZmZnGkZqaWibzAwAAAAAAFKfChVIuLi63PIaTk5PVuclkKtGm5S4uLoX2fRo+fLji4+O1cOFC7dy5U/Hx8apevbquXLkiSfL399fRo0e1ePFiubi46JlnnlH79u2NPaxuVlxcnPr06aO33npLw4YNu25fs9ksT09PqwMAAAAAAKA8VbhQKigoSC4uLtqyZUuhaw0aNFBCQoIuXLhgtO3YsUMODg6qX79+iedwdnZWfn5+ifru2LFDkyZNUs+ePdWwYUOZzWadOnXKqo+Li4v69OmjRYsWadu2bdq1a5cOHTpU6rmu2rZtm3r16qV58+Zp7NixpboXAAAAAADAFipcKFW5cmVNmzZNU6dO1QcffKBjx47p22+/1fLly/XUU0+pcuXKGj58uL7//ntt3bpVEydO1NChQ439pEoiMDBQ//vf//Tbb78VCpiuFRQUpFWrVikxMVG7d+/WU089ZbWaKyYmRsuXL9f333+vn3/+Wf/617/k4uKie++9t9RzSX+8sterVy9NmjRJjz32mNLT05Wenq4zZ86U+PkAAAAAAADKW4ULpSTplVde0fPPP6/p06erQYMGevzxx5WRkSFXV1dt2rRJZ86cUcuWLTVw4EB17txZ0dHRpRp/1qxZSk5OVp06deTt7X3dvsuXL9fZs2fVrFkzDR06VJMmTVLNmjWN61WqVNF7772ntm3bKjQ0VF999ZX+/e9/q3r16qWeS5JWrlypixcvau7cufLz8zOORx99tFTPCAAAAAAAUJ5MFovFYu8icHvJysqSl5eX/Cd/Igezq73LAQAAtyg5qpe9SwAAAHeRq7lCZmbmdfetrpArpQAAAAAAAHB7I5S6w/To0UPu7u5FHnPmzLF3eQAAAAAAACVSyd4FoHSWLVumS5cuFXmtWrVqNq4GAAAAAADg5rCnFAop6bufAAAAAAAA12JPKQAAAAAAANy2CKUAAAAAAABgc4RSAAAAAAAAsDlCKQAAAAAAANgcX99DsR6YsUkOZld7lwEAAEohOaqXvUsAAAAoEVZKAQAAAAAAwOYIpQAAAAAAAGBzhFIAAAAAAACwOUIpAAAAAAAA2NxdEUqFhYVp8uTJ9i6jSGVZW25urqZNm6ZGjRrJzc1NtWrV0rBhw3TixIkyGR8AAAAAAKCs3BWh1N3i4sWLOnDggF555RUdOHBA69at09GjR9W3b197lwYAAAAAAGClwodS4eHhiouL08KFC2UymWQymZScnKy4uDg9+OCDMpvN8vPz04svvqi8vDzjvrCwME2aNElTp05VtWrV5Ovrq5kzZ1qNbTKZtGzZMg0YMECurq4KCgpSbGysVZ/vv/9ePXr0kLu7u3x8fDR06FCdOnXqurWFh4cb538+tm3bdt1n9fLy0ubNmzV48GDVr19fDz30kKKjo7V//36lpKSUye8JAAAAAABQFip8KLVw4UK1bt1aY8aMUVpamtLS0uTk5KSePXuqZcuWSkhI0JIlS7R8+XLNnj3b6t6VK1fKzc1Nu3fv1vz58zVr1ixt3rzZqk9kZKQGDx6sgwcPqmfPnnrqqad05swZSdK5c+fUqVMnNW3aVPv27dPGjRt18uRJDR48uNja/P39tXDhQuM8LS1Nzz77rGrWrKng4OBSP39mZqZMJpOqVKlSbJ+cnBxlZWVZHQAAAAAAAOWpwodSXl5ecnZ2lqurq3x9feXr66vFixfL399f0dHRCg4OVv/+/RUZGak33nhDBQUFxr2hoaGaMWOGgoKCNGzYMLVo0UJbtmyxGj88PFxDhgxR3bp1NWfOHGVnZ2vPnj2SpOjoaDVt2lRz5sxRcHCwmjZtqvfff19bt27Vjz/+WGRtjo6O8vLyMs537typpUuXat26dfL19S3Vs1++fFnTpk3TkCFD5OnpWWy/uXPnysvLyzj8/f1LNQ8AAAAAAEBpVfhQqiiJiYlq3bq1TCaT0da2bVtlZ2fr119/NdpCQ0Ot7vPz81NGRoZV25/7uLm5ydPT0+iTkJCgrVu3yt3d3TiurnY6duzYDev87rvvNHToUEVHR6tt27alesbc3FwNHjxYFotFS5YsuW7fiIgIZWZmGkdqamqp5gIAAAAAACitSvYu4Hbm5ORkdW4ymaxWUt2oT3Z2tvr06aN58+YVGtvPz++6c6enp6tv374aPXq0Ro0aVaq6rwZSv/zyi77++uvrrpKSJLPZLLPZXKo5AAAAAAAAbsVdEUo5OzsrPz/fOG/QoIHWrl0ri8VirJbasWOHPDw8VLt27TKbt1mzZlq7dq0CAwNVqVLRP/W1tUl/vHbXr18/BQcH68033yzVnFcDqaSkJG3dulXVq1e/6foBAAAAAADKy13x+l5gYKB2796t5ORknTp1Ss8884xSU1M1ceJEHTlyROvXr9eMGTP03HPPycGh7H6S8ePH68yZMxoyZIj27t2rY8eOadOmTRoxYoQRRF1bW0FBgZ5++mmlpqZq0aJF+v3335Wenq709HRduXLluvPl5uZq4MCB2rdvn1avXq38/PwS3wsAAAAAAGBLd0UoNWXKFDk6OiokJETe3t7Kzc3Vhg0btGfPHjVu3Fjjxo3TqFGj9PLLL5fpvLVq1dKOHTuUn5+vrl27qlGjRpo8ebKqVKlihF/X1paSkqK4uDilpaUpJCREfn5+xrFz587rzvfbb78pNjZWv/76q5o0aVKqewEAAAAAAGzJZLFYLPYuAreXrKysP77CN/kTOZhd7V0OAAAoheSoXvYuAQAA3OWu5gqZmZnX3ef6rlgpBQAAAAAAgNsLodQdZPv27XJ3dy/2AAAAAAAAuFPcFV/fqyhatGih+Ph4e5cBAAAAAABwy9hTCoWU9N1PAAAAAACAa7GnFAAAAAAAAG5bhFIAAAAAAACwOUIpAAAAAAAA2ByhFAAAAAAAAGyOr++hWA/M2CQHs6u9ywAAoFwlR/WydwkAAAB3JVZKAQAAAAAAwOYIpQAAAAAAAGBzhFIAAAAAAACwuVKFUmFhYZo8eXI5lVK89PR0PfLII3Jzc1OVKlVsPj8AAAAAAADK1h2x0flbb72ltLQ0xcfHy8vLy97lAAAAAAAA4BbdEaHUsWPH1Lx5cwUFBRXbJzc3V05OTjasynby8/NlMpnk4MDblgAAAAAAoGK46ZTj7NmzGjZsmKpWrSpXV1f16NFDSUlJkiSLxSJvb299+umnRv8mTZrIz8/POP/mm29kNpt18eLF684TGBiotWvX6oMPPpDJZFJ4eLgkyWQyacmSJerbt6/c3Nz02muvSZLWr1+vZs2aqXLlyrr//vsVGRmpvLw8Y7ykpCS1b99elStXVkhIiDZv3iyTyaTPP/+8RM+dmpqqwYMHq0qVKqpWrZr69eun5ORk43p4eLj69++v119/XX5+fqpevbrGjx+v3Nxco09OTo6mTJmie+65R25ubmrVqpW2bdtmXI+JiVGVKlUUGxurkJAQmc1mpaSkKC0tTb169ZKLi4vuu+8+ffjhhwoMDNSCBQskSSNHjlTv3r2t6s3NzVXNmjW1fPnyEj0fAAAAAACALdx0KBUeHq59+/YpNjZWu3btksViUc+ePZWbmyuTyaT27dsbQcvZs2eVmJioS5cu6ciRI5KkuLg4tWzZUq6urtedZ+/everevbsGDx6stLQ0LVy40Lg2c+ZMDRgwQIcOHdLIkSO1fft2DRs2TM8++6x++OEHLV26VDExMUZgVVBQoEcffVTOzs7avXu33nnnHU2bNq3Ez5ybm6tu3brJw8ND27dv144dO+Tu7q7u3bvrypUrRr+tW7fq2LFj2rp1q1auXKmYmBjFxMQY1ydMmKBdu3bpo48+0sGDBzVo0CB1797dCPUk6eLFi5o3b56WLVumw4cPq2bNmho2bJhOnDihbdu2ae3atXr33XeVkZFh3DN69Ght3LhRaWlpRtsXX3yhixcv6vHHHy/2uXJycpSVlWV1AAAAAAAAlKebCqWSkpIUGxurZcuWqV27dmrcuLFWr16t3377zVhxFBYWZoRS//vf/9S0aVOrtm3btqlDhw43nMvb21tms1kuLi7y9fW12lPqySef1IgRI3T//fcrICBAkZGRevHFFzV8+HDdf//9euSRR/Tqq69q6dKlkqSvvvpKR44c0QcffKDGjRurffv2mjNnTomf++OPP1ZBQYGWLVumRo0aqUGDBlqxYoVSUlKsVjpVrVpV0dHRCg4OVu/evdWrVy9t2bJFkpSSkqIVK1ZozZo1ateunerUqaMpU6bo4Ycf1ooVK4wxcnNztXjxYrVp00b169dXSkqKvvrqK7333ntq1aqVmjVrpmXLlunSpUvGPVf7rlq1ymhbsWKFBg0aJHd392Kfa+7cufLy8jIOf3//Ev8mAAAAAAAAN+OmQqnExERVqlRJrVq1MtqqV6+u+vXrKzExUZLUoUMH/fDDD/r9998VFxensLAwI5TKzc3Vzp07FRYWdkvFt2jRwuo8ISFBs2bNkru7u3GMGTNGaWlpunjxohITE+Xv769atWoZ97Ru3brE8yUkJOinn36Sh4eHMX61atV0+fJlHTt2zOjXsGFDOTo6Gud+fn7GiqZDhw4pPz9f9erVs6ozLi7OagxnZ2eFhoYa50ePHlWlSpXUrFkzo61u3bqqWrWqVY2jR482wq2TJ0/qyy+/1MiRI6/7XBEREcrMzDSO1NTUEv8mAAAAAAAAN6PcNjpv1KiRqlWrpri4OMXFxem1116Tr6+v5s2bp7179yo3N1dt2rS5pTnc3NyszrOzsxUZGalHH320UN/KlSvf0lxXx2/evLlWr15d6Jq3t7fx97UbrptMJhUUFBhjODo6av/+/VbBlSSr1UwuLi4ymUylrnHYsGF68cUXtWvXLu3cuVP33Xef2rVrd917zGazzGZzqecCAAAAAAC4WTcVSjVo0EB5eXnavXu3ESydPn1aR48eVUhIiKQ/gph27dpp/fr1Onz4sB5++GG5uroqJydHS5cuVYsWLQqFSreqWbNmOnr0qOrWrVts3ampqUpLSzM2Xf/2229LNf7HH3+smjVrytPT86ZqbNq0qfLz85WRkXHDsOjP6tevr7y8PH333Xdq3ry5JOmnn37S2bNnrfpVr15d/fv314oVK7Rr1y6NGDHipuoEAAAAAAAoTzf1+l5QUJD69eunMWPG6JtvvlFCQoL+8pe/6J577lG/fv2MfmFhYfq///s/NWnSRO7u7nJwcFD79u21evXqEu0nVVrTp0/XBx98oMjISB0+fFiJiYn66KOP9PLLL0uSunTponr16mn48OFKSEjQ9u3b9dJLL5V4/Keeeko1atRQv379tH37dh0/flzbtm3TpEmT9Ouvv5ZojHr16umpp57SsGHDtG7dOh0/flx79uzR3Llz9Z///KfY+4KDg9WlSxeNHTtWe/bs0XfffaexY8cWuaJq9OjRWrlypRITEzV8+PASPx8AAAAAAICt3PTX91asWKHmzZurd+/eat26tSwWizZs2GD16lqHDh2Un59vtXdUWFhYobay0q1bN33xxRf673//q5YtW+qhhx7SW2+9pXvvvVeS5ODgoM8++0yXLl3Sgw8+qNGjRxtf5isJV1dX/e9//1NAQIAeffRRNWjQQKNGjdLly5dLtXJqxYoVGjZsmJ5//nnVr19f/fv31969exUQEHDd+z744AP5+Pioffv2GjBggMaMGSMPD49CryZ26dJFfn5+6tatm9X+WQAAAAAAALcLk8Visdi7CHszmUz67LPP1L9/f3uXUiq//vqr/P399dVXX6lz585Ge3Z2tu655x6tWLGiyP21biQrK+uPr/BN/kQOZteyLBkAgNtOclQve5cAAABQoVzNFTIzM6+7iKfcNjpH2fv666+VnZ2tRo0aKS0tTVOnTlVgYKDat28vSSooKNCpU6f0xhtvqEqVKurbt6+dKwYAAAAAACjaTb++V1ZWr14td3f3Io+GDRvatJY5c+YUW0uPHj1sWktRcnNz9fe//10NGzbUgAED5O3trW3bthmvTKakpMjHx0cffvih3n//fVWqROYIAAAAAABuT3Z/fe/8+fM6efJkkdecnJyM/aBs4cyZMzpz5kyR11xcXHTPPffYrBZ74vU9AMDdhNf3AAAAylZJX9+zeyiF209J/+MBAAAAAAC4VklzBbu/vgcAAAAAAIC7D6EUAAAAAAAAbI5QCgAAAAAAADZHKAUAAAAAAACbq2TvAnD7emDGJr6+BwC4Y/AVPQAAgDsLK6UAAAAAAABgc4RSAAAAAAAAsDlCKQAAAAAAANjcXR1KJScny2QyKT4+3t6lAAAAAAAA3FXu6lCqItq7d686d+6sKlWqqGrVqurWrZsSEhLsXRYAAAAAAIAVQqlSys/PV0FBgb3LKFJ2dra6d++ugIAA7d69W9988408PDzUrVs35ebm2rs8AAAAAAAAw10RShUUFGj+/PmqW7euzGazAgIC9NprrxnXf/75Z3Xs2FGurq5q3Lixdu3aZVyLiYlRlSpVFBsbq5CQEJnNZqWkpOjs2bMaNmyYqlatKldXV/Xo0UNJSUmF7vviiy9Uv359ubq6auDAgbp48aJWrlypwMBAVa1aVZMmTVJ+fr5x36pVq9SiRQt5eHjI19dXTz75pDIyMkr0nEeOHNGZM2c0a9Ys1a9fXw0bNtSMGTN08uRJ/fLLL2XwSwIAAAAAAJSNuyKUioiIUFRUlF555RX98MMP+vDDD+Xj42Ncf+mllzRlyhTFx8erXr16GjJkiPLy8ozrFy9e1Lx587Rs2TIdPnxYNWvWVHh4uPbt26fY2Fjt2rVLFotFPXv2tFqRdPHiRS1atEgfffSRNm7cqG3btmnAgAHasGGDNmzYoFWrVmnp0qX69NNPjXtyc3P16quvKiEhQZ9//rmSk5MVHh5eouesX7++qlevruXLl+vKlSu6dOmSli9frgYNGigwMLDY+3JycpSVlWV1AAAAAAAAlCeTxWKx2LuI8nT+/Hl5e3srOjpao0ePtrqWnJys++67T8uWLdOoUaMkST/88IMaNmyoxMREBQcHKyYmRiNGjFB8fLwaN24sSUpKSlK9evW0Y8cOtWnTRpJ0+vRp+fv7a+XKlRo0aJBx308//aQ6depIksaNG6dVq1bp5MmTcnd3lyR1795dgYGBeuedd4qsf9++fWrZsqXOnz9v3HM933//vfr376/jx49LkoKCgrRp0ybde++9xd4zc+ZMRUZGFmr3n/yJHMyuN5wTAIDbQXJUL3uXAAAAAElZWVny8vJSZmamPD09i+1X4VdKJSYmKicnR507dy62T2hoqPG3n5+fJFm9Mufs7GzVJzExUZUqVVKrVq2MturVq6t+/fpKTEw02lxdXY1ASpJ8fHwUGBhoFS75+PhYzbV//3716dNHAQEB8vDwUIcOHSRJKSkpN3zWS5cuadSoUWrbtq2+/fZb7dixQw888IB69eqlS5cuFXtfRESEMjMzjSM1NfWGcwEAAAAAANyKSvYuoLy5uLjcsI+Tk5Pxt8lkkiSrzcxdXFyM9tL487hXxy6q7epcFy5cULdu3dStWzetXr1a3t7eSklJUbdu3XTlypUbzvfhhx8qOTlZu3btkoODg9FWtWpVrV+/Xk888USR95nNZpnN5lI/HwAAAAAAwM2q8CulgoKC5OLioi1btpTZmA0aNFBeXp52795ttJ0+fVpHjx5VSEjITY975MgRnT59WlFRUWrXrp2Cg4NLvMm59MceVg4ODlYB2tXz2/WLgQAAAAAA4O5U4UOpypUra9q0aZo6dao++OADHTt2TN9++62WL19+02MGBQWpX79+GjNmjL755hslJCToL3/5i+655x7169fvpscNCAiQs7Oz3n77bf3888+KjY3Vq6++WuL7H3nkEZ09e1bjx49XYmKiDh8+rBEjRqhSpUrq2LHjTdcFAAAAAABQ1ip8KCVJr7zyip5//nlNnz5dDRo00OOPP16qFUhFWbFihZo3b67evXurdevWslgs2rBhQ6HX80rD29tbMTExWrNmjUJCQhQVFaXXX3+9xPcHBwfr3//+tw4ePKjWrVurXbt2OnHihDZu3GjslQUAAAAAAHA7qPBf30PpXd0ln6/vAQDuJHx9DwAA4PbA1/cAAAAAAABw2yKUuoOMGzdO7u7uRR7jxo2zd3kAAAAAAAAlVsneBaDkZs2apSlTphR57XrL4QAAAAAAAG437CmFQkr67icAAAAAAMC12FMKAAAAAAAAty1CKQAAAAAAANgcoRQAAAAAAABsjlAKAAAAAAAANsfX91CsB2ZskoPZ1d5lAABQSHJUL3uXAAAAgFvESikAAAAAAADYHKEUAAAAAAAAbI5QCgAAAAAAADZXoUOp5ORkmUwmxcfH27sUAAAAAAAA/EmFDqUqokmTJql58+Yym81q0qRJkX0OHjyodu3aqXLlyvL399f8+fNtWyQAAAAAAMANEEpdIz8/XwUFBfYu47pGjhypxx9/vMhrWVlZ6tq1q+69917t379f//jHPzRz5ky9++67Nq4SAAAAAACgeBUilCooKND8+fNVt25dmc1mBQQE6LXXXjOu//zzz+rYsaNcXV3VuHFj7dq1y7gWExOjKlWqKDY2ViEhITKbzUpJSdHZs2c1bNgwVa1aVa6ururRo4eSkpIK3ffFF1+ofv36cnV11cCBA3Xx4kWtXLlSgYGBqlq1qiZNmqT8/HzjvlWrVqlFixby8PCQr6+vnnzySWVkZJT4WRctWqTx48fr/vvvL/L66tWrdeXKFb3//vtq2LChnnjiCU2aNElvvvlmaX5SAAAAAACAclUhQqmIiAhFRUXplVde0Q8//KAPP/xQPj4+xvWXXnpJU6ZMUXx8vOrVq6chQ4YoLy/PuH7x4kXNmzdPy5Yt0+HDh1WzZk2Fh4dr3759io2N1a5du2SxWNSzZ0/l5uZa3bdo0SJ99NFH2rhxo7Zt26YBAwZow4YN2rBhg1atWqWlS5fq008/Ne7Jzc3Vq6++qoSEBH3++edKTk5WeHh4mf0Wu3btUvv27eXs7Gy0devWTUePHtXZs2eLvCcnJ0dZWVlWBwAAAAAAQHmqZO8CbtX58+e1cOFCRUdHa/jw4ZKkOnXq6OGHH1ZycrIkacqUKerVq5ckKTIyUg0bNtRPP/2k4OBgSX8ERYsXL1bjxo0lSUlJSYqNjdWOHTvUpk0bSX+sQPL399fnn3+uQYMGGfctWbJEderUkSQNHDhQq1at0smTJ+Xu7q6QkBB17NhRW7duNV63GzlypFH7/fffr0WLFqlly5bKzs6Wu7v7Lf8e6enpuu+++6zargZ06enpqlq1aqF75s6dq8jIyFueGwAAAAAAoKTu+JVSiYmJysnJUefOnYvtExoaavzt5+cnSVavzDk7O1v1SUxMVKVKldSqVSujrXr16qpfv74SExONNldXVyOQkv4IfwIDA63CJR8fH6u59u/frz59+iggIEAeHh7q0KGDJCklJaVUz12WIiIilJmZaRypqal2qwUAAAAAANwd7viVUi4uLjfs4+TkZPxtMpkkyWozcxcXF6O9NP487tWxi2q7OteFCxfUrVs3devWTatXr5a3t7dSUlLUrVs3XblypdTzF8XX11cnT560art67uvrW+Q9ZrNZZrO5TOYHAAAAAAAoiTt+pVRQUJBcXFy0ZcuWMhuzQYMGysvL0+7du42206dP6+jRowoJCbnpcY8cOaLTp08rKipK7dq1U3BwcKk2OS+J1q1b63//+5/V3lebN29W/fr1i3x1DwAAAAAAwB7u+FCqcuXKmjZtmqZOnaoPPvhAx44d07fffqvly5ff9JhBQUHq16+fxowZo2+++UYJCQn6y1/+onvuuUf9+vW76XEDAgLk7Oyst99+Wz///LNiY2P16quvlmqMn376SfHx8UpPT9elS5cUHx+v+Ph4Y6XVk08+KWdnZ40aNUqHDx/Wxx9/rIULF+q555676boBAAAAAADK2h3/+p4kvfLKK6pUqZKmT5+uEydOyM/PT+PGjbulMVesWKFnn31WvXv31pUrV9S+fXtt2LCh0Ot5peHt7a2YmBj9/e9/16JFi9SsWTO9/vrr6tu3b4nHGD16tOLi4ozzpk2bSpKOHz+uwMBAeXl56b///a/Gjx+v5s2bq0aNGpo+fbrGjh1703UDAAAAAACUNZPFYrHYuwjcXrKysuTl5SX/yZ/Iwexq73IAACgkOaqXvUsAAABAMa7mCpmZmfL09Cy23x3/+h4AAAAAAADuPIRSt5Fx48bJ3d29yONWX0cEAAAAAAC4nVSIPaUqilmzZmnKlClFXrvecjcAAAAAAIA7DXtKoZCSvvsJAAAAAABwLfaUAgAAAAAAwG2LUAoAAAAAAAA2RygFAAAAAAAAmyOUAgAAAAAAgM3x9T0U64EZm+RgdrV3GQAASJKSo3rZuwQAAACUIVZKAQAAAAAAwOYIpQAAAAAAAGBzhFIAAAAAAACwOUIpAAAAAAAA2NxdEUqFhYVp8uTJ9i6jSGVdm8Vi0fTp0+Xn5ycXFxd16dJFSUlJZTY+AAAAAABAWbgrQqm7yfz587Vo0SK988472r17t9zc3NStWzddvnzZ3qUBAAAAAAAYKnwoFR4erri4OC1cuFAmk0kmk0nJycmKi4vTgw8+KLPZLD8/P7344ovKy8sz7gsLC9OkSZM0depUVatWTb6+vpo5c6bV2CaTScuWLdOAAQPk6uqqoKAgxcbGWvX5/vvv1aNHD7m7u8vHx0dDhw7VqVOnrltbeHi4cf7nY9u2bdd9VovFogULFujll19Wv379FBoaqg8++EAnTpzQ559/XhY/JwAAAAAAQJmo8KHUwoUL1bp1a40ZM0ZpaWlKS0uTk5OTevbsqZYtWyohIUFLlizR8uXLNXv2bKt7V65cKTc3N+3evVvz58/XrFmztHnzZqs+kZGRGjx4sA4ePKiePXvqqaee0pkzZyRJ586dU6dOndS0aVPt27dPGzdu1MmTJzV48OBia/P399fChQuN87S0ND377LOqWbOmgoODr/usx48fV3p6urp06WK0eXl5qVWrVtq1a1ex9+Xk5CgrK8vqAAAAAAAAKE8VPpTy8vKSs7OzXF1d5evrK19fXy1evFj+/v6Kjo5WcHCw+vfvr8jISL3xxhsqKCgw7g0NDdWMGTMUFBSkYcOGqUWLFtqyZYvV+OHh4RoyZIjq1q2rOXPmKDs7W3v27JEkRUdHq2nTppozZ46Cg4PVtGlTvf/++9q6dat+/PHHImtzdHSUl5eXcb5z504tXbpU69atk6+v73WfNT09XZLk4+Nj1e7j42NcK8rcuXPl5eVlHP7+/qX6jQEAAAAAAEqrwodSRUlMTFTr1q1lMpmMtrZt2yo7O1u//vqr0RYaGmp1n5+fnzIyMqza/tzHzc1Nnp6eRp+EhARt3bpV7u7uxnF1tdOxY8duWOd3332noUOHKjo6Wm3bti39g5ZQRESEMjMzjSM1NbXc5gIAAAAAAJCkSvYu4Hbm5ORkdW4ymaxWUt2oT3Z2tvr06aN58+YVGtvPz++6c6enp6tv374aPXq0Ro0aVaJ6r66kOnnypNX4J0+eVJMmTYq9z2w2y2w2l2gOAAAAAACAsnBXrJRydnZWfn6+cd6gQQPt2rVLFovFaNuxY4c8PDxUu3btMpu3WbNmOnz4sAIDA1W3bl2rw83NrcjaJOny5cvq16+fgoOD9eabb5Z4vvvuu0++vr5WrxhmZWVp9+7dat26ddk8FAAAAAAAQBm4K0KpwMBA7d69W8nJyTp16pSeeeYZpaamauLEiTpy5IjWr1+vGTNm6LnnnpODQ9n9JOPHj9eZM2c0ZMgQ7d27V8eOHdOmTZs0YsQII4i6traCggI9/fTTSk1N1aJFi/T7778rPT1d6enpunLlynXnM5lMmjx5smbPnq3Y2FgdOnRIw4YNU61atdS/f/8yey4AAAAAAIBbdVeEUlOmTJGjo6NCQkLk7e2t3NxcbdiwQXv27FHjxo01btw4jRo1Si+//HKZzlurVi3t2LFD+fn56tq1qxo1aqTJkyerSpUqRvh1bW0pKSmKi4tTWlqaQkJC5OfnZxw7d+684ZxTp07VxIkTNXbsWLVs2VLZ2dnauHGjKleuXKbPBgAAAAAAcCtMlj+/wwboj1f+vLy85D/5EzmYXe1dDgAAkqTkqF72LgEAAAAlcDVXyMzMlKenZ7H97oqVUgAAAAAAALi9EErdQbZv3y53d/diDwAAAAAAgDtFJXsXgJJr0aKF4uPj7V0GAAAAAADALWNPKRRS0nc/AQAAAAAArsWeUgAAAAAAALhtEUoBAAAAAADA5gilAAAAAAAAYHOEUgAAAAAAALA5vr6HYj0wY5MczK72LgMAcIdLjupl7xIAAABwG2KlFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAmyOUsoEdO3aoUaNGcnJyUv/+/e1dDgAAAAAAgN0RStnAc889pyZNmuj48eOKiYkpt3nOnDmjiRMnqn79+nJxcVFAQIAmTZqkzMzMcpsTAAAAAADgZvD1PRs4duyYxo0bp9q1a5frPCdOnNCJEyf0+uuvKyQkRL/88ovGjRunEydO6NNPPy3XuQEAAAAAAEqDlVKlFBYWpokTJ2ry5MmqWrWqfHx89N577+nChQsaMWKEPDw8VLduXX355ZdKTk6WyWTS6dOnNXLkSJlMJmOl1OHDh9W7d295enrKw8ND7dq107Fjx4x53n//fTVs2FBms1l+fn6aMGHCDWt74IEHtHbtWvXp00d16tRRp06d9Nprr+nf//638vLyyusnAQAAAAAAKDVCqZuwcuVK1ahRQ3v27NHEiRP117/+VYMGDVKbNm104MABde3aVUOHDpW3t7fS0tLk6empBQsWKC0tTY8//rh+++03tW/fXmazWV9//bX279+vkSNHGsHRkiVLNH78eI0dO1aHDh1SbGys6tate1O1ZmZmytPTU5UqFb8oLicnR1lZWVYHAAAAAABAeTJZLBaLvYu4k4SFhSk/P1/bt2+XJOXn58vLy0uPPvqoPvjgA0lSenq6/Pz8tGvXLj300EOqUqWKFixYoPDwcEnS3//+d3300Uc6evSonJycCs1xzz33aMSIEZo9e/Yt1Xrq1Ck1b95cf/nLX/Taa68V22/mzJmKjIws1O4/+RM5mF1vqQYAAJKjetm7BAAAANhQVlaWvLy8jIUyxWGl1E0IDQ01/nZ0dFT16tXVqFEjo83Hx0eSlJGRUeT98fHxateuXZGBVEZGhk6cOKHOnTvfUo1ZWVnq1auXQkJCNHPmzOv2jYiIUGZmpnGkpqbe0twAAAAAAAA3wkbnN+HaMMlkMlm1mUwmSVJBQUGR97u4uBQ79vWuldT58+fVvXt3eXh46LPPPisy/Pozs9kss9l8y/MCAAAAAACUFCul7CA0NFTbt29Xbm5uoWseHh4KDAzUli1bbmrsrKwsde3aVc7OzoqNjVXlypVvtVwAAAAAAIAyRyhlBxMmTFBWVpaeeOIJ7du3T0lJSVq1apWOHj0q6Y89nt544w0tWrRISUlJOnDggN5+++0bjns1kLpw4YKWL1+urKwspaenKz09Xfn5+eX9WAAAAAAAACXG63t2UL16dX399dd64YUX1KFDBzk6OqpJkyZq27atJGn48OG6fPmy3nrrLU2ZMkU1atTQwIEDbzjugQMHtHv3bkkq9LW+48ePKzAwsMyfBQAAAAAA4Gbw9T0UcnWXfL6+BwAoC3x9DwAA4O7C1/cAAAAAAABw2yKUuoOsXr1a7u7uRR4NGza0d3kAAAAAAAAlxp5Sd5C+ffuqVatWRV5zcnKycTUAAAAAAAA3jz2lUEhJ3/0EAAAAAAC4FntKAQAAAAAA4LZFKAUAAAAAAACbI5QCAAAAAACAzRFKAQAAAAAAwOb4+h6K9cCMTXIwu9q7DADAbSI5qpe9SwAAAEAFwkopAAAAAAAA2ByhFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAm7srQqmwsDBNnjzZ3mUUqaxrCw8Pl8lksjq6d+9eZuMDAAAAAACUBb6+VwF1795dK1asMM7NZrMdqwEAAAAAACiswq+UCg8PV1xcnBYuXGisHEpOTlZcXJwefPBBmc1m+fn56cUXX1ReXp5xX1hYmCZNmqSpU6eqWrVq8vX11cyZM63GNplMWrZsmQYMGCBXV1cFBQUpNjbWqs/333+vHj16yN3dXT4+Pho6dKhOnTp13dqKWu1kMpm0bdu2Ej2z2WyWr6+vcVStWvWWfkMAAAAAAICyVuFDqYULF6p169YaM2aM0tLSlJaWJicnJ/Xs2VMtW7ZUQkKClixZouXLl2v27NlW965cuVJubm7avXu35s+fr1mzZmnz5s1WfSIjIzV48GAdPHhQPXv21FNPPaUzZ85Iks6dO6dOnTqpadOm2rdvnzZu3KiTJ09q8ODBxdbm7++vhQsXGudpaWl69tlnVbNmTQUHB5fombdt26aaNWuqfv36+utf/6rTp0+XwS8JAAAAAABQdir863teXl5ydnaWq6urfH19JUkvvfSS/P39FR0dLZPJpODgYJ04cULTpk3T9OnT5eDwR1YXGhqqGTNmSJKCgoIUHR2tLVu26JFHHjHGDw8P15AhQyRJc+bM0aJFi7Rnzx51795d0dHRatq0qebMmWP0f//99+Xv768ff/xR9erVK1Tb1Zq9vLwkSevWrdPSpUv11VdfWfUpTvfu3fXoo4/qvvvu07Fjx/T3v/9dPXr00K5du+To6FjkPTk5OcrJyTHOs7KySvTbAgAAAAAA3KwKH0oVJTExUa1bt5bJZDLa2rZtq+zsbP36668KCAiQ9Eco9Wd+fn7KyMiwavtzHzc3N3l6ehp9EhIStHXrVrm7uxeq4dixY6pXr9516/zuu+80dOhQRUdHq23btiV6tieeeML4u1GjRgoNDVWdOnW0bds2de7cuch75s6dq8jIyBKNDwAAAAAAUBYq/Ot7t8LJycnq3GQyqaCgoMR9srOz1adPH8XHx1sdSUlJat++/XXnTk9PV9++fTV69GiNGjXqpp/h/vvvV40aNfTTTz8V2yciIkKZmZnGkZqaetPzAQAAAAAAlMRdsVLK2dlZ+fn5xnmDBg20du1aWSwWY7XUjh075OHhodq1a5fZvM2aNdPatWsVGBioSpWK/qmvrU2SLl++rH79+ik4OFhvvvnmLdXw66+/6vTp0/Lz8yu2j9ls5gt9AAAAAADApu6KlVKBgYHavXu3kpOTderUKT3zzDNKTU3VxIkTdeTIEa1fv14zZszQc889Z+wnVRbGjx+vM2fOaMiQIdq7d6+OHTumTZs2acSIEUYQdW1tBQUFevrpp5WamqpFixbp999/V3p6utLT03XlypXrzpedna0XXnhB3377rZKTk7Vlyxb169dPdevWVbdu3crsuQAAAAAAAG7VXRFKTZkyRY6OjgoJCZG3t7dyc3O1YcMG7dmzR40bN9a4ceM0atQovfzyy2U6b61atbRjxw7l5+era9euatSokSZPnqwqVaoY4de1taWkpCguLk5paWkKCQmRn5+fcezcufO68zk6OurgwYPq27ev6tWrp1GjRql58+bavn07K6EAAAAAAMBtxWSxWCz2LgK3l6ysLHl5ecl/8idyMLvauxwAwG0iOaqXvUsAAADAHeBqrpCZmSlPT89i+90VK6UAAAAAAABweyGUuoNs375d7u7uxR4AAAAAAAB3irvi63sVRYsWLRQfH2/vMgAAAAAAAG4Ze0qhkJK++wkAAAAAAHAt9pQCAAAAAADAbYtQCgAAAAAAADZHKAUAAAAAAACbI5QCAAAAAACAzfH1PRTrgRmb5GB2tXcZAIBykBzVy94lAAAA4C7HSikAAAAAAADYHKEUAAAAAAAAbI5QCgAAAAAAADZHKAUAAAAAAACbI5S6CYGBgVqwYMFtOdfRo0fVsWNH+fj4qHLlyrr//vv18ssvKzc3t/yKBAAAAAAAKCW+vldO8vPzZTKZ5OBg29zPyclJw4YNU7NmzVSlShUlJCRozJgxKigo0Jw5c2xaCwAAAAAAQHEq5EqpgoICzZ8/X3Xr1pXZbFZAQIBee+01SdKhQ4fUqVMnubi4qHr16ho7dqyys7ONe8PDw9W/f3+9/vrr8vPzU/Xq1TV+/HhjpVFYWJh++eUX/e1vf5PJZJLJZJIkxcTEqEqVKoqNjVVISIjMZrNSUlK0d+9ePfLII6pRo4a8vLzUoUMHHThwwJjPYrFo5syZCggIkNlsVq1atTRp0qTrznU9999/v0aMGKHGjRvr3nvvVd++ffXUU09p+/btZfb7AgAAAAAA3KoKGUpFREQoKipKr7zyin744Qd9+OGH8vHx0YULF9StWzdVrVpVe/fu1Zo1a/TVV19pwoQJVvdv3bpVx44d09atW7Vy5UrFxMQoJiZGkrRu3TrVrl1bs2bNUlpamtLS0oz7Ll68qHnz5mnZsmU6fPiwatasqfPnz2v48OH65ptv9O233yooKEg9e/bU+fPnJUlr167VW2+9paVLlyopKUmff/65GjVqdMO5Suqnn37Sxo0b1aFDh5v8NQEAAAAAAMpehXt97/z581q4cKGio6M1fPhwSVKdOnX08MMP67333tPly5f1wQcfyM3NTZIUHR2tPn36aN68efLx8ZEkVa1aVdHR0XJ0dFRwcLB69eqlLVu2aMyYMapWrZocHR3l4eEhX19fq7lzc3O1ePFiNW7c2Gjr1KmTVZ93331XVapUUVxcnHr37q2UlBT5+vqqS5cucnJyUkBAgB588EFJuu5cN9KmTRsdOHBAOTk5Gjt2rGbNmlVs35ycHOXk5BjnWVlZpZoLAAAAAACgtCrcSqnExETl5OSoc+fORV5r3LixEUhJUtu2bVVQUKCjR48abQ0bNpSjo6Nx7ufnp4yMjBvO7ezsrNDQUKu2kydPasyYMQoKCpKXl5c8PT2VnZ2tlJQUSdKgQYN06dIl3X///RozZow+++wz5eXllfq5r/Xxxx/rwIED+vDDD/Wf//xHr7/+erF9586dKy8vL+Pw9/e/5fkBAAAAAACup8KFUi4uLrc8hpOTk9W5yWRSQUFBiea+dt+n4cOHKz4+XgsXLtTOnTsVHx+v6tWr68qVK5Ikf39/HT16VIsXL5aLi4ueeeYZtW/f/pa/lufv76+QkBANGTJEUVFRmjlzpvLz84vsGxERoczMTONITU29pbkBAAAAAABupMKFUkFBQXJxcdGWLVsKXWvQoIESEhJ04cIFo23Hjh1ycHBQ/fr1SzyHs7NzsQHPtXbs2KFJkyapZ8+eatiwocxms06dOmXVx8XFRX369NGiRYu0bds27dq1S4cOHSr1XMUpKChQbm5uscGa2WyWp6en1QEAAAAAAFCeKtyeUpUrV9a0adM0depUOTs7q23btvr99991+PBhPfXUU5oxY4aGDx+umTNn6vfff9fEiRM1dOhQYz+pkggMDNT//vc/PfHEEzKbzapRo0axfYOCgrRq1Sq1aNFCWVlZeuGFF6xWc8XExCg/P1+tWrWSq6ur/vWvf8nFxUX33ntvqeeSpNWrV8vJyUmNGjWS2WzWvn37FBERoccff7zQCjAAAAAAAAB7qXArpSTplVde0fPPP6/p06erQYMGevzxx5WRkSFXV1dt2rRJZ86cUcuWLTVw4EB17txZ0dHRpRp/1qxZSk5OVp06deTt7X3dvsuXL9fZs2fVrFkzDR06VJMmTVLNmjWN61WqVNF7772ntm3bKjQ0VF999ZX+/e9/q3r16qWeS5IqVaqkefPm6cEHH1RoaKgiIyM1YcIELVu2rFTPCAAAAAAAUJ5MFovFYu8icHvJysr6Y8PzyZ/Iwexq73IAAOUgOaqXvUsAAABABXU1V8jMzLzuFkEVcqUUAAAAAAAAbm+EUneYHj16yN3dvchjzpw59i4PAAAAAACgRCrcRucV3bJly3Tp0qUir1WrVs3G1QAAAAAAANwc9pRCISV99xMAAAAAAOBa7CkFAAAAAACA2xahFAAAAAAAAGyOUAoAAAAAAAA2RygFAAAAAAAAm+PreyjWAzM2ycHsau8yAAAllBzVy94lAAAAACXGSikAAAAAAADYHKEUAAAAAAAAbI5QCgAAAAAAADZHKGUDO3bsUKNGjeTk5KT+/fvbuxwAAAAAAAC7I5Sygeeee05NmjTR8ePHFRMTU65zvfvuuwoLC5Onp6dMJpPOnTtXrvMBAAAAAADcDEIpGzh27Jg6deqk2rVrq0qVKuU618WLF9W9e3f9/e9/L9d5AAAAAAAAbgWhVCmFhYVp4sSJmjx5sqpWrSofHx+99957unDhgkaMGCEPDw/VrVtXX375pZKTk2UymXT69GmNHDlSJpPJWCl1+PBh9e7dW56envLw8FC7du107NgxY573339fDRs2lNlslp+fnyZMmFCi+iZPnqwXX3xRDz30UHk8PgAAAAAAQJkglLoJK1euVI0aNbRnzx5NnDhRf/3rXzVo0CC1adNGBw4cUNeuXTV06FB5e3srLS1Nnp6eWrBggdLS0vT444/rt99+U/v27WU2m/X1119r//79GjlypPLy8iRJS5Ys0fjx4zV27FgdOnRIsbGxqlu3rp2fGgAAAAAAoOxUsncBd6LGjRvr5ZdfliRFREQoKipKNWrU0JgxYyRJ06dP15IlS3To0CE99NBDMplM8vLykq+vryTpn//8p7y8vPTRRx/JyclJklSvXj1j/NmzZ+v555/Xs88+a7S1bNmy3J4nJydHOTk5xnlWVla5zQUAAAAAACCxUuqmhIaGGn87OjqqevXqatSokdHm4+MjScrIyCjy/vj4eLVr184IpP4sIyNDJ06cUOfOncu46uLNnTtXXl5exuHv72+zuQEAAAAAwN2JUOomXBsmmUwmqzaTySRJKigoKPJ+FxeXYse+3rXyEhERoczMTONITU21eQ0AAAAAAODuQihlB6Ghodq+fbtyc3MLXfPw8FBgYKC2bNlis3rMZrM8PT2tDgAAAAAAgPJEKGUHEyZMUFZWlp544gnt27dPSUlJWrVqlY4ePSpJmjlzpt544w0tWrRISUlJOnDggN5+++0SjZ2enq74+Hj99NNPkqRDhw4pPj5eZ86cKbfnAQAAAAAAKC1CKTuoXr26vv76a2VnZ6tDhw5q3ry53nvvPeMVwOHDh2vBggVavHixGjZsqN69eyspKalEY7/zzjtq2rSpsel6+/bt1bRpU8XGxpbb8wAAAAAAAJSWyWKxWOxdBG4vWVlZf2x4PvkTOZhd7V0OAKCEkqN62bsEAAAAwMgVMjMzr7tFECulAAAAAAAAYHOEUneQ1atXy93dvcijYcOG9i4PAAAAAACgxCrZuwCUXN++fdWqVasir13djwoAAAAAAOBOwJ5SKKSk734CAAAAAABciz2lAAAAAAAAcNsilAIAAAAAAIDNEUoBAAAAAADA5gilAAAAAAAAYHN8fQ/FemDGJjmYXe1dBgDgGslRvexdAgAAAHDLWCkFAAAAAAAAmyOUAgAAAAAAgM0RSgEAAAAAAMDmCKUAAAAAAABgc3dFKBUWFqbJkyfbu4wilWdt48aNk8lk0oIFC8plfAAAAAAAgJt1V4RSd6PPPvtM3377rWrVqmXvUgAAAAAAAAqp8KFUeHi44uLitHDhQplMJplMJiUnJysuLk4PPvigzGaz/Pz89OKLLyovL8+4LywsTJMmTdLUqVNVrVo1+fr6aubMmVZjm0wmLVu2TAMGDJCrq6uCgoIUGxtr1ef7779Xjx495O7uLh8fHw0dOlSnTp26bm3h4eHG+Z+Pbdu2leiZf/vtN02cOFGrV6+Wk5PTLf1+AAAAAAAA5aHCh1ILFy5U69atNWbMGKWlpSktLU1OTk7q2bOnWrZsqYSEBC1ZskTLly/X7Nmzre5duXKl3NzctHv3bs2fP1+zZs3S5s2brfpERkZq8ODBOnjwoHr27KmnnnpKZ86ckSSdO3dOnTp1UtOmTbVv3z5t3LhRJ0+e1ODBg4utzd/fXwsXLjTO09LS9Oyzz6pmzZoKDg6+4fMWFBRo6NCheuGFF9SwYcMy+hUBAAAAAADKViV7F1DevLy85OzsLFdXV/n6+kqSXnrpJfn7+ys6Olomk0nBwcE6ceKEpk2bpunTp8vB4Y+sLjQ0VDNmzJAkBQUFKTo6Wlu2bNEjjzxijB8eHq4hQ4ZIkubMmaNFixZpz5496t69u6Kjo9W0aVPNmTPH6P/+++/L399fP/74o+rVq1eotqs1e3l5SZLWrVunpUuX6quvvrLqU5x58+apUqVKmjRpUol/o5ycHOXk5BjnWVlZJb4XAAAAAADgZlT4lVJFSUxMVOvWrWUymYy2tm3bKjs7W7/++qvRFhoaanWfn5+fMjIyrNr+3MfNzU2enp5Gn4SEBG3dulXu7u7GcXW107Fjx25Y53fffaehQ4cqOjpabdu2vWH//fv3a+HChYqJibF6thuZO3euEYR5eXnJ39+/xPcCAAAAAADcjLsylCqpa/djMplMKigoKHGf7Oxs9enTR/Hx8VZHUlKS2rdvf92509PT1bdvX40ePVqjRo0qUb3bt29XRkaGAgICVKlSJVWqVEm//PKLnn/+eQUGBhZ7X0REhDIzM40jNTW1RPMBAAAAAADcrAr/+p4kOTs7Kz8/3zhv0KCB1q5dK4vFYqwo2rFjhzw8PFS7du0ym7dZs2Zau3atAgMDValS0T/1tbVJ0uXLl9WvXz8FBwfrzTffLPF8Q4cOVZcuXazaunXrpqFDh2rEiBHF3mc2m2U2m0s8DwAAAAAAwK26K1ZKBQYGavfu3UpOTtapU6f0zDPPKDU1VRMnTtSRI0e0fv16zZgxQ88995yxn1RZGD9+vM6cOaMhQ4Zo7969OnbsmDZt2qQRI0YYQdS1tRUUFOjpp59WamqqFi1apN9//13p6elKT0/XlStXrjtf9erV9cADD1gdTk5O8vX1Vf369cvsuQAAAAAAAG7VXRFKTZkyRY6OjgoJCZG3t7dyc3O1YcMG7dmzR40bN9a4ceM0atQovfzyy2U6b61atbRjxw7l5+era9euatSokSZPnqwqVaoY4de1taWkpCguLk5paWkKCQmRn5+fcezcubNM6wMAAAAAALAXk8Visdi7CNxesrKy/tjwfPIncjC72rscAMA1kqN62bsEAAAAoFhXc4XMzEx5enoW2++uWCkFAAAAAACA2wuh1B1k+/btcnd3L/YAAAAAAAC4U9wVX9+rKFq0aKH4+Hh7lwEAAAAAAHDL2FMKhZT03U8AAAAAAIBrsacUAAAAAAAAbluEUgAAAAAAALA5QikAAAAAAADYHKEUAAAAAAAAbI6v76FYD8zYJAezq73LAABcIzmql71LAAAAAG4ZK6UAAAAAAABgc4RSAAAAAAAAsDlCKQAAAAAAANgcoRQAAAAAAABs7q4IpcLCwjR58mR7l1Gksq5t5syZCg4Olpubm6pWraouXbpo9+7dZTY+AAAAAABAWbgrQqm7Sb169RQdHa1Dhw7pm2++UWBgoLp27arff//d3qUBAAAAAAAYKnwoFR4erri4OC1cuFAmk0kmk0nJycmKi4vTgw8+KLPZLD8/P7344ovKy8sz7gsLC9OkSZM0depUVatWTb6+vpo5c6bV2CaTScuWLdOAAQPk6uqqoKAgxcbGWvX5/vvv1aNHD7m7u8vHx0dDhw7VqVOnrltbeHi4cf7nY9u2bTd83ieffFJdunTR/fffr4YNG+rNN99UVlaWDh48eMu/JQAAAAAAQFmp8KHUwoUL1bp1a40ZM0ZpaWlKS0uTk5OTevbsqZYtWyohIUFLlizR8uXLNXv2bKt7V65cKTc3N+3evVvz58/XrFmztHnzZqs+kZGRGjx4sA4ePKiePXvqqaee0pkzZyRJ586dU6dOndS0aVPt27dPGzdu1MmTJzV48OBia/P399fChQuN87S0ND377LOqWbOmgoODS/XsV65c0bvvvisvLy81btz4Fn5FAAAAAACAslXJ3gWUNy8vLzk7O8vV1VW+vr6SpJdeekn+/v6Kjo6WyWRScHCwTpw4oWnTpmn69OlycPgjqwsNDdWMGTMkSUFBQYqOjtaWLVv0yCOPGOOHh4dryJAhkqQ5c+Zo0aJF2rNnj7p3767o6Gg1bdpUc+bMMfq///778vf3148//qh69eoVqu1qzV5eXpKkdevWaenSpfrqq6+s+lzPF198oSeeeEIXL16Un5+fNm/erBo1ahTbPycnRzk5OcZ5VlZWieYBAAAAAAC4WRV+pVRREhMT1bp1a5lMJqOtbdu2ys7O1q+//mq0hYaGWt3n5+enjIwMq7Y/93Fzc5Onp6fRJyEhQVu3bpW7u7txXF3tdOzYsRvW+d1332no0KGKjo5W27ZtS/x8HTt2VHx8vHbu3Knu3btr8ODBher+s7lz5xpBmJeXl/z9/Us8FwAAAAAAwM24K0OpknJycrI6N5lMKigoKHGf7Oxs9enTR/Hx8VZHUlKS2rdvf92509PT1bdvX40ePVqjRo0qVd1ubm6qW7euHnroIS1fvlyVKlXS8uXLi+0fERGhzMxM40hNTS3VfAAAAAAAAKVV4V/fkyRnZ2fl5+cb5w0aNNDatWtlsViM1VI7duyQh4eHateuXWbzNmvWTGvXrlVgYKAqVSr6p762Nkm6fPmy+vXrp+DgYL355pu3XEdBQYHV63nXMpvNMpvNtzwPAAAAAABASd0VK6UCAwO1e/duJScn69SpU3rmmWeUmpqqiRMn6siRI1q/fr1mzJih5557zthPqiyMHz9eZ86c0ZAhQ7R3714d+3/t3XtUVWX+x/HPAfQAIhikIgaClZcUSbPUTNEBxTLNy6TjlIWXihxz1MHM8kapZOVoauNtVUgX7UaXEcYLFCZ5S4W8xqBL1BTSnIQAE4L9+8PF/nVUCPVwQH2/1jpL2Pt59v4+6KOuz3r2sw8d0rp16zRixAgziLqwtrKyMj311FM6duyYFi5cqFOnTik3N1e5ubkqLi6u9H6FhYV6/vnntXXrVh05ckQ7d+7UyJEjdfz4cT388MN2GxcAAAAAAMDVuiFCqejoaDk7O+uOO+5Qw4YNVVJSoqSkJG3fvl0hISGKiorSqFGjNHXqVLve18/PT998841KS0vVu3dvBQcHa/z48WrQoIEZfl1Y29GjR7Vx40bl5OTojjvuUJMmTczP5s2bK72fs7Ozvv/+ew0ePFgtWrRQv379dPr0aW3atElt2rSx69gAAAAAAACuhsUwDKOmi0Dtkp+ff37D8/EfysnqXtPlAAAukP1y35ouAQAAAKhQea6Ql5cnT0/PCtvdECulAAAAAAAAULsQSl1DNm3aJA8Pjwo/AAAAAAAA14ob4u1714uOHTsqIyOjpssAAAAAAAC4auwphYtU9dlPAAAAAACAC7GnFAAAAAAAAGotQikAAAAAAAA4HKEUAAAAAAAAHI5QCgAAAAAAAA7H2/dQobYz1snJ6l7TZQDADSf75b41XQIAAABQ7VgpBQAAAAAAAIcjlAIAAAAAAIDDEUoBAAAAAADA4QilAAAAAAAA4HCEUtUsOztbFotFGRkZNV0KAAAAAABArXHNhlIlJSU1XYJdFRcX13QJAAAAAAAADmOXUKqsrEyxsbEKCgqSm5ubQkJC9PHHH0uSUlNTZbFYlJKSoo4dO8rd3V333nuvMjMzba7x+eefq0OHDnJ1dVXz5s0VExOj3377zTxvsVi0ZMkS9e/fX/Xq1dPs2bMlSbNmzVKjRo1Uv359jR49Ws8995zuvPNOSdLXX3+tOnXqKDc31+Ze48ePV7du3ao0trS0NHXr1k1ubm7y9/fXuHHjVFhYaJ4PDAzUnDlzNHLkSNWvX18BAQFavny5eT4oKEiS1L59e1ksFvXo0UOSFBkZqQEDBmj27Nny8/NTy5YtJUl79uzRn/70J7m5ucnHx0dPPvmkCgoKzOuV94uJiVHDhg3l6empqKgoM9SKj4+Xj4+Pzp07ZzOOAQMGaPjw4VUaMwAAAAAAQHWzSygVGxur+Ph4LV26VPv27dOECRP06KOPauPGjWabF154QfPmzdOOHTvk4uKikSNHmuc2bdqkxx57TH//+9+1f/9+LVu2THFxcWbwVG7mzJkaOHCg9uzZo5EjR+q9997T7NmzNXfuXO3cuVMBAQFasmSJ2b579+5q3ry53nnnHfNYSUmJ3nvvPZv7V+TQoUPq06ePBg8erN27d+uDDz5QWlqaxo4da9Nu3rx56tixo9LT0zVmzBg9/fTTZui2fft2SVJycrJycnKUkJBg9ktJSVFmZqY2bNigNWvWqLCwUBEREbrpppv07bff6qOPPlJycvJF90tJSdGBAweUmpqqVatWKSEhQTExMZKkhx9+WKWlpfriiy/M9idPnlRiYmKVxgwAAAAAAOAIFsMwjKu5wLlz5+Tt7a3k5GR16dLFPD569GgVFRXpySefVM+ePZWcnKywsDBJUlJSkvr27auzZ8/K1dVV4eHhCgsL05QpU8z+7777rp599lmdOHHifKEWi8aPH6/58+ebbTp37qyOHTtq8eLF5rH77rtPBQUF5h5Or7zyiuLi4rR//35JUkJCgh5//HHl5uaqXr16lY5t9OjRcnZ21rJly8xjaWlpCg0NVWFhoVxdXRUYGKhu3bqZwZdhGPL19VVMTIyioqKUnZ2toKAgpaenmyu4pPMrntauXaujR4+qbt26kqQVK1Zo8uTJOnbsmFlbUlKS+vXrpxMnTqhx48aKjIzUv//9bx07dkzu7u6SpKVLl2rSpEnKy8uTk5OTxowZo+zsbCUlJUmS/vnPf+qNN97QwYMHZbFYLvl7+PuVVfn5+fL395f/+A/lZHWv9GcEALC/7Jf71nQJAAAAwBXLz8+Xl5eX8vLy5OnpWWG7q14pdfDgQRUVFalXr17y8PAwP/Hx8Tp06JDZrl27dubXTZo0kXR+BY8kfffdd3rxxRdt+j/xxBPKyclRUVGR2a9jx442987MzNQ999xjc+zC7yMjI3Xw4EFt3bpVkhQXF6chQ4b8YSBVXldcXJxNXRERESorK9Phw4cvOTaLxSJfX19zbJUJDg42AylJOnDggEJCQmxq69q1q8rKymwedwwJCTEDKUnq0qWLCgoKdOzYMUnSE088ofXr1+v48ePmmCMjIy8ZSEnnV7p5eXmZH39//z+sHQAAAAAA4Gq4XO0Fyvc7SkxMVNOmTW3OWa1WM5iqU6eOebw8HCkrKzOvERMTo0GDBl10fVdXV/PrqgRJF2rUqJH69eunt99+W0FBQfrPf/6j1NTUKvUtKCjQU089pXHjxl10LiAgwPz692OTzo+vfGyVuZLxVEX79u0VEhKi+Ph49e7dW/v27VNiYmKF7adMmaKJEyea35evlAIAAAAAAKguVx1K3XHHHbJarTp69KhCQ0MvOv/71VIV6dChgzIzM3Xbbbdd1r1btmypb7/9Vo899ph57Ntvv72o3ejRozVs2DDdcsstuvXWW9W1a9cqXb9Dhw7av3//Zdf1e+UroUpLS/+wbevWrRUXF6fCwkIzsPrmm2/k5ORkboQunV/BdfbsWbm5uUmStm7dKg8PD5sgafTo0VqwYIGOHz+u8PDwSkMmq9Uqq9V6ReMDAAAAAAC4Elf9+F79+vUVHR2tCRMmaOXKlTp06JB27dqlRYsWaeXKlVW6xvTp0xUfH6+YmBjt27dPBw4c0OrVqzV16tRK+z3zzDN68803tXLlSmVlZWnWrFnavXv3RY+pRUREyNPTU7NmzdKIESOqPLbJkydr8+bNGjt2rDIyMpSVlaXPP//8oo3HK9OoUSO5ublp7dq1+vHHH5WXl1dh20ceeUSurq56/PHHtXfvXn311Vd65plnNHz4cDVu3NhsV1xcrFGjRmn//v1KSkrSjBkzNHbsWDk5/f9v51//+lf98MMPWrFiBRucAwAAAACAWscub9976aWXNG3aNMXGxqp169bq06ePEhMTFRQUVKX+ERERWrNmjdavX6+7775bnTt31vz589WsWbNK+z3yyCOaMmWKoqOj1aFDBx0+fFiRkZE2j/xJkpOTkyIjI1VaWmqzquqPtGvXThs3btR///tfdevWTe3bt9f06dPl5+dX5Wu4uLho4cKFWrZsmfz8/PTQQw9V2Nbd3V3r1q3T//73P919993685//rLCwMJuN3CUpLCxMt99+u7p3766hQ4eqf//+mjlzpk0bLy8vDR48WB4eHhowYECV6wUAAAAAAHCEq377Xm3Tq1cv+fr6mm/DKzdq1CidOnVKX3zxRQ1VZh+RkZE6c+aMPvvssz9sGxYWpjZt2mjhwoWXdY/yXfJ5+x4A1AzevgcAAIBrWVXfvnfVe0rVpKKiIi1dulQRERFydnbWqlWrlJycrA0bNpht8vLytGfPHr3//vvXfCBVVT///LNSU1OVmpqqf/3rXzVdDgAAAAAAwEWu6VDKYrEoKSlJs2fP1q+//qqWLVvqk08+UXh4uNnmoYce0vbt2xUVFaVevXrZ9L///vu1adOmS177+eef1/PPP1+t9VeX9u3b6+eff9bcuXNtNkgHAAAAAACoLa67x/cux/Hjx3X27NlLnvP29pa3t7eDK6odeHwPAGoWj+8BAADgWlbVx/du6FAKl1bVPzwAAAAAAAAXqmquYJe37wEAAAAAAACXg1AKAAAAAAAADkcoBQAAAAAAAIcjlAIAAAAAAIDDudR0Aai92s5Yx9v3AMCBeOseAAAAbiSslAIAAAAAAIDDEUoBAAAAAADA4QilAAAAAAAA4HDXVSgVGRmpAQMG1Mi94+Li1KBBgyvqm52dLYvFooyMjArbpKamymKx6MyZM5VeKzAwUAsWLLiiOgAAAAAAABzlugqlrlX+/v7KyclR27Ztq9znakIwAAAAAACAmsbb92oBZ2dn+fr61nQZAAAAAAAADnNFK6U+/vhjBQcHy83NTT4+PgoPD1dhYaH5+FxMTIwaNmwoT09PRUVFqbi42OxbVlam2NhYBQUFyc3NTSEhIfr4449trr9v3z49+OCD8vT0VP369dWtWzcdOnSoyvVVdv+1a9fqvvvuU4MGDeTj46MHH3zQ5trlj9IlJCSoZ8+ecnd3V0hIiLZs2WJzj7i4OAUEBMjd3V0DBw7U6dOnzXN5eXlydnbWjh07zDF7e3urc+fOZpt3331X/v7+Nvf8/eN7SUlJatGihdzc3NSzZ09lZ2eb51JTUzVixAjl5eXJYrHIYrFo5syZ5vmioiKNHDlS9evXV0BAgJYvX17lnx0AAAAAAIAjXHYolZOTo2HDhmnkyJE6cOCAUlNTNWjQIBmGIUlKSUkxj69atUoJCQmKiYkx+8fGxio+Pl5Lly7Vvn37NGHCBD366KPauHGjJOn48ePq3r27rFarvvzyS+3cuVMjR47Ub7/9VqX6/uj+hYWFmjhxonbs2KGUlBQ5OTlp4MCBKisrs7nOCy+8oOjoaGVkZKhFixYaNmyYWcO2bds0atQojR07VhkZGerZs6dmzZpl9vXy8tKdd96p1NRUSdKePXtksViUnp6ugoICSdLGjRsVGhp6yTEcO3ZMgwYNUr9+/ZSRkaHRo0frueeeM8/fe++9WrBggTw9PZWTk6OcnBxFR0eb5+fNm6eOHTsqPT1dY8aM0dNPP63MzMwq/fwAAAAAAAAc4bIf38vJydFvv/2mQYMGqVmzZpKk4OBg83zdunX11ltvyd3dXW3atNGLL76oSZMm6aWXXlJJSYnmzJmj5ORkdenSRZLUvHlzpaWladmyZQoNDdUbb7whLy8vrV69WnXq1JEktWjRosr1VXZ/JycnDR482Kb9W2+9pYYNG2r//v02ezpFR0erb9++ks6vvGrTpo0OHjyoVq1a6fXXX1efPn307LPPmvVt3rxZa9euNfv36NFDqampio6OVmpqqnr16qXvv/9eaWlp6tOnj1JTU83+F1qyZIluvfVWzZs3T5LUsmVL7dmzR3PnzjXH6OXlJYvFcsnH/h544AGNGTNGkjR58mTNnz9fX331lVq2bHnJ+507d07nzp0zv8/Pz6/8hwwAAAAAAHCVLnulVEhIiMLCwhQcHKyHH35YK1as0M8//2xz3t3d3fy+S5cuKigo0LFjx3Tw4EEVFRWpV69e8vDwMD/x8fHmI3QZGRnq1q2bGUhdSX0V3V+SsrKyNGzYMDVv3lyenp4KDAyUJB09etTmOu3atTO/btKkiSTp5MmTkqQDBw6oU6dONu3LQ7ZyoaGhSktLU2lpqTZu3KgePXqYQdWJEyd08OBB9ejR45JjqMr1K/P72suDq/LaLyU2NlZeXl7mp/yxQgAAAAAAgOpy2SulnJ2dtWHDBm3evFnr16/XokWL9MILL2jbtm1/2Lf80bXExEQ1bdrU5pzVapUkubm5XW5Jl6Vfv35q1qyZVqxYIT8/P5WVlalt27Y2+05JsgnFLBaLJF30iF9lunfvrl9++UW7du3S119/rTlz5sjX11cvv/yyQkJC5Ofnp9tvv90+g7rAhYGexWKptPYpU6Zo4sSJ5vf5+fkEUwAAAAAAoFpd0dv3LBaLunbtqq5du2r69Olq1qyZPv30U0nSd999p7Nnz5rh0tatW+Xh4SF/f395e3vLarXq6NGjFe6n1K5dO61cuVIlJSVXtFqqsvufPn1amZmZWrFihbp16yZJSktLu+x7tG7d+qIQbuvWrTbfN2jQQO3atdPixYtVp04dtWrVSo0aNdLQoUO1Zs2aCsdffv0vvvii0uvXrVtXpaWll137pVitVjMUBAAAAAAAcITLfnxv27ZtmjNnjnbs2KGjR48qISFBp06dUuvWrSVJxcXFGjVqlPbv36+kpCTNmDFDY8eOlZOTk+rXr6/o6GhNmDBBK1eu1KFDh7Rr1y4tWrRIK1eulCSNHTtW+fn5+stf/qIdO3YoKytL77zzTpU36q7s/jfddJN8fHy0fPlyHTx4UF9++aXNCqGqGjdunNauXavXXntNWVlZWrx4sc1+UuV69Oih9957zwygvL291bp1a33wwQeVhlJRUVHKysrSpEmTlJmZqffff19xcXE2bQIDA1VQUKCUlBT99NNPKioquuxxAAAAAAAA1JTLDqU8PT319ddf64EHHlCLFi00depUzZs3T/fff78kKSwsTLfffru6d++uoUOHqn///po5c6bZ/6WXXtK0adMUGxur1q1bq0+fPkpMTFRQUJAkycfHR19++aUKCgoUGhqqu+66SytWrKjyqqnK7u/k5KTVq1dr586datu2rSZMmKBXX331cn8E6ty5s1asWKHXX39dISEhWr9+vaZOnXpRu9DQUJWWltrsHdWjR4+Ljl0oICBAn3zyiT777DOFhIRo6dKlmjNnjk2be++9V1FRURo6dKgaNmyoV1555bLHAQAAAAAAUFMshmEY9rpYZGSkzpw5o88++8xel0QNyM/PP7/h+fgP5WR1/+MOAAC7yH65b02XAAAAAFy18lwhLy9Pnp6eFba77JVSAAAAAAAAwNW6pkIpDw+PCj+bNm2q6fIAAAAAAABQRVf09r2KXLgZt71lZGRUeK5p06bVem8AAAAAAADYj133lML1oarPfgIAAAAAAFyIPaUAAAAAAABQaxFKAQAAAAAAwOEIpQAAAAAAAOBwhFIAAAAAAABwOEIpAAAAAAAAOByhFAAAAAAAAByOUAoAAAAAAAAORygFAAAAAAAAhyOUAgAAAAAAgMMRSgEAAAAAAMDhCKUAAAAAAADgcIRSAAAAAAAAcDhCKQAAAAAAADgcoRQAAAAAAAAcjlAKAAAAAAAADkcoBQAAAAAAAIcjlAIAAAAAAIDDEUoBAAAAAADA4QilAAAAAAAA4HCEUgAAAAAAAHA4QikAAAAAAAA4HKEUAAAAAAAAHM6lpgtA7WMYhiQpPz+/hisBAAAAAADXmvI8oTxfqAihFC5y+vRpSZK/v38NVwIAAAAAAK5Vv/zyi7y8vCo8TyiFi3h7e0uSjh49WukfHgBVk5+fL39/fx07dkyenp41XQ5wzWNOAfbFnALsizkFnF8h9csvv8jPz6/SdoRSuIiT0/mtxry8vPhLFLAjT09P5hRgR8wpwL6YU4B9Madwo6vKIhc2OgcAAAAAAIDDEUoBAAAAAADA4QilcBGr1aoZM2bIarXWdCnAdYE5BdgXcwqwL+YUYF/MKaDqLMYfvZ8PAAAAAAAAsDNWSgEAAAAAAMDhCKUAAAAAAADgcIRSAAAAAAAAcDhCKQAAAAAAADgcodQN4I033lBgYKBcXV3VqVMnbd++vdL2H330kVq1aiVXV1cFBwcrKSnJ5rxhGJo+fbqaNGkiNzc3hYeHKysrqzqHANQq9p5TCQkJ6t27t3x8fGSxWJSRkVGN1QO1jz3nVElJiSZPnqzg4GDVq1dPfn5+euyxx3TixInqHgZQq9j736qZM2eqVatWqlevnm666SaFh4dr27Zt1TkEoFax95z6vaioKFksFi1YsMDOVQO1H6HUde6DDz7QxIkTNWPGDO3atUshISGKiIjQyZMnL9l+8+bNGjZsmEaNGqX09HQNGDBAAwYM0N69e802r7zyihYuXKilS5dq27ZtqlevniIiIvTrr786alhAjamOOVVYWKj77rtPc+fOddQwgFrD3nOqqKhIu3bt0rRp07Rr1y4lJCQoMzNT/fv3d+SwgBpVHf9WtWjRQosXL9aePXuUlpamwMBA9e7dW6dOnXLUsIAaUx1zqtynn36qrVu3ys/Pr7qHAdROBq5r99xzj/G3v/3N/L60tNTw8/MzYmNjL9l+yJAhRt++fW2OderUyXjqqacMwzCMsrIyw9fX13j11VfN82fOnDGsVquxatWqahgBULvYe0793uHDhw1JRnp6ul1rBmqz6pxT5bZv325IMo4cOWKfooFazhHzKi8vz5BkJCcn26dooBarrjn1ww8/GE2bNjX27t1rNGvWzJg/f77dawdqO1ZKXceKi4u1c+dOhYeHm8ecnJwUHh6uLVu2XLLPli1bbNpLUkREhNn+8OHDys3NtWnj5eWlTp06VXhN4HpRHXMKuJE5ak7l5eXJYrGoQYMGdqkbqM0cMa+Ki4u1fPlyeXl5KSQkxH7FA7VQdc2psrIyDR8+XJMmTVKbNm2qp3jgGkAodR376aefVFpaqsaNG9scb9y4sXJzcy/ZJzc3t9L25b9ezjWB60V1zCngRuaIOfXrr79q8uTJGjZsmDw9Pe1TOFCLVee8WrNmjTw8POTq6qr58+drw4YNuvnmm+07AKCWqa45NXfuXLm4uGjcuHH2Lxq4hhBKAQCA61JJSYmGDBkiwzC0ZMmSmi4HuOb17NlTGRkZ2rx5s/r06aMhQ4ZUuKcOgIrt3LlTr7/+uuLi4mSxWGq6HKBGEUpdx26++WY5Ozvrxx9/tDn+448/ytfX95J9fH19K21f/uvlXBO4XlTHnAJuZNU5p8oDqSNHjmjDhg2sksINozrnVb169XTbbbepc+fOevPNN+Xi4qI333zTvgMAapnqmFObNm3SyZMnFRAQIBcXF7m4uOjIkSP6xz/+ocDAwGoZB1BbEUpdx+rWrau77rpLKSkp5rGysjKlpKSoS5cul+zTpUsXm/aStGHDBrN9UFCQfH19bdrk5+dr27ZtFV4TuF5Ux5wCbmTVNafKA6msrCwlJyfLx8enegYA1EKO/LeqrKxM586du/qigVqsOubU8OHDtXv3bmVkZJgfPz8/TZo0SevWrau+wQC1UU3vtI7qtXr1asNqtRpxcXHG/v37jSeffNJo0KCBkZubaxiGYQwfPtx47rnnzPbffPON4eLiYrz22mvGgQMHjBkzZhh16tQx9uzZY7Z5+eWXjQYNGhiff/65sXv3buOhhx4ygoKCjLNnzzp8fICjVcecOn36tJGenm4kJiYakozVq1cb6enpRk5OjsPHBziavedUcXGx0b9/f+OWW24xMjIyjJycHPNz7ty5Ghkj4Gj2nlcFBQXGlClTjC1bthjZ2dnGjh07jBEjRhhWq9XYu3dvjYwRcKTq+P/fhXj7Hm5UhFI3gEWLFhkBAQFG3bp1jXvuucfYunWreS40NNR4/PHHbdp/+OGHRosWLYy6desabdq0MRITE23Ol5WVGdOmTTMaN25sWK1WIywszMjMzHTEUIBawd5z6u233zYkXfSZMWOGA0YD1Dx7zqnDhw9fcj5JMr766isHjQioefacV2fPnjUGDhxo+Pn5GXXr1jWaNGli9O/f39i+fbujhgPUOHv//+9ChFK4UVkMwzBqZo0WAAAAAAAAblTsKQUAAAAAAACHI5QCAAAAAACAwxFKAQAAAAAAwOEIpQAAAAAAAOBwhFIAAAAAAABwOEIpAAAAAAAAOByhFAAAAAAAAByOUAoAAAAAAAAORygFAAAAAAAAhyOUAgAAAAAAgMMRSgEAAAAAAMDhCKUAAAAAAADgcP8HzkeTe/OATdAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_names = (\n",
    "    [f'mfcc_{i}' for i in range(13)] +\n",
    "    [f'delta_mfcc_{i}' for i in range(13)] +\n",
    "    [f'chroma_{i}' for i in range(12)] +\n",
    "    [f'contrast_{i}' for i in range(7)] +\n",
    "    [f'tonnetz_{i}' for i in range(6)] +\n",
    "    ['zcr', 'rms', 'tempo'] +\n",
    "    ['spec_centroid', 'spec_bandwidth', 'spec_rolloff', 'spec_flatness'] +\n",
    "    ['pitch_std', 'pitch_mean', 'pitch_entropy', 'max_pitch_excursion'] +\n",
    "    ['onset_sharpness', 'pause_regularity'] +\n",
    "    ['low_freq_energy', 'hnr', 'energy_entropy'] +\n",
    "    ['source_code']\n",
    ")\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Now use meaningful names\n",
    "indices = np.argsort(importances)[-20:][::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.barh(range(20), importances[indices], align='center')\n",
    "plt.yticks(range(20), [feature_names[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c87f226-950e-40db-87bd-b239f4624b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75        75\n",
      "           1       0.78      0.83      0.81        75\n",
      "           2       0.42      0.41      0.42        39\n",
      "           3       0.68      0.55      0.61        75\n",
      "           4       0.76      0.69      0.73        75\n",
      "           5       0.74      0.74      0.74        38\n",
      "           6       0.62      0.67      0.65        75\n",
      "           7       0.51      0.67      0.58        39\n",
      "\n",
      "    accuracy                           0.68       491\n",
      "   macro avg       0.66      0.66      0.66       491\n",
      "weighted avg       0.68      0.68      0.68       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "importances = model.feature_importances_\n",
    "median_importance = np.median(importances)\n",
    "\n",
    "important_indices = np.where(importances > median_importance)[0]\n",
    "\n",
    "X_important = X[:, important_indices]  # assuming X is a NumPy array\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split again using filtered features\n",
    "X_train_imp, X_test_imp, y_train, y_test = train_test_split(\n",
    "    X_important, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train new model\n",
    "model_imp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_imp.fit(X_train_imp, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model_imp.predict(X_test_imp)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c18e5112-0339-49d8-a418-a87ee7937f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Features used for new model (above median importance):\n",
      "['mfcc_0', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'delta_mfcc_0', 'chroma_0', 'chroma_3', 'chroma_5', 'chroma_7', 'chroma_8', 'chroma_9', 'chroma_10', 'contrast_0', 'contrast_1', 'contrast_2', 'contrast_3', 'contrast_5', 'tonnetz_0', 'tonnetz_1', 'tonnetz_2', 'tonnetz_3', 'tonnetz_4', 'tonnetz_5', 'rms', 'spec_bandwidth', 'spec_rolloff', 'pitch_entropy', 'onset_sharpness', 'low_freq_energy', 'hnr', 'energy_entropy']\n"
     ]
    }
   ],
   "source": [
    "important_feature_names = [feature_names[i] for i in important_indices]\n",
    "print(\"🧠 Features used for new model (above median importance):\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4503fcd9-ee6a-4fba-a78c-509874a79ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion_from_filename(filename):\n",
    "    emotion_code = int(filename.split(\"-\")[2])\n",
    "    emotion_map = {\n",
    "        1: 'neutral',\n",
    "        2: 'calm',\n",
    "        3: 'happy',\n",
    "        4: 'sad',\n",
    "        5: 'angry',\n",
    "        6: 'fearful',\n",
    "        7: 'disgust',\n",
    "        8: 'surprised'\n",
    "    }\n",
    "    return emotion_map.get(emotion_code, 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0072f846-3f06-4fab-9e6d-dc204e993618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=full_data, x='emotion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Emotion Label Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccb96d51-e54b-4de0-9265-ab89fd02ccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "calm         376\n",
       "happy        376\n",
       "sad          376\n",
       "angry        376\n",
       "fearful      376\n",
       "disgust      192\n",
       "surprised    192\n",
       "neutral      188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['emotion'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8edcc2c2-b774-476f-802f-52bbe5f928ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack(full_data['features'].dropna().values)\n",
    "y = full_data['emotion'][full_data['features'].notnull()].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c4ca312-dc5e-4d9a-8c74-ba6794e9e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [301 301 153 301 301 150 301 153]\n",
      "After SMOTE: [301 301 301 301 301 301 301 301]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(pd.Series(y_train).astype('category').cat.codes))\n",
    "print(\"After SMOTE:\", np.bincount(pd.Series(y_train_balanced).astype('category').cat.codes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "338d1be3-2ac3-4475-91c6-2a25dbf5c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.79      0.73      0.76        75\n",
      "        calm       0.79      0.88      0.83        75\n",
      "     disgust       0.38      0.46      0.42        39\n",
      "     fearful       0.68      0.52      0.59        75\n",
      "       happy       0.76      0.64      0.70        75\n",
      "     neutral       0.69      0.87      0.77        38\n",
      "         sad       0.71      0.64      0.67        75\n",
      "   surprised       0.50      0.69      0.58        39\n",
      "\n",
      "    accuracy                           0.68       491\n",
      "   macro avg       0.66      0.68      0.66       491\n",
      "weighted avg       0.69      0.68      0.68       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ae5d5a8-e933-4a0d-a489-c4d90af71605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.75      0.75        75\n",
      "        calm       0.75      0.87      0.80        75\n",
      "     disgust       0.35      0.51      0.42        39\n",
      "     fearful       0.64      0.45      0.53        75\n",
      "       happy       0.76      0.59      0.66        75\n",
      "     neutral       0.65      0.79      0.71        38\n",
      "         sad       0.66      0.56      0.60        75\n",
      "   surprised       0.43      0.56      0.49        39\n",
      "\n",
      "    accuracy                           0.64       491\n",
      "   macro avg       0.62      0.63      0.62       491\n",
      "weighted avg       0.65      0.64      0.64       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Baseline model to get feature importances\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "importances = base_model.feature_importances_\n",
    "feature_names = X.columns if isinstance(X, pd.DataFrame) else [f\"feat_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# 3. Select features above mean or median importance\n",
    "threshold = np.mean(importances)  # or use np.median(importances)\n",
    "selected_indices = np.where(importances > threshold)[0]\n",
    "selected_features = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "# 4. Filter dataset by selected features\n",
    "X_train_selected = X_train[:, selected_indices] if isinstance(X_train, np.ndarray) else X_train[selected_features]\n",
    "X_test_selected = X_test[:, selected_indices] if isinstance(X_test, np.ndarray) else X_test[selected_features]\n",
    "\n",
    "# 5. Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# 6. Train final model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 7. Predict & Evaluate\n",
    "y_pred = model.predict(X_test_selected)\n",
    "print(\"🎯 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cf417d0-cf5e-444a-b4e5-b7f825bddd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2452, 68)\n",
      "Sample Feature Vector (First Row):\n",
      "[-6.70195435e+02  6.50638504e+01  8.88954341e-01  1.47159786e+01\n",
      "  9.18216515e+00  6.60574794e-01 -3.84683585e+00 -3.58394599e+00\n",
      " -1.29590063e+01 -3.30013299e+00  9.10779595e-01 -3.59703588e+00\n",
      "  2.37627435e+00 -4.79661534e-03 -6.77440967e-03 -6.74729561e-03\n",
      " -6.70217862e-03 -6.63951505e-03 -6.55954005e-03 -6.46282732e-03\n",
      " -6.34998130e-03 -6.22162083e-03 -6.07853336e-03 -5.92157943e-03\n",
      " -5.75155905e-03 -5.56945708e-03  6.25734925e-01  5.94129920e-01\n",
      "  5.20973027e-01  5.15360773e-01  5.28446078e-01  5.05175412e-01\n",
      "  5.47890604e-01  6.11973763e-01  6.39426827e-01  6.05721235e-01\n",
      "  6.00476801e-01  5.51562428e-01  2.09453247e+01  1.18614319e+01\n",
      "  1.57830801e+01  1.43947067e+01  1.53847683e+01  1.74785937e+01\n",
      "  4.59629738e+01 -4.95951333e-02  2.56191153e-02 -4.41293368e-02\n",
      " -8.04766070e-02  2.37735057e-02  9.14605229e-03  3.50109762e-01\n",
      "  2.66674696e-03  8.07495117e+01  3.17567553e+03  2.52715125e+03\n",
      "  5.84475199e+03  1.55038655e-01  3.28650590e+02  1.89209055e+02\n",
      "  1.73697072e+00  1.95386076e+03  2.10883350e+01  0.00000000e+00\n",
      "  2.02215880e-01  3.30150753e-01  3.64445162e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Sample Feature Vector (First Row):\")\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5acc1626-f483-493f-9c84-f7e321ea42b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a DataFrame with emotion and source_code from full_data\n",
    "df_dist = pd.DataFrame({\n",
    "    'emotion': full_data['emotion'],\n",
    "    'source_code': full_data['features'].apply(lambda x: x[-1] if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "})\n",
    "\n",
    "# Step 2: Map source_code to readable labels\n",
    "df_dist['source_type'] = df_dist['source_code'].map({0: 'speech', 1: 'song'})\n",
    "\n",
    "# Step 3: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_dist, x='emotion', hue='source_type', palette='Set2')\n",
    "plt.title('Speech vs Song Distribution per Emotion')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Source Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8bd418ca-600f-4d4e-8080-c3ce8a191560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\n",
      "Last feature (should be source_code): 1.0\n",
      "\n",
      "File: audio_speech_actors_01-24/Actor_01/03-01-01-01-01-02-01.wav\n",
      "Last feature (should be source_code): 1.0\n",
      "\n",
      "File: audio_speech_actors_01-24/Actor_01/03-01-01-01-02-01-01.wav\n",
      "Last feature (should be source_code): 1.0\n",
      "\n",
      "File: audio_speech_actors_01-24/Actor_01/03-01-01-01-02-02-01.wav\n",
      "Last feature (should be source_code): 1.0\n",
      "\n",
      "File: audio_speech_actors_01-24/Actor_01/03-01-02-01-01-01-01.wav\n",
      "Last feature (should be source_code): 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"File: {full_data.iloc[i]['file_path']}\")\n",
    "    print(f\"Last feature (should be source_code): {full_data.iloc[i]['features'][-1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "082c4922-a69d-4566-9ade-6cacd1dcdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_source_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Returns 'speech' or 'song' based on file naming or folder logic.\n",
    "    Update this if your structure changes.\n",
    "    \"\"\"\n",
    "    file_path = str(file_path).lower()\n",
    "    if \"song\" in file_path or \"sung\" in file_path:\n",
    "        return \"song\"\n",
    "    else:\n",
    "        return \"speech\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6980e901-1ba6-4768-8ca9-e04b8500a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Source Type Distribution:\n",
      "source\n",
      "speech    1440\n",
      "song      1012\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "full_data['source'] = full_data['file_path'].apply(detect_source_from_path)\n",
    "\n",
    "# Count\n",
    "source_distribution = full_data['source'].value_counts()\n",
    "print(\"📊 Source Type Distribution:\")\n",
    "print(source_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83b7d2c0-b810-433f-a5f8-704084cd5a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=full_data, x='source')\n",
    "plt.title(\"Speech vs Song Distribution\")\n",
    "plt.xlabel(\"Source Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a071cbfd-7a46-4a16-9799-bbafd07d4662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=966\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17208\\3950322558.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tempo = float(tempo)\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=978\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=955\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=920\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=989\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1001\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=932\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=943\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=897\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=886\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=851\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=863\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=874\n",
      "  warnings.warn(\n",
      "100%|██████████| 2452/2452 [24:06<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Assuming 'source' and 'file_path' columns are now present and correct in full_data\n",
    "full_data['features'] = full_data.progress_apply(\n",
    "    lambda row: extract_all_audio_features(row['file_path'], source=row['source']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38e03df6-ab61-46a4-bae2-e452638fa354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.71      0.80      0.75        75\n",
      "        calm       0.76      0.91      0.83        75\n",
      "     disgust       0.46      0.49      0.47        39\n",
      "     fearful       0.65      0.45      0.54        75\n",
      "       happy       0.73      0.68      0.70        75\n",
      "     neutral       0.82      0.74      0.78        38\n",
      "         sad       0.66      0.68      0.67        75\n",
      "   surprised       0.56      0.62      0.59        39\n",
      "\n",
      "    accuracy                           0.68       491\n",
      "   macro avg       0.67      0.67      0.67       491\n",
      "weighted avg       0.68      0.68      0.68       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Train classifier with balanced class weight\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',  # ✅ auto handles imbalance\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "print(\"🎯 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1b67a8e-411d-4c8b-99e2-9ecc74c6a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 03-01-06-01-02-01-12.wav → Emotion: fear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Emotion mapping (from RAVDESS docs)\n",
    "emotion_map = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fear\", 7: \"disgust\", 8: \"surprised\"\n",
    "}\n",
    "\n",
    "# Extract emotion from filename\n",
    "def get_emotion(filename):\n",
    "    parts = filename.split('-')\n",
    "    emotion_code = int(parts[2])  # 3rd part is emotion\n",
    "    return emotion_map[emotion_code]\n",
    "\n",
    "# Test on a sample filename\n",
    "filename = \"03-01-06-01-02-01-12.wav\"\n",
    "print(f\"File: {filename} → Emotion: {get_emotion(filename)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e91ce4d-1d63-4e7d-8b96-2513e1ddf0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "speech    1440\n",
      "song      1012\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Emotion mapping (same for speech & song)\n",
    "emotion_map = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fear\", 7: \"disgust\", 8: \"surprised\"\n",
    "}\n",
    "\n",
    "def get_emotion(filename):\n",
    "    parts = filename.split('-')\n",
    "    emotion_code = int(parts[2])  # 3rd part is emotion\n",
    "    return emotion_map[emotion_code]\n",
    "\n",
    "# Load all files from BOTH folders\n",
    "def load_data(folder_path):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                emotion = get_emotion(file)\n",
    "                data.append({\n",
    "                    \"file\": os.path.join(root, file),\n",
    "                    \"emotion\": emotion,\n",
    "                    \"type\": \"speech\" if \"speech\" in folder_path else \"song\"\n",
    "                })\n",
    "    return data\n",
    "\n",
    "# Load speech and song data\n",
    "speech_data = load_data(\"audio_speech_actors_01-24\")\n",
    "song_data = load_data(\"audio_song_actors_01-24\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.DataFrame(speech_data + song_data)\n",
    "print(df[\"type\"].value_counts())  # Check balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8c30349f-6def-4c03-85fa-1d9157147e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, duration=3.0)  # Trim to 3 seconds\n",
    "    \n",
    "    # MFCC (40 coefficients) + Deltas (1st and 2nd order)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    # Chroma (12 pitch classes)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    \n",
    "    # Spectral Features\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    \n",
    "    # Tonnetz (Tonal Centroid)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    \n",
    "    # Stack all features and take mean over time\n",
    "    features = np.vstack([\n",
    "        mfcc, mfcc_delta, mfcc_delta2,\n",
    "        chroma,\n",
    "        spectral_contrast,\n",
    "        spectral_rolloff,\n",
    "        tonnetz\n",
    "    ])\n",
    "    \n",
    "    return np.mean(features.T, axis=0)  # Mean across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d9adc2b5-c760-4e82-83d8-15465e64c708",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     27\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m---> 28\u001b[0m     file_meta \u001b[38;5;241m=\u001b[39m extract_metadata_from_filename(file)\n\u001b[0;32m     29\u001b[0m     file_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file_path\n\u001b[0;32m     30\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mappend(file_meta)\n",
      "Cell \u001b[1;32mIn[96], line 14\u001b[0m, in \u001b[0;36mextract_metadata_from_filename\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extracts emotion, actor ID, etc. from RAVDESS filenames.\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m parts \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(parts[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m: emotion_map[\u001b[38;5;28mint\u001b[39m(parts[\u001b[38;5;241m2\u001b[39m])],\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor\u001b[39m\u001b[38;5;124m\"\u001b[39m: parts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Last part before .wav\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodality\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msong\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m03\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 03 = song, 01 = speech\u001b[39;00m\n\u001b[0;32m     18\u001b[0m }\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define emotion mapping (from RAVDESS documentation)\n",
    "emotion_map = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fear\", 7: \"disgust\", 8: \"surprised\"\n",
    "}\n",
    "\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"Extracts emotion, actor ID, etc. from RAVDESS filenames.\"\"\"\n",
    "    parts = filename.split('-')\n",
    "    return {\n",
    "        \"emotion_code\": int(parts[2]),\n",
    "        \"emotion\": emotion_map[int(parts[2])],\n",
    "        \"actor\": parts[-1].split('.')[0],  # Last part before .wav\n",
    "        \"modality\": \"song\" if parts[0] == \"03\" else \"speech\"  # 03 = song, 01 = speech\n",
    "    }\n",
    "\n",
    "# Scan folders and build metadata\n",
    "metadata = []\n",
    "base_path = \".\"  # Change this to your actual path if needed\n",
    "\n",
    "for root, _, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_meta = extract_metadata_from_filename(file)\n",
    "            file_meta[\"file_path\"] = file_path\n",
    "            metadata.append(file_meta)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metadata)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d1b3c0f-819e-40c0-8b0c-ca1a6d3b3cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2452 valid audio files.\n",
      "   emotion_code  emotion actor modality  \\\n",
      "0             1  neutral    01     song   \n",
      "1             1  neutral    01     song   \n",
      "2             1  neutral    01     song   \n",
      "3             1  neutral    01     song   \n",
      "4             2     calm    01     song   \n",
      "\n",
      "                                           file_path  \n",
      "0  .\\Audio_Song_Actors_01-24\\Actor_01\\03-02-01-01...  \n",
      "1  .\\Audio_Song_Actors_01-24\\Actor_01\\03-02-01-01...  \n",
      "2  .\\Audio_Song_Actors_01-24\\Actor_01\\03-02-01-01...  \n",
      "3  .\\Audio_Song_Actors_01-24\\Actor_01\\03-02-01-01...  \n",
      "4  .\\Audio_Song_Actors_01-24\\Actor_01\\03-02-02-01...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Emotion mapping (from RAVDESS docs)\n",
    "emotion_map = {\n",
    "    1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "    5: \"angry\", 6: \"fear\", 7: \"disgust\", 8: \"surprised\"\n",
    "}\n",
    "\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"Safely extracts metadata from RAVDESS filenames.\"\"\"\n",
    "    parts = filename.split('-')\n",
    "    \n",
    "    # Skip files that don't match the expected format\n",
    "    if len(parts) < 7 or not parts[-1].endswith('.wav'):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            \"emotion_code\": int(parts[2]),\n",
    "            \"emotion\": emotion_map[int(parts[2])],\n",
    "            \"actor\": parts[-1].split('.')[0],  # Last part before .wav\n",
    "            \"modality\": \"song\" if parts[0] == \"03\" else \"speech\"\n",
    "        }\n",
    "    except (ValueError, KeyError):\n",
    "        return None  # Skip files with invalid codes\n",
    "\n",
    "# Scan folders and build metadata\n",
    "metadata = []\n",
    "base_path = \".\"  # Change to your actual path\n",
    "\n",
    "for root, _, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_meta = extract_metadata_from_filename(file)\n",
    "            if file_meta:  # Only add valid files\n",
    "                file_meta[\"file_path\"] = file_path\n",
    "                metadata.append(file_meta)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metadata)\n",
    "\n",
    "# Check results\n",
    "print(f\"Found {len(df)} valid audio files.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5cbba484-4a90-423c-b5a3-3f32a3c34e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2452 valid RAVDESS files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "base_path = \".\"  # Change to your dataset path if needed\n",
    "valid_files = []\n",
    "\n",
    "for root, _, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\") and re.match(r\"^\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}\\.wav$\", file):\n",
    "            valid_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(valid_files)} valid RAVDESS files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de500624-383f-4a6a-9399-dbd55fcff478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, duration=3.0)\n",
    "    \n",
    "    # 1. MFCCs + Deltas\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    # 2. Spectral Features (reshape to 1D arrays)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]  # Take the first channel\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr).mean(axis=1)  # Mean over frames\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "    \n",
    "    # 3. Pitch/Tonal\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr).mean(axis=1)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).mean(axis=1)\n",
    "    pitch = librosa.piptrack(y=y, sr=sr)[0]\n",
    "    pitch = pitch[pitch > 0]  # Ignore silent frames\n",
    "    \n",
    "    # 4. Rhythm/Energy\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    tempogram = librosa.feature.tempogram(y=y, sr=sr).mean(axis=1)\n",
    "    \n",
    "    # Combine all features (ensure all are 1D arrays)\n",
    "    features = np.concatenate([\n",
    "        mfcc.mean(axis=1),          # Shape: (n_mfcc,)\n",
    "        mfcc_delta.mean(axis=1),    # Shape: (n_mfcc,)\n",
    "        mfcc_delta2.mean(axis=1),   # Shape: (n_mfcc,)\n",
    "        np.array([spectral_centroid.mean()]),  # Force 1D\n",
    "        np.array([spectral_bandwidth.mean()]),\n",
    "        spectral_contrast,          # Already 1D from mean(axis=1)\n",
    "        np.array([spectral_rolloff.mean()]),\n",
    "        chroma,                     # Shape: (12,)\n",
    "        tonnetz,                    # Shape: (6,)\n",
    "        np.array([np.mean(pitch)] if len(pitch) > 0 else [0]),  # Handle empty pitch\n",
    "        np.array([rms.mean()]),\n",
    "        np.array([zcr.mean()]),\n",
    "        tempogram                   # Shape: (384,)\n",
    "    ])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79427cec-751b-48d5-af74-8716f1504804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "914538f9-ac3a-4cf4-ab3d-4d6703c8e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting advanced features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n",
      "100%|██████████| 2452/2452 [23:27<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to X_advanced_features.npy and y_emotion_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load your metadata DataFrame (df)\n",
    "# df = pd.read_csv(\"ravdess_metadata.csv\")  # Uncomment if needed\n",
    "\n",
    "# Extract and save features\n",
    "print(\"Extracting advanced features...\")\n",
    "X_advanced = []\n",
    "for file_path in tqdm(df[\"file_path\"]):\n",
    "    X_advanced.append(extract_advanced_features(file_path))\n",
    "\n",
    "X_advanced = np.array(X_advanced)\n",
    "y = df[\"emotion\"].values\n",
    "\n",
    "# Save to disk (so you don't recompute)\n",
    "np.save(\"X_advanced_features.npy\", X_advanced)\n",
    "np.save(\"y_emotion_labels.npy\", y)\n",
    "print(\"Saved features to X_advanced_features.npy and y_emotion_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8501e40f-b359-45a5-8e9f-764dbc64f348",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load your labels\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_emotion_labels.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y)\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(fid, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[0;32m    457\u001b[0m                                  pickle_kwargs\u001b[38;5;241m=\u001b[39mpickle_kwargs,\n\u001b[0;32m    458\u001b[0m                                  max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\format.py:795\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your labels\n",
    "y = np.load(\"y_emotion_labels.npy\")\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16ba417b-97bb-4364-917a-768ed7e0ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels sample: ['neutral' 'neutral' 'neutral' 'neutral' 'calm']\n",
      "Encoded labels sample: [5 5 5 5 1]\n",
      "Saved 2452 numeric labels\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your pre-extracted features\n",
    "X = np.load(\"X_advanced_features.npy\")\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# TEMPORARY: Load original labels with allow_pickle\n",
    "y_strings = np.load(\"y_emotion_labels.npy\", allow_pickle=True)  # Only time we use this\n",
    "\n",
    "# Convert to integers\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_strings)\n",
    "\n",
    "# Save as NEW numeric labels\n",
    "np.save(\"y_emotion_labels_encoded.npy\", y_encoded)\n",
    "\n",
    "# Verify\n",
    "print(\"Original labels sample:\", y_strings[:5])\n",
    "print(\"Encoded labels sample:\", y_encoded[:5])\n",
    "print(f\"Saved {len(y_encoded)} numeric labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c495d782-448b-48cb-85b7-cadd27175eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2452, 535), Labels: (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SAFE loading from now on\n",
    "X = np.load(\"X_advanced_features.npy\")\n",
    "y = np.load(\"y_emotion_labels_encoded.npy\")  # No allow_pickle needed!\n",
    "\n",
    "print(f\"Features: {X.shape}, Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd70824c-0a80-4dfa-9f88-3afff8e68d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced features: 105/535 (keeps 95% variance)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale BEFORE PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your original (535 features)\n",
    "\n",
    "# Automatically retain 95% variance\n",
    "pca = PCA(n_components=0.95, random_state=42)  \n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "print(f\"Reduced features: {X_reduced.shape[1]}/535 (keeps 95% variance)\")\n",
    "\n",
    "# Plot variance to verify\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c8f4aba4-66f8-4368-9cea-cf672c5e9fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1961, 105), Val: (491, 105)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_reduced, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # Preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "397b3736-10fa-4b36-bc42-4eded236d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,424</span> (251.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,424\u001b[0m (251.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,168</span> (250.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,168\u001b[0m (250.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization, MaxPooling1D\n",
    "\n",
    "model = Sequential([\n",
    "    # CNN Block (input_shape matches your PCA-reduced features: 105)\n",
    "    Conv1D(128, 5, activation='relu', input_shape=(105, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # LSTM Block\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    \n",
    "    # Classifier (ensure output matches number of classes)\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y)), activation='softmax')  # Single output layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd8b32d6-b7b6-4ff1-81ad-f42dacb7ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: {0: 301, 1: 301, 2: 153, 3: 301, 4: 301, 5: 150, 6: 301, 7: 153}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains integer class labels (not one-hot encoded)\n",
    "unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique_classes, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1412cc8e-1a9f-490a-987e-46531d72334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.8143687707641196, 1: 0.8143687707641196, 2: 1.602124183006536, 3: 0.8143687707641196, 4: 0.8143687707641196, 5: 1.6341666666666668, 6: 0.8143687707641196, 7: 1.602124183006536}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# For integer labels (if y_train is one-hot encoded, use np.argmax(y_train, axis=1))\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Convert to a dictionary format expected by Keras\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e3c680ea-ccea-425d-a6b4-06498120c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.2703 - loss: 1.7670 - val_accuracy: 0.1833 - val_loss: 2.0357\n",
      "Epoch 2/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.2750 - loss: 1.7602 - val_accuracy: 0.1752 - val_loss: 2.0217\n",
      "Epoch 3/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.2749 - loss: 1.7291 - val_accuracy: 0.1833 - val_loss: 2.0345\n",
      "Epoch 4/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.2697 - loss: 1.7574 - val_accuracy: 0.1731 - val_loss: 2.0832\n",
      "Epoch 5/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.2704 - loss: 1.7481 - val_accuracy: 0.1813 - val_loss: 2.0279\n",
      "Epoch 6/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.2927 - loss: 1.7073 - val_accuracy: 0.1976 - val_loss: 2.0462\n",
      "Epoch 7/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.2974 - loss: 1.6659 - val_accuracy: 0.1914 - val_loss: 2.0517\n",
      "Epoch 8/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.3064 - loss: 1.6392 - val_accuracy: 0.1752 - val_loss: 2.0919\n",
      "Epoch 9/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.2947 - loss: 1.6447 - val_accuracy: 0.1996 - val_loss: 2.1083\n",
      "Epoch 10/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.3275 - loss: 1.6035 - val_accuracy: 0.1813 - val_loss: 2.0627\n",
      "Epoch 11/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.3200 - loss: 1.5817 - val_accuracy: 0.1853 - val_loss: 2.1556\n",
      "Epoch 12/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.3497 - loss: 1.5372 - val_accuracy: 0.2037 - val_loss: 2.1227\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    np.expand_dims(X_train, axis=2),  # Shape: (1961, 105, 1)\n",
    "    y_train,\n",
    "    validation_data=(np.expand_dims(X_val, axis=2), y_val),  # Shape: (491, 105, 1)\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,  # From earlier\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=10),\n",
    "        ModelCheckpoint(\"best_model_reduced.keras\", save_best_only=True)\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ce98419b-5516-4cde-9c72-6d8dd393341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature mean: 0.003216799623119027, std: 2.1952224010058727\n",
      "{0: 301, 1: 301, 2: 153, 3: 301, 4: 301, 5: 150, 6: 301, 7: 153}\n"
     ]
    }
   ],
   "source": [
    "# Check feature statistics\n",
    "print(f\"Feature mean: {np.mean(X_train)}, std: {np.std(X_train)}\")\n",
    "\n",
    "# Verify label distribution again\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cd0a9404-fb25-4307-a0e0-7d1d47ab5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: {0: 301, 1: 301, 2: 153, 3: 301, 4: 301, 5: 150, 6: 301, 7: 153}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c00ad356-84b3-4bdc-b474-c9cf62a0b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(105, 1)),  # Simple flattening\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "507ce2f8-0f9c-4e54-8e00-5390ba8c9da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.1647 - loss: 2.7735 - val_accuracy: 0.2118 - val_loss: 2.1185\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3071 - loss: 1.8222 - val_accuracy: 0.3218 - val_loss: 1.8189\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4456 - loss: 1.4661 - val_accuracy: 0.3829 - val_loss: 1.6398\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5416 - loss: 1.2531 - val_accuracy: 0.4358 - val_loss: 1.5290\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 1.1216 - val_accuracy: 0.4766 - val_loss: 1.4315\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6505 - loss: 0.9970 - val_accuracy: 0.4949 - val_loss: 1.3887\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6894 - loss: 0.9196 - val_accuracy: 0.5193 - val_loss: 1.3374\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6882 - loss: 0.8940 - val_accuracy: 0.5193 - val_loss: 1.3059\n",
      "Epoch 9/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7248 - loss: 0.8083 - val_accuracy: 0.5397 - val_loss: 1.2895\n",
      "Epoch 10/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7427 - loss: 0.7690 - val_accuracy: 0.5275 - val_loss: 1.2761\n",
      "Epoch 11/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7620 - loss: 0.7091 - val_accuracy: 0.5336 - val_loss: 1.2682\n",
      "Epoch 12/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.6602 - val_accuracy: 0.5458 - val_loss: 1.2674\n",
      "Epoch 13/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7794 - loss: 0.6344 - val_accuracy: 0.5499 - val_loss: 1.2750\n",
      "Epoch 14/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8068 - loss: 0.6124 - val_accuracy: 0.5499 - val_loss: 1.2654\n",
      "Epoch 15/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8134 - loss: 0.5947 - val_accuracy: 0.5336 - val_loss: 1.2753\n",
      "Epoch 16/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.5564 - val_accuracy: 0.5560 - val_loss: 1.2755\n",
      "Epoch 17/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8350 - loss: 0.5477 - val_accuracy: 0.5540 - val_loss: 1.2845\n",
      "Epoch 18/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8487 - loss: 0.5056 - val_accuracy: 0.5560 - val_loss: 1.2947\n",
      "Epoch 19/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8402 - loss: 0.5058 - val_accuracy: 0.5519 - val_loss: 1.3047\n",
      "Epoch 20/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8695 - loss: 0.4631 - val_accuracy: 0.5682 - val_loss: 1.3243\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d960a041-181b-42ae-97fb-18963cbf0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "X_val_normalized = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "\n",
    "# Reshape back to (samples, timesteps, channels)\n",
    "X_train = np.expand_dims(X_train_normalized, axis=2)\n",
    "X_val = np.expand_dims(X_val_normalized, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "527e8537-b8f3-4225-94ce-7a0b337eb11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(105, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b80a7d54-2f1c-436e-a6e3-81cdd18c7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aeace066-6f3b-4aa6-aed7-22de204f3cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.1120 - loss: 2.0570 - val_accuracy: 0.0876 - val_loss: 2.0797 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.1079 - loss: 2.0945 - val_accuracy: 0.1039 - val_loss: 2.0763 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1254 - loss: 2.0773 - val_accuracy: 0.1181 - val_loss: 2.0726 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.1444 - loss: 2.0400 - val_accuracy: 0.1202 - val_loss: 2.0668 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.1569 - loss: 2.0705 - val_accuracy: 0.1202 - val_loss: 2.0629 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1410 - loss: 2.0543 - val_accuracy: 0.1141 - val_loss: 2.0553 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1475 - loss: 2.0466 - val_accuracy: 0.1222 - val_loss: 2.0488 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.1688 - loss: 2.0620 - val_accuracy: 0.1181 - val_loss: 2.0421 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1589 - loss: 2.0225 - val_accuracy: 0.1161 - val_loss: 2.0356 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.1620 - loss: 2.0456 - val_accuracy: 0.1283 - val_loss: 2.0401 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.1657 - loss: 1.9926 - val_accuracy: 0.1202 - val_loss: 2.0396 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1544 - loss: 2.0058 - val_accuracy: 0.1527 - val_loss: 2.0368 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.1953 - loss: 1.9810 - val_accuracy: 0.1385 - val_loss: 2.0393 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m60/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1641 - loss: 1.9921\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.1645 - loss: 1.9922 - val_accuracy: 0.1853 - val_loss: 2.0366 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.1721 - loss: 2.0037 - val_accuracy: 0.1792 - val_loss: 2.0324 - learning_rate: 5.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.1672 - loss: 2.0004 - val_accuracy: 0.1792 - val_loss: 2.0314 - learning_rate: 5.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.1765 - loss: 1.9818 - val_accuracy: 0.1711 - val_loss: 2.0335 - learning_rate: 5.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1821 - loss: 1.9807 - val_accuracy: 0.1711 - val_loss: 2.0327 - learning_rate: 5.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.1778 - loss: 2.0139 - val_accuracy: 0.1772 - val_loss: 2.0321 - learning_rate: 5.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.1904 - loss: 1.9865 - val_accuracy: 0.1813 - val_loss: 2.0331 - learning_rate: 5.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.1542 - loss: 1.9941 - val_accuracy: 0.1833 - val_loss: 2.0297 - learning_rate: 5.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.1901 - loss: 1.9804 - val_accuracy: 0.1833 - val_loss: 2.0352 - learning_rate: 5.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.1826 - loss: 2.0033 - val_accuracy: 0.1833 - val_loss: 2.0301 - learning_rate: 5.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.1789 - loss: 1.9622 - val_accuracy: 0.1813 - val_loss: 2.0305 - learning_rate: 5.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.1845 - loss: 1.9836 - val_accuracy: 0.1874 - val_loss: 2.0265 - learning_rate: 5.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.1947 - loss: 1.9828 - val_accuracy: 0.1853 - val_loss: 2.0350 - learning_rate: 5.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.1897 - loss: 1.9714 - val_accuracy: 0.1894 - val_loss: 2.0283 - learning_rate: 5.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.1607 - loss: 1.9989 - val_accuracy: 0.1792 - val_loss: 2.0299 - learning_rate: 5.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.2129 - loss: 1.9481 - val_accuracy: 0.1853 - val_loss: 2.0309 - learning_rate: 5.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1818 - loss: 1.9876\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.1818 - loss: 1.9874 - val_accuracy: 0.1874 - val_loss: 2.0326 - learning_rate: 5.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.1633 - loss: 1.9666 - val_accuracy: 0.1874 - val_loss: 2.0326 - learning_rate: 2.5000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.1867 - loss: 1.9611 - val_accuracy: 0.1853 - val_loss: 2.0264 - learning_rate: 2.5000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.1839 - loss: 1.9928 - val_accuracy: 0.1874 - val_loss: 2.0293 - learning_rate: 2.5000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.2192 - loss: 1.9530 - val_accuracy: 0.1833 - val_loss: 2.0277 - learning_rate: 2.5000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1817 - loss: 1.9674\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.1817 - loss: 1.9675 - val_accuracy: 0.1853 - val_loss: 2.0312 - learning_rate: 2.5000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.1808 - loss: 1.9648 - val_accuracy: 0.1874 - val_loss: 2.0312 - learning_rate: 1.2500e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.2082 - loss: 1.9450 - val_accuracy: 0.1894 - val_loss: 2.0326 - learning_rate: 1.2500e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.2024 - loss: 1.9787 - val_accuracy: 0.1894 - val_loss: 2.0308 - learning_rate: 1.2500e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.1894 - loss: 2.0131 - val_accuracy: 0.1874 - val_loss: 2.0278 - learning_rate: 1.2500e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m61/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1793 - loss: 1.9898\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.1794 - loss: 1.9893 - val_accuracy: 0.1874 - val_loss: 2.0283 - learning_rate: 1.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.1729 - loss: 1.9539 - val_accuracy: 0.1874 - val_loss: 2.0276 - learning_rate: 6.2500e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.1785 - loss: 1.9695 - val_accuracy: 0.1853 - val_loss: 2.0264 - learning_rate: 6.2500e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.2099 - loss: 1.9651 - val_accuracy: 0.1894 - val_loss: 2.0268 - learning_rate: 6.2500e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.1846 - loss: 1.9716 - val_accuracy: 0.1853 - val_loss: 2.0266 - learning_rate: 6.2500e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m61/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2037 - loss: 1.9812\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - accuracy: 0.2029 - loss: 1.9810 - val_accuracy: 0.1853 - val_loss: 2.0265 - learning_rate: 6.2500e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.1714 - loss: 1.9939 - val_accuracy: 0.1894 - val_loss: 2.0269 - learning_rate: 3.1250e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.1921 - loss: 1.9828 - val_accuracy: 0.1894 - val_loss: 2.0275 - learning_rate: 3.1250e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=20, monitor='val_accuracy', restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.keras\", save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "097c7d50-80c9-4df2-be8d-3c9894c4f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1961, 105, 1)\n",
      "y_train samples: [3 5 0 5 1 0 1 1 7 6]\n",
      "NaN in X_train: False\n",
      "Inf in X_train: False\n"
     ]
    }
   ],
   "source": [
    "# Check if features/labels are corrupted\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train samples:\", y_train[:10])  # Should show integers 0-7\n",
    "\n",
    "# Check for NaN/inf values\n",
    "print(\"NaN in X_train:\", np.isnan(X_train).any())\n",
    "print(\"Inf in X_train:\", np.isinf(X_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "46141009-2bfe-4b7b-82dc-176819e60902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature mean/std: 0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check feature statistics\n",
    "print(\"Feature mean/std:\", X_train.mean(), X_train.std())  # Should be ~0 and ~1 after StandardScaler\n",
    "\n",
    "# Plot feature distributions\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.hist(X_train.flatten(), bins=50)\n",
    "plt.title(\"Train Features\")\n",
    "plt.subplot(122)\n",
    "plt.hist(X_val.flatten(), bins=50)\n",
    "plt.title(\"Validation Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fa361326-3c9f-422e-bfa5-aed19aab7ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Flatten features for sklearn\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Baseline model\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(X_train_flat, y_train)\n",
    "y_pred = lr.predict(X_val_flat)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_val, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2216e6bc-ab2a-419d-82e7-2a2056f89330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(128, 5, activation='relu', input_shape=(105, 1)),  # Fewer, wider filters\n",
    "    GlobalMaxPooling1D(),  # Replaces LSTM (faster and often better for audio)\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Increased from 0.0001\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aca7abe1-7c6a-4529-bb6e-0b70f0b7901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1163 - loss: 2.0966 - val_accuracy: 0.1527 - val_loss: 2.0695 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1327 - loss: 2.0673 - val_accuracy: 0.1161 - val_loss: 2.0713 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1233 - loss: 2.0838 - val_accuracy: 0.1161 - val_loss: 2.0670 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1402 - loss: 2.0753 - val_accuracy: 0.1527 - val_loss: 2.0633 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1517 - loss: 2.0629 - val_accuracy: 0.1426 - val_loss: 2.0536 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1382 - loss: 2.0392 - val_accuracy: 0.1650 - val_loss: 2.0520 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1681 - loss: 2.0586 - val_accuracy: 0.1670 - val_loss: 2.0357 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1608 - loss: 2.0708 - val_accuracy: 0.1813 - val_loss: 2.0416 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1928 - loss: 2.0201 - val_accuracy: 0.1568 - val_loss: 2.0448 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1893 - loss: 2.0121 - val_accuracy: 0.1711 - val_loss: 2.0310 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1585 - loss: 2.0096 - val_accuracy: 0.1487 - val_loss: 2.0213 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1729 - loss: 2.0054 - val_accuracy: 0.1650 - val_loss: 2.0449 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1931 - loss: 1.9835 - val_accuracy: 0.1792 - val_loss: 2.0115 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1693 - loss: 1.9992 - val_accuracy: 0.1914 - val_loss: 2.0269 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2017 - loss: 1.9603 - val_accuracy: 0.1792 - val_loss: 2.0097 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2005 - loss: 1.9926 - val_accuracy: 0.1772 - val_loss: 2.0268 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1886 - loss: 1.9925 - val_accuracy: 0.1792 - val_loss: 2.0035 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1802 - loss: 1.9811 - val_accuracy: 0.1772 - val_loss: 2.0124 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1822 - loss: 1.9737 - val_accuracy: 0.1853 - val_loss: 2.0047 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1855 - loss: 1.9616 - val_accuracy: 0.1955 - val_loss: 2.0058 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1889 - loss: 1.9573 - val_accuracy: 0.1609 - val_loss: 2.0151 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1994 - loss: 1.9534 - val_accuracy: 0.1752 - val_loss: 2.0294 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1946 - loss: 1.9582 - val_accuracy: 0.1813 - val_loss: 2.0196 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1922 - loss: 1.9340 - val_accuracy: 0.1752 - val_loss: 2.0202 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1903 - loss: 1.9258 - val_accuracy: 0.1894 - val_loss: 2.0194 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1989 - loss: 1.9217 - val_accuracy: 0.2037 - val_loss: 2.0146 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1860 - loss: 1.9600 - val_accuracy: 0.1629 - val_loss: 2.0393 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1979 - loss: 1.9242 - val_accuracy: 0.1752 - val_loss: 2.0154 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1941 - loss: 1.9341 - val_accuracy: 0.1894 - val_loss: 2.0201 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1952 - loss: 1.9339 - val_accuracy: 0.1853 - val_loss: 2.0234 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1742 - loss: 1.9501 - val_accuracy: 0.1894 - val_loss: 2.0190 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1869 - loss: 1.9414 - val_accuracy: 0.1711 - val_loss: 2.0224 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1836 - loss: 1.9189 - val_accuracy: 0.1813 - val_loss: 2.0200 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2000 - loss: 1.9142 - val_accuracy: 0.1813 - val_loss: 2.0215 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2086 - loss: 1.9154 - val_accuracy: 0.1853 - val_loss: 2.0211 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1915 - loss: 1.9421 - val_accuracy: 0.1792 - val_loss: 2.0184 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2079 - loss: 1.9293 - val_accuracy: 0.1833 - val_loss: 2.0197 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1951 - loss: 1.9439 - val_accuracy: 0.1833 - val_loss: 2.0206 - learning_rate: 6.2500e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1822 - loss: 1.9316 - val_accuracy: 0.1853 - val_loss: 2.0211 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2005 - loss: 1.9441 - val_accuracy: 0.1853 - val_loss: 2.0199 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1813 - loss: 1.9526 - val_accuracy: 0.1833 - val_loss: 2.0188 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=15, monitor='val_accuracy', mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.keras\", monitor='val_accuracy', save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "12abeab2-a018-4481-a2eb-db6562e3fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: [0 1 2 3 4 5 6 7]\n",
      "Label counts: {0: 301, 1: 301, 2: 153, 3: 301, 4: 301, 5: 150, 6: 301, 7: 153}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Unique labels in y_train:\", unique_values)\n",
    "print(\"Label counts:\", dict(zip(unique_values, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f6568f44-de38-44ae-b662-2b75c2e2250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(105, 1)),\n",
    "    Dense(64, activation='relu', kernel_regularizer='l2'),  # L2 regularization\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "928e6acd-1bd8-4cff-8408-6f116eb2e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.1317 - loss: 4.2021 - val_accuracy: 0.1303 - val_loss: 3.6111\n",
      "Epoch 2/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1465 - loss: 4.0268 - val_accuracy: 0.1324 - val_loss: 3.6135\n",
      "Epoch 3/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1391 - loss: 3.9113 - val_accuracy: 0.1303 - val_loss: 3.5996\n",
      "Epoch 4/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1402 - loss: 3.8413 - val_accuracy: 0.1283 - val_loss: 3.5694\n",
      "Epoch 5/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1230 - loss: 3.8193 - val_accuracy: 0.1365 - val_loss: 3.5295\n",
      "Epoch 6/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1622 - loss: 3.6182 - val_accuracy: 0.1426 - val_loss: 3.4867\n",
      "Epoch 7/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1581 - loss: 3.5740 - val_accuracy: 0.1446 - val_loss: 3.4413\n",
      "Epoch 8/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1519 - loss: 3.5651 - val_accuracy: 0.1507 - val_loss: 3.3987\n",
      "Epoch 9/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1488 - loss: 3.5331 - val_accuracy: 0.1609 - val_loss: 3.3587\n",
      "Epoch 10/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1651 - loss: 3.4383 - val_accuracy: 0.1690 - val_loss: 3.3207\n",
      "Epoch 11/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1540 - loss: 3.3922 - val_accuracy: 0.1731 - val_loss: 3.2850\n",
      "Epoch 12/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1833 - loss: 3.3501 - val_accuracy: 0.1772 - val_loss: 3.2529\n",
      "Epoch 13/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1868 - loss: 3.2707 - val_accuracy: 0.1731 - val_loss: 3.2216\n",
      "Epoch 14/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1816 - loss: 3.2363 - val_accuracy: 0.1731 - val_loss: 3.1933\n",
      "Epoch 15/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1966 - loss: 3.2086 - val_accuracy: 0.1772 - val_loss: 3.1649\n",
      "Epoch 16/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1801 - loss: 3.2532 - val_accuracy: 0.1772 - val_loss: 3.1396\n",
      "Epoch 17/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1910 - loss: 3.1660 - val_accuracy: 0.1996 - val_loss: 3.1128\n",
      "Epoch 18/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2076 - loss: 3.1271 - val_accuracy: 0.2179 - val_loss: 3.0889\n",
      "Epoch 19/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2216 - loss: 3.1139 - val_accuracy: 0.2200 - val_loss: 3.0649\n",
      "Epoch 20/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2336 - loss: 3.0516 - val_accuracy: 0.2240 - val_loss: 3.0423\n",
      "Epoch 21/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2500 - loss: 3.0407 - val_accuracy: 0.2322 - val_loss: 3.0210\n",
      "Epoch 22/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2143 - loss: 3.0084 - val_accuracy: 0.2383 - val_loss: 2.9988\n",
      "Epoch 23/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2289 - loss: 2.9978 - val_accuracy: 0.2444 - val_loss: 2.9781\n",
      "Epoch 24/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2393 - loss: 2.9629 - val_accuracy: 0.2505 - val_loss: 2.9574\n",
      "Epoch 25/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2454 - loss: 2.9269 - val_accuracy: 0.2525 - val_loss: 2.9376\n",
      "Epoch 26/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2734 - loss: 2.9294 - val_accuracy: 0.2525 - val_loss: 2.9193\n",
      "Epoch 27/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2677 - loss: 2.9232 - val_accuracy: 0.2566 - val_loss: 2.8998\n",
      "Epoch 28/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2683 - loss: 2.8797 - val_accuracy: 0.2525 - val_loss: 2.8811\n",
      "Epoch 29/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2958 - loss: 2.8073 - val_accuracy: 0.2587 - val_loss: 2.8623\n",
      "Epoch 30/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2685 - loss: 2.8250 - val_accuracy: 0.2607 - val_loss: 2.8437\n",
      "Epoch 31/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2988 - loss: 2.7636 - val_accuracy: 0.2729 - val_loss: 2.8249\n",
      "Epoch 32/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3225 - loss: 2.7516 - val_accuracy: 0.2749 - val_loss: 2.8079\n",
      "Epoch 33/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2913 - loss: 2.7841 - val_accuracy: 0.2811 - val_loss: 2.7903\n",
      "Epoch 34/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3210 - loss: 2.7193 - val_accuracy: 0.2851 - val_loss: 2.7733\n",
      "Epoch 35/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3288 - loss: 2.7377 - val_accuracy: 0.2933 - val_loss: 2.7564\n",
      "Epoch 36/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3144 - loss: 2.7195 - val_accuracy: 0.2953 - val_loss: 2.7398\n",
      "Epoch 37/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3124 - loss: 2.7148 - val_accuracy: 0.2994 - val_loss: 2.7231\n",
      "Epoch 38/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3252 - loss: 2.6851 - val_accuracy: 0.3116 - val_loss: 2.7058\n",
      "Epoch 39/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3571 - loss: 2.6363 - val_accuracy: 0.3136 - val_loss: 2.6903\n",
      "Epoch 40/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3329 - loss: 2.6382 - val_accuracy: 0.3218 - val_loss: 2.6742\n",
      "Epoch 41/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3450 - loss: 2.6092 - val_accuracy: 0.3259 - val_loss: 2.6571\n",
      "Epoch 42/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3825 - loss: 2.5736 - val_accuracy: 0.3279 - val_loss: 2.6413\n",
      "Epoch 43/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3711 - loss: 2.5328 - val_accuracy: 0.3340 - val_loss: 2.6254\n",
      "Epoch 44/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3682 - loss: 2.5638 - val_accuracy: 0.3360 - val_loss: 2.6098\n",
      "Epoch 45/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3546 - loss: 2.5292 - val_accuracy: 0.3442 - val_loss: 2.5938\n",
      "Epoch 46/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3860 - loss: 2.5099 - val_accuracy: 0.3503 - val_loss: 2.5786\n",
      "Epoch 47/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3861 - loss: 2.5110 - val_accuracy: 0.3503 - val_loss: 2.5629\n",
      "Epoch 48/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3888 - loss: 2.4950 - val_accuracy: 0.3564 - val_loss: 2.5477\n",
      "Epoch 49/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3768 - loss: 2.4630 - val_accuracy: 0.3646 - val_loss: 2.5307\n",
      "Epoch 50/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3885 - loss: 2.4637 - val_accuracy: 0.3666 - val_loss: 2.5153\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(\"final_model.keras\", save_best_only=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d5d8fea-2498-4e55-8fe7-48bcef290e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(105, 1)),\n",
    "    Dense(128, activation='relu'),  # Increased capacity\n",
    "    Dropout(0.3),  # Reduced from 0.5\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Increased from 0.0001\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1fc79ee-8574-4e5d-869f-35f0134f6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1400 - loss: 2.2398 - val_accuracy: 0.2688 - val_loss: 1.9237\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2914 - loss: 1.8714 - val_accuracy: 0.3870 - val_loss: 1.7262\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4436 - loss: 1.5781 - val_accuracy: 0.4562 - val_loss: 1.5306\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5442 - loss: 1.3016 - val_accuracy: 0.5031 - val_loss: 1.4096\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6007 - loss: 1.1178 - val_accuracy: 0.5275 - val_loss: 1.3362\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6068 - loss: 1.0481 - val_accuracy: 0.5519 - val_loss: 1.2861\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6523 - loss: 0.9143 - val_accuracy: 0.5601 - val_loss: 1.2710\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6740 - loss: 0.8802 - val_accuracy: 0.5540 - val_loss: 1.2563\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7088 - loss: 0.7658 - val_accuracy: 0.5540 - val_loss: 1.2755\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7412 - loss: 0.7098 - val_accuracy: 0.5662 - val_loss: 1.2629\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7483 - loss: 0.6720 - val_accuracy: 0.5642 - val_loss: 1.2748\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7568 - loss: 0.6605 - val_accuracy: 0.5743 - val_loss: 1.2751\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7613 - loss: 0.6081 - val_accuracy: 0.5580 - val_loss: 1.2858\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7635 - loss: 0.6032 - val_accuracy: 0.5682 - val_loss: 1.3004\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.5148 - val_accuracy: 0.5438 - val_loss: 1.2929\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8047 - loss: 0.5169 - val_accuracy: 0.5723 - val_loss: 1.3046\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8157 - loss: 0.4906 - val_accuracy: 0.5601 - val_loss: 1.3060\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8312 - loss: 0.4366 - val_accuracy: 0.5601 - val_loss: 1.3180\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8296 - loss: 0.4463 - val_accuracy: 0.5743 - val_loss: 1.3672\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8534 - loss: 0.4016 - val_accuracy: 0.5804 - val_loss: 1.3754\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8421 - loss: 0.3924 - val_accuracy: 0.5784 - val_loss: 1.3676\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.3753 - val_accuracy: 0.5947 - val_loss: 1.3952\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8706 - loss: 0.3276 - val_accuracy: 0.5784 - val_loss: 1.4189\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8690 - loss: 0.3575 - val_accuracy: 0.5784 - val_loss: 1.4234\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8809 - loss: 0.3122 - val_accuracy: 0.5886 - val_loss: 1.4386\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8740 - loss: 0.3164 - val_accuracy: 0.5886 - val_loss: 1.4315\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8977 - loss: 0.2899 - val_accuracy: 0.5845 - val_loss: 1.4787\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9079 - loss: 0.2689 - val_accuracy: 0.5845 - val_loss: 1.5025\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8993 - loss: 0.2636 - val_accuracy: 0.5866 - val_loss: 1.5430\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2514 - val_accuracy: 0.5906 - val_loss: 1.5232\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9089 - loss: 0.2473 - val_accuracy: 0.5866 - val_loss: 1.5425\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9075 - loss: 0.2562 - val_accuracy: 0.5886 - val_loss: 1.5737\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9162 - loss: 0.2290 - val_accuracy: 0.5825 - val_loss: 1.5670\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9032 - loss: 0.2601 - val_accuracy: 0.5804 - val_loss: 1.5882\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9190 - loss: 0.2207 - val_accuracy: 0.5866 - val_loss: 1.6190\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9098 - loss: 0.2129 - val_accuracy: 0.6008 - val_loss: 1.6109\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9297 - loss: 0.2159 - val_accuracy: 0.5845 - val_loss: 1.6196\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9370 - loss: 0.1736 - val_accuracy: 0.5866 - val_loss: 1.6412\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9249 - loss: 0.2040 - val_accuracy: 0.5743 - val_loss: 1.6676\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9451 - loss: 0.1660 - val_accuracy: 0.5784 - val_loss: 1.7049\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9268 - loss: 0.1845 - val_accuracy: 0.5743 - val_loss: 1.7238\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9189 - loss: 0.1989 - val_accuracy: 0.5804 - val_loss: 1.7395\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9371 - loss: 0.1750 - val_accuracy: 0.5886 - val_loss: 1.7526\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.2010 - val_accuracy: 0.5906 - val_loss: 1.7427\n",
      "Epoch 45/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9400 - loss: 0.1609 - val_accuracy: 0.5927 - val_loss: 1.7598\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.1640 - val_accuracy: 0.5866 - val_loss: 1.7968\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9469 - loss: 0.1411 - val_accuracy: 0.5764 - val_loss: 1.8215\n",
      "Epoch 48/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9415 - loss: 0.1627 - val_accuracy: 0.5743 - val_loss: 1.8406\n",
      "Epoch 49/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9397 - loss: 0.1543 - val_accuracy: 0.5642 - val_loss: 1.8466\n",
      "Epoch 50/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1340 - val_accuracy: 0.5784 - val_loss: 1.8786\n",
      "Epoch 51/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9383 - loss: 0.1570 - val_accuracy: 0.5804 - val_loss: 1.8780\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,  # Critical for imbalance\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=15, monitor='val_accuracy'), \n",
    "        ModelCheckpoint(\"best_simple_model.keras\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34528056-1459-429a-b670-f5e4da8d65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(105, 1)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),  # Combined L1/L2\n",
    "    Dropout(0.5),  # Increased from 0.4\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),\n",
    "    Dropout(0.5),  # Additional dropout\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "# 2. Compile the Model (MUST DO THIS BEFORE TRAINING)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e27e3623-49c0-444f-bf10-5faf29ac9c4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=best_weights.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m----> 3\u001b[0m     ModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m     ReduceLROnPlateau(factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m)  \u001b[38;5;66;03m# More aggressive reduction\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      7\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      8\u001b[0m     X_train, y_train,\n\u001b[0;32m      9\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:183\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_weights_only:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `save_weights_only=True` in `ModelCheckpoint`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, the filepath provided must end in `.weights.h5` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras weights format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=best_weights.h5"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=25, monitor='val_accuracy', mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_weights.h5\", save_weights_only=True, monitor='val_accuracy'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-7)  # More aggressive reduction\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,  # Increased max epochs\n",
    "    batch_size=64,  # Larger batch size\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "21079162-90fc-4197-b981-b1b76ae8fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1192 - loss: 4.8730 - val_accuracy: 0.1772 - val_loss: 4.0150 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1493 - loss: 4.4094 - val_accuracy: 0.2587 - val_loss: 3.8219 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1761 - loss: 4.1094 - val_accuracy: 0.3218 - val_loss: 3.7044 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2026 - loss: 3.9413 - val_accuracy: 0.3564 - val_loss: 3.6069 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2456 - loss: 3.7414 - val_accuracy: 0.3829 - val_loss: 3.5164 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2690 - loss: 3.6302 - val_accuracy: 0.4155 - val_loss: 3.4308 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2930 - loss: 3.4927 - val_accuracy: 0.4297 - val_loss: 3.3384 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3091 - loss: 3.4255 - val_accuracy: 0.4542 - val_loss: 3.2515 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3494 - loss: 3.2869 - val_accuracy: 0.4582 - val_loss: 3.1596 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3618 - loss: 3.1885 - val_accuracy: 0.4521 - val_loss: 3.0666 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3789 - loss: 3.0585 - val_accuracy: 0.4644 - val_loss: 2.9682 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4032 - loss: 2.9419 - val_accuracy: 0.4827 - val_loss: 2.8774 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4405 - loss: 2.8220 - val_accuracy: 0.4929 - val_loss: 2.7907 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4636 - loss: 2.7043 - val_accuracy: 0.4949 - val_loss: 2.7022 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4753 - loss: 2.6693 - val_accuracy: 0.5132 - val_loss: 2.6225 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5096 - loss: 2.5663 - val_accuracy: 0.5214 - val_loss: 2.5477 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5219 - loss: 2.4611 - val_accuracy: 0.5377 - val_loss: 2.4794 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5400 - loss: 2.3764 - val_accuracy: 0.5418 - val_loss: 2.4141 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5150 - loss: 2.3577 - val_accuracy: 0.5540 - val_loss: 2.3570 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5459 - loss: 2.2582 - val_accuracy: 0.5519 - val_loss: 2.3036 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5822 - loss: 2.1745 - val_accuracy: 0.5499 - val_loss: 2.2514 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5910 - loss: 2.1056 - val_accuracy: 0.5438 - val_loss: 2.2134 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5924 - loss: 2.0792 - val_accuracy: 0.5479 - val_loss: 2.1734 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6113 - loss: 1.9868 - val_accuracy: 0.5499 - val_loss: 2.1350 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6060 - loss: 1.9419 - val_accuracy: 0.5621 - val_loss: 2.0932 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6232 - loss: 1.9132 - val_accuracy: 0.5519 - val_loss: 2.0563 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6165 - loss: 1.8603 - val_accuracy: 0.5601 - val_loss: 2.0265 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6085 - loss: 1.8115 - val_accuracy: 0.5601 - val_loss: 2.0013 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6520 - loss: 1.7494 - val_accuracy: 0.5621 - val_loss: 1.9755 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6552 - loss: 1.7166 - val_accuracy: 0.5601 - val_loss: 1.9549 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6709 - loss: 1.6582 - val_accuracy: 0.5601 - val_loss: 1.9240 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6541 - loss: 1.6695 - val_accuracy: 0.5682 - val_loss: 1.9061 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6787 - loss: 1.6075 - val_accuracy: 0.5743 - val_loss: 1.8744 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6733 - loss: 1.6115 - val_accuracy: 0.5723 - val_loss: 1.8411 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6728 - loss: 1.5784 - val_accuracy: 0.5825 - val_loss: 1.8369 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6651 - loss: 1.5672 - val_accuracy: 0.5825 - val_loss: 1.8221 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6938 - loss: 1.5052 - val_accuracy: 0.5764 - val_loss: 1.8072 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7006 - loss: 1.4804 - val_accuracy: 0.5723 - val_loss: 1.8099 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6737 - loss: 1.5084 - val_accuracy: 0.5662 - val_loss: 1.8054 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6916 - loss: 1.4761 - val_accuracy: 0.5642 - val_loss: 1.7939 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7057 - loss: 1.4346 - val_accuracy: 0.5723 - val_loss: 1.7862 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7199 - loss: 1.4282 - val_accuracy: 0.5764 - val_loss: 1.7638 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7129 - loss: 1.4387 - val_accuracy: 0.5784 - val_loss: 1.7588 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7217 - loss: 1.3615 - val_accuracy: 0.5804 - val_loss: 1.7383 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7234 - loss: 1.3454 - val_accuracy: 0.5764 - val_loss: 1.7446 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7019 - loss: 1.3796 - val_accuracy: 0.5845 - val_loss: 1.7481 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7157 - loss: 1.3676 - val_accuracy: 0.5967 - val_loss: 1.7511 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7306 - loss: 1.3523 - val_accuracy: 0.5866 - val_loss: 1.7486 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7247 - loss: 1.3442 - val_accuracy: 0.5906 - val_loss: 1.7416 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7353 - loss: 1.2729 - val_accuracy: 0.5906 - val_loss: 1.7179 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7444 - loss: 1.2917 - val_accuracy: 0.5866 - val_loss: 1.7173 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7344 - loss: 1.2854 - val_accuracy: 0.5947 - val_loss: 1.7031 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 1.2867 - val_accuracy: 0.5804 - val_loss: 1.7021 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7610 - loss: 1.2290 - val_accuracy: 0.5866 - val_loss: 1.7107 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7172 - loss: 1.2642 - val_accuracy: 0.5947 - val_loss: 1.7111 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7278 - loss: 1.2823 - val_accuracy: 0.5703 - val_loss: 1.7121 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7363 - loss: 1.2986 - val_accuracy: 0.5866 - val_loss: 1.7193 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7439 - loss: 1.2702 - val_accuracy: 0.5804 - val_loss: 1.7094 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7741 - loss: 1.2076 - val_accuracy: 0.5906 - val_loss: 1.6971 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7546 - loss: 1.2211 - val_accuracy: 0.5927 - val_loss: 1.6912 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7547 - loss: 1.2076 - val_accuracy: 0.5784 - val_loss: 1.7027 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7547 - loss: 1.2136 - val_accuracy: 0.5804 - val_loss: 1.7241 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7477 - loss: 1.2132 - val_accuracy: 0.5927 - val_loss: 1.7130 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7807 - loss: 1.1834 - val_accuracy: 0.5682 - val_loss: 1.6972 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7743 - loss: 1.1844 - val_accuracy: 0.5743 - val_loss: 1.6998 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7594 - loss: 1.2032 - val_accuracy: 0.5967 - val_loss: 1.7022 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7610 - loss: 1.1696 - val_accuracy: 0.5988 - val_loss: 1.7161 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7765 - loss: 1.1698 - val_accuracy: 0.5804 - val_loss: 1.6977 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7814 - loss: 1.1329 - val_accuracy: 0.6008 - val_loss: 1.6700 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7893 - loss: 1.1338 - val_accuracy: 0.6008 - val_loss: 1.6765 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7724 - loss: 1.1276 - val_accuracy: 0.6191 - val_loss: 1.6894 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7787 - loss: 1.1372 - val_accuracy: 0.6090 - val_loss: 1.6783 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7790 - loss: 1.1360 - val_accuracy: 0.5906 - val_loss: 1.6809 - learning_rate: 0.0010\n",
      "Epoch 74/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7940 - loss: 1.1409 - val_accuracy: 0.6029 - val_loss: 1.6834 - learning_rate: 0.0010\n",
      "Epoch 75/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7831 - loss: 1.1225 - val_accuracy: 0.6110 - val_loss: 1.6613 - learning_rate: 0.0010\n",
      "Epoch 76/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7869 - loss: 1.1099 - val_accuracy: 0.5988 - val_loss: 1.6584 - learning_rate: 0.0010\n",
      "Epoch 77/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7562 - loss: 1.1615 - val_accuracy: 0.5967 - val_loss: 1.6697 - learning_rate: 0.0010\n",
      "Epoch 78/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8043 - loss: 1.0808 - val_accuracy: 0.6090 - val_loss: 1.6324 - learning_rate: 0.0010\n",
      "Epoch 79/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8174 - loss: 1.0751 - val_accuracy: 0.6212 - val_loss: 1.6476 - learning_rate: 0.0010\n",
      "Epoch 80/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7787 - loss: 1.1295 - val_accuracy: 0.6110 - val_loss: 1.6596 - learning_rate: 0.0010\n",
      "Epoch 81/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8035 - loss: 1.0693 - val_accuracy: 0.6069 - val_loss: 1.6876 - learning_rate: 0.0010\n",
      "Epoch 82/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8106 - loss: 1.1062 - val_accuracy: 0.5886 - val_loss: 1.6959 - learning_rate: 0.0010\n",
      "Epoch 83/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7903 - loss: 1.1126 - val_accuracy: 0.6049 - val_loss: 1.6798 - learning_rate: 0.0010\n",
      "Epoch 84/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8076 - loss: 1.0785 - val_accuracy: 0.6151 - val_loss: 1.6794 - learning_rate: 0.0010\n",
      "Epoch 85/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7749 - loss: 1.1292 - val_accuracy: 0.6130 - val_loss: 1.6709 - learning_rate: 0.0010\n",
      "Epoch 86/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8103 - loss: 1.0549 - val_accuracy: 0.5988 - val_loss: 1.6955 - learning_rate: 0.0010\n",
      "Epoch 87/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8024 - loss: 1.0876 - val_accuracy: 0.6069 - val_loss: 1.7079 - learning_rate: 0.0010\n",
      "Epoch 88/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7884 - loss: 1.1344 - val_accuracy: 0.6049 - val_loss: 1.6787 - learning_rate: 0.0010\n",
      "Epoch 89/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7921 - loss: 1.0693 - val_accuracy: 0.6110 - val_loss: 1.6781 - learning_rate: 2.0000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7896 - loss: 1.0802 - val_accuracy: 0.6130 - val_loss: 1.6722 - learning_rate: 2.0000e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8159 - loss: 1.0214 - val_accuracy: 0.6130 - val_loss: 1.6690 - learning_rate: 2.0000e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8427 - loss: 0.9790 - val_accuracy: 0.6151 - val_loss: 1.6650 - learning_rate: 2.0000e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8287 - loss: 0.9753 - val_accuracy: 0.6151 - val_loss: 1.6637 - learning_rate: 2.0000e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8385 - loss: 0.9794 - val_accuracy: 0.6110 - val_loss: 1.6632 - learning_rate: 2.0000e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8289 - loss: 0.9778 - val_accuracy: 0.6110 - val_loss: 1.6657 - learning_rate: 2.0000e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8408 - loss: 0.9567 - val_accuracy: 0.6069 - val_loss: 1.6642 - learning_rate: 2.0000e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8371 - loss: 0.9379 - val_accuracy: 0.6008 - val_loss: 1.6657 - learning_rate: 2.0000e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8428 - loss: 0.9595 - val_accuracy: 0.6049 - val_loss: 1.6649 - learning_rate: 2.0000e-04\n",
      "Epoch 99/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.9506 - val_accuracy: 0.6130 - val_loss: 1.6666 - learning_rate: 4.0000e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8507 - loss: 0.9206 - val_accuracy: 0.6110 - val_loss: 1.6684 - learning_rate: 4.0000e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8437 - loss: 0.9272 - val_accuracy: 0.6090 - val_loss: 1.6690 - learning_rate: 4.0000e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8276 - loss: 0.9888 - val_accuracy: 0.6090 - val_loss: 1.6685 - learning_rate: 4.0000e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8571 - loss: 0.9182 - val_accuracy: 0.6110 - val_loss: 1.6687 - learning_rate: 4.0000e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8490 - loss: 0.9335 - val_accuracy: 0.6110 - val_loss: 1.6672 - learning_rate: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 1. Model Definition\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(105, 1)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "# 2. Compile\n",
    "model.compile(optimizer=Adam(0.001),\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 3. Callbacks with FIXED ModelCheckpoint\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=25, monitor='val_accuracy', mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.weights.h5\",  # Correct extension\n",
    "                  save_weights_only=True,\n",
    "                  monitor='val_accuracy',\n",
    "                  save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "# 4. Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3fa5e365-441e-4b20-a34a-0d7bb644fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1961, 535), Val: (491, 535)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # Preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "87e43fcb-e362-40e0-9d14-4dcf464bd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scaled - mean: 0.22425733921644148 std: 1.5387565764871016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit ONLY on train\n",
    "X_val_scaled = scaler.transform(X_val)  # Transform val\n",
    "\n",
    "print(\"Train scaled - mean:\", X_train_scaled.mean(), \"std:\", X_train_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4ebdda27-f3c9-4351-a9cc-e9809216b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1786 - loss: 2.9022 - val_accuracy: 0.2729 - val_loss: 2.2713\n",
      "Epoch 2/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3005 - loss: 2.1005 - val_accuracy: 0.3686 - val_loss: 1.8449\n",
      "Epoch 3/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3409 - loss: 1.9164 - val_accuracy: 0.4338 - val_loss: 1.6065\n",
      "Epoch 4/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4280 - loss: 1.6815 - val_accuracy: 0.4562 - val_loss: 1.6322\n",
      "Epoch 5/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4600 - loss: 1.5465 - val_accuracy: 0.4644 - val_loss: 1.4762\n",
      "Epoch 6/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4876 - loss: 1.4439 - val_accuracy: 0.4969 - val_loss: 1.4219\n",
      "Epoch 7/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5013 - loss: 1.4109 - val_accuracy: 0.4949 - val_loss: 1.3856\n",
      "Epoch 8/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5508 - loss: 1.2147 - val_accuracy: 0.4847 - val_loss: 1.3436\n",
      "Epoch 9/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5485 - loss: 1.2399 - val_accuracy: 0.4949 - val_loss: 1.3517\n",
      "Epoch 10/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5682 - loss: 1.1389 - val_accuracy: 0.4766 - val_loss: 1.4120\n",
      "Epoch 11/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5869 - loss: 1.1338 - val_accuracy: 0.5092 - val_loss: 1.2518\n",
      "Epoch 12/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6154 - loss: 1.0547 - val_accuracy: 0.5071 - val_loss: 1.2554\n",
      "Epoch 13/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6388 - loss: 0.9825 - val_accuracy: 0.5092 - val_loss: 1.2780\n",
      "Epoch 14/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6283 - loss: 1.0341 - val_accuracy: 0.5234 - val_loss: 1.2215\n",
      "Epoch 15/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6195 - loss: 1.0308 - val_accuracy: 0.5377 - val_loss: 1.2253\n",
      "Epoch 16/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6709 - loss: 0.9022 - val_accuracy: 0.5601 - val_loss: 1.1585\n",
      "Epoch 17/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7005 - loss: 0.8518 - val_accuracy: 0.5662 - val_loss: 1.2049\n",
      "Epoch 18/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6780 - loss: 0.8151 - val_accuracy: 0.5438 - val_loss: 1.1996\n",
      "Epoch 19/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6852 - loss: 0.8601 - val_accuracy: 0.5418 - val_loss: 1.2129\n",
      "Epoch 20/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7050 - loss: 0.7863 - val_accuracy: 0.5234 - val_loss: 1.2909\n",
      "Epoch 21/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7122 - loss: 0.7377 - val_accuracy: 0.5519 - val_loss: 1.2224\n",
      "Epoch 22/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7257 - loss: 0.7591 - val_accuracy: 0.5662 - val_loss: 1.1481\n",
      "Epoch 23/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7228 - loss: 0.7186 - val_accuracy: 0.5845 - val_loss: 1.1473\n",
      "Epoch 24/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7323 - loss: 0.7128 - val_accuracy: 0.5682 - val_loss: 1.1367\n",
      "Epoch 25/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7414 - loss: 0.6704 - val_accuracy: 0.5988 - val_loss: 1.1131\n",
      "Epoch 26/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7534 - loss: 0.6565 - val_accuracy: 0.5927 - val_loss: 1.1566\n",
      "Epoch 27/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7601 - loss: 0.6451 - val_accuracy: 0.5866 - val_loss: 1.1791\n",
      "Epoch 28/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7520 - loss: 0.6261 - val_accuracy: 0.5662 - val_loss: 1.2268\n",
      "Epoch 29/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7645 - loss: 0.6088 - val_accuracy: 0.5642 - val_loss: 1.2005\n",
      "Epoch 30/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7730 - loss: 0.6147 - val_accuracy: 0.6029 - val_loss: 1.1422\n",
      "Epoch 31/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8053 - loss: 0.5417 - val_accuracy: 0.5967 - val_loss: 1.1334\n",
      "Epoch 32/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7832 - loss: 0.5856 - val_accuracy: 0.5703 - val_loss: 1.1872\n",
      "Epoch 33/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7877 - loss: 0.5436 - val_accuracy: 0.5845 - val_loss: 1.1729\n",
      "Epoch 34/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7931 - loss: 0.5491 - val_accuracy: 0.5845 - val_loss: 1.1820\n",
      "Epoch 35/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8210 - loss: 0.4741 - val_accuracy: 0.5825 - val_loss: 1.2131\n",
      "Epoch 36/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8008 - loss: 0.5370 - val_accuracy: 0.5845 - val_loss: 1.1503\n",
      "Epoch 37/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8131 - loss: 0.4988 - val_accuracy: 0.6171 - val_loss: 1.1499\n",
      "Epoch 38/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8101 - loss: 0.5125 - val_accuracy: 0.5723 - val_loss: 1.3014\n",
      "Epoch 39/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8118 - loss: 0.4656 - val_accuracy: 0.6110 - val_loss: 1.1680\n",
      "Epoch 40/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8348 - loss: 0.4388 - val_accuracy: 0.5988 - val_loss: 1.2352\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(535,)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[EarlyStopping(patience=15)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac94130-1af3-4368-a2f8-3d5ef8d2ceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1961, 535), Val: (491, 535)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # Preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90a4aa6-0eb9-444c-a039-10b65443392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scaled - mean: 0.22425733921644148 std: 1.5387565764871016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Fit only on training set\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"Train scaled - mean:\", X_train_scaled.mean(), \"std:\", X_train_scaled.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee1649a-4123-4c4c-aed4-345f0587f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_context(features, n_steps=3):\n",
    "    stacked = []\n",
    "    for i in range(n_steps, len(features)):\n",
    "        stacked.append(np.hstack([features[i-j] for j in range(n_steps)]))\n",
    "    return np.array(stacked)\n",
    "n_steps = 3  # Or whatever number of past frames you want to use\n",
    "n_steps = 3  # Or whatever number of past frames you want to use\n",
    "\n",
    "\n",
    "\n",
    "X_train_temp = add_temporal_context(X_train_scaled, n_steps=n_steps)\n",
    "X_val_temp = add_temporal_context(X_val_scaled, n_steps=n_steps)\n",
    "\n",
    "# Now this works:\n",
    "y_train_temp = y_train[n_steps:]\n",
    "y_val_temp = y_val[n_steps:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a0f565-fdfb-4293-880b-cc89fb5a9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_temp),\n",
    "    y=y_train_temp\n",
    ")\n",
    "class_weight_dict = {i: weight * 1.2 for i, weight in enumerate(class_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09826b1a-b2f7-4688-829d-7b453f536035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "input_dim = X_train_temp.shape[1]\n",
    "num_classes = len(np.unique(y_train_temp))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024, activation='swish', input_shape=(input_dim,),\n",
    "          kernel_regularizer=l1_l2(l1=0.002, l2=0.002)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(512, activation='swish',\n",
    "          kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bb478a-8b06-483f-b504-22bb9478134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39adc9ec-efe2-4a53-89db-60d0d25e2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.00005),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "   metrics=['accuracy']\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c522e7a7-9bd9-458a-bca7-202994a31d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=50, monitor='val_f1_score', mode='max',\n",
    "                  restore_best_weights=True, baseline=0.75),\n",
    "    ModelCheckpoint(\"champion.keras\", save_best_only=True, monitor='val_f1_score', mode='max'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=15, min_lr=1e-7)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734b9944-38a1-4941-a54b-5e4b966319ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.1320 - loss: 101.6760 - val_accuracy: 0.1373 - val_loss: 97.4598 - learning_rate: 5.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m 3/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1389 - loss: 99.6209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_f1_score` which is not available. Available metrics are: accuracy,loss,val_accuracy,val_loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_f1_score available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.1527 - loss: 98.7553 - val_accuracy: 0.1721 - val_loss: 94.8637 - learning_rate: 5.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.1387 - loss: 96.0950 - val_accuracy: 0.1824 - val_loss: 92.4018 - learning_rate: 5.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1566 - loss: 93.4529 - val_accuracy: 0.2049 - val_loss: 90.0656 - learning_rate: 5.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.1897 - loss: 90.8902 - val_accuracy: 0.2049 - val_loss: 87.8267 - learning_rate: 5.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.1745 - loss: 88.7357 - val_accuracy: 0.2070 - val_loss: 85.6651 - learning_rate: 5.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.1684 - loss: 86.6842 - val_accuracy: 0.2295 - val_loss: 83.6472 - learning_rate: 5.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.1745 - loss: 84.5434 - val_accuracy: 0.2316 - val_loss: 81.6806 - learning_rate: 5.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.1973 - loss: 82.5837 - val_accuracy: 0.2398 - val_loss: 79.8152 - learning_rate: 5.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.1961 - loss: 80.8415 - val_accuracy: 0.2439 - val_loss: 78.0406 - learning_rate: 5.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.2018 - loss: 78.8868 - val_accuracy: 0.2520 - val_loss: 76.3610 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2011 - loss: 77.3001 - val_accuracy: 0.2602 - val_loss: 74.7583 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2062 - loss: 75.7102 - val_accuracy: 0.2541 - val_loss: 73.2874 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2215 - loss: 74.0727 - val_accuracy: 0.2582 - val_loss: 71.8951 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.2341 - loss: 72.6663 - val_accuracy: 0.2541 - val_loss: 70.5684 - learning_rate: 5.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.2504 - loss: 71.3426 - val_accuracy: 0.2766 - val_loss: 69.3646 - learning_rate: 5.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.2492 - loss: 70.1695 - val_accuracy: 0.2992 - val_loss: 68.1761 - learning_rate: 5.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.2692 - loss: 68.8956 - val_accuracy: 0.2992 - val_loss: 67.0095 - learning_rate: 5.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2697 - loss: 67.7248 - val_accuracy: 0.3053 - val_loss: 65.8412 - learning_rate: 5.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.2653 - loss: 66.6545 - val_accuracy: 0.3176 - val_loss: 64.6840 - learning_rate: 5.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.2491 - loss: 65.3687 - val_accuracy: 0.3094 - val_loss: 63.5492 - learning_rate: 5.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.2852 - loss: 64.1514 - val_accuracy: 0.3197 - val_loss: 62.4185 - learning_rate: 5.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.2945 - loss: 63.0504 - val_accuracy: 0.3033 - val_loss: 61.3111 - learning_rate: 5.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.2807 - loss: 61.9381 - val_accuracy: 0.3238 - val_loss: 60.2026 - learning_rate: 5.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.2928 - loss: 60.8357 - val_accuracy: 0.3299 - val_loss: 59.1207 - learning_rate: 5.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.2883 - loss: 59.7004 - val_accuracy: 0.3279 - val_loss: 58.0491 - learning_rate: 5.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2916 - loss: 58.6130 - val_accuracy: 0.3320 - val_loss: 57.0105 - learning_rate: 5.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.3211 - loss: 57.5417 - val_accuracy: 0.3525 - val_loss: 55.9755 - learning_rate: 5.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.3215 - loss: 56.4837 - val_accuracy: 0.3566 - val_loss: 54.9537 - learning_rate: 5.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.3525 - loss: 55.4276 - val_accuracy: 0.3525 - val_loss: 53.9469 - learning_rate: 5.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.3372 - loss: 54.4151 - val_accuracy: 0.3566 - val_loss: 52.9770 - learning_rate: 5.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.3360 - loss: 53.4133 - val_accuracy: 0.3607 - val_loss: 51.9990 - learning_rate: 5.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.3177 - loss: 52.5104 - val_accuracy: 0.3627 - val_loss: 51.0215 - learning_rate: 5.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.3472 - loss: 51.4911 - val_accuracy: 0.3750 - val_loss: 50.0585 - learning_rate: 5.0000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.3548 - loss: 50.5573 - val_accuracy: 0.3750 - val_loss: 49.1279 - learning_rate: 5.0000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.3635 - loss: 49.6438 - val_accuracy: 0.3586 - val_loss: 48.2239 - learning_rate: 5.0000e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.3836 - loss: 48.5759 - val_accuracy: 0.3689 - val_loss: 47.3747 - learning_rate: 5.0000e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.3602 - loss: 47.7936 - val_accuracy: 0.3689 - val_loss: 46.5044 - learning_rate: 5.0000e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.3769 - loss: 46.9214 - val_accuracy: 0.3791 - val_loss: 45.6411 - learning_rate: 5.0000e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.3909 - loss: 46.1026 - val_accuracy: 0.3852 - val_loss: 44.8463 - learning_rate: 5.0000e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.3641 - loss: 45.2295 - val_accuracy: 0.3934 - val_loss: 44.0189 - learning_rate: 5.0000e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.3975 - loss: 44.3768 - val_accuracy: 0.3955 - val_loss: 43.2081 - learning_rate: 5.0000e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.3939 - loss: 43.6159 - val_accuracy: 0.3934 - val_loss: 42.4785 - learning_rate: 5.0000e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.4171 - loss: 42.8308 - val_accuracy: 0.3975 - val_loss: 41.7275 - learning_rate: 5.0000e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.4156 - loss: 41.9243 - val_accuracy: 0.4037 - val_loss: 40.9594 - learning_rate: 5.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.4106 - loss: 41.2356 - val_accuracy: 0.3996 - val_loss: 40.2318 - learning_rate: 5.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.4129 - loss: 40.5754 - val_accuracy: 0.4037 - val_loss: 39.5807 - learning_rate: 5.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.4194 - loss: 39.8450 - val_accuracy: 0.4057 - val_loss: 38.8726 - learning_rate: 5.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.4110 - loss: 39.1729 - val_accuracy: 0.3975 - val_loss: 38.2080 - learning_rate: 5.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.4053 - loss: 38.5339 - val_accuracy: 0.3955 - val_loss: 37.5791 - learning_rate: 5.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4479 - loss: 37.8330 - val_accuracy: 0.4098 - val_loss: 36.8894 - learning_rate: 5.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.4533 - loss: 37.0681 - val_accuracy: 0.4180 - val_loss: 36.2240 - learning_rate: 5.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.4346 - loss: 36.4962 - val_accuracy: 0.4201 - val_loss: 35.6365 - learning_rate: 5.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4642 - loss: 35.9114 - val_accuracy: 0.4037 - val_loss: 35.0447 - learning_rate: 5.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.4368 - loss: 35.3330 - val_accuracy: 0.4098 - val_loss: 34.4320 - learning_rate: 5.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.4351 - loss: 34.7342 - val_accuracy: 0.4160 - val_loss: 33.8913 - learning_rate: 5.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.4496 - loss: 34.2373 - val_accuracy: 0.4242 - val_loss: 33.3622 - learning_rate: 5.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.4567 - loss: 33.5100 - val_accuracy: 0.4406 - val_loss: 32.7885 - learning_rate: 5.0000e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.4706 - loss: 33.0407 - val_accuracy: 0.4098 - val_loss: 32.2885 - learning_rate: 5.0000e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4816 - loss: 32.5288 - val_accuracy: 0.4221 - val_loss: 31.7457 - learning_rate: 5.0000e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4578 - loss: 32.0130 - val_accuracy: 0.4221 - val_loss: 31.2163 - learning_rate: 5.0000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4788 - loss: 31.4672 - val_accuracy: 0.4098 - val_loss: 30.7397 - learning_rate: 5.0000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.5005 - loss: 30.9598 - val_accuracy: 0.4160 - val_loss: 30.2558 - learning_rate: 5.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4684 - loss: 30.5326 - val_accuracy: 0.4201 - val_loss: 29.8501 - learning_rate: 5.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4751 - loss: 29.9923 - val_accuracy: 0.4160 - val_loss: 29.3499 - learning_rate: 5.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.4792 - loss: 29.4814 - val_accuracy: 0.4242 - val_loss: 28.9299 - learning_rate: 5.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.4865 - loss: 29.1259 - val_accuracy: 0.4201 - val_loss: 28.5153 - learning_rate: 5.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.5078 - loss: 28.6263 - val_accuracy: 0.4262 - val_loss: 28.1381 - learning_rate: 5.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5044 - loss: 28.1318 - val_accuracy: 0.4201 - val_loss: 27.7167 - learning_rate: 5.0000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4797 - loss: 27.9140 - val_accuracy: 0.4201 - val_loss: 27.3258 - learning_rate: 5.0000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4900 - loss: 27.4422 - val_accuracy: 0.4221 - val_loss: 26.9384 - learning_rate: 5.0000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.5145 - loss: 26.9891 - val_accuracy: 0.4242 - val_loss: 26.5381 - learning_rate: 5.0000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.5035 - loss: 26.6798 - val_accuracy: 0.4139 - val_loss: 26.2189 - learning_rate: 5.0000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.4935 - loss: 26.3356 - val_accuracy: 0.4221 - val_loss: 25.8744 - learning_rate: 5.0000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5175 - loss: 25.9782 - val_accuracy: 0.4365 - val_loss: 25.4769 - learning_rate: 5.0000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5238 - loss: 25.5897 - val_accuracy: 0.4385 - val_loss: 25.1804 - learning_rate: 5.0000e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5109 - loss: 25.2888 - val_accuracy: 0.4365 - val_loss: 24.8577 - learning_rate: 5.0000e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5203 - loss: 24.9619 - val_accuracy: 0.4447 - val_loss: 24.5097 - learning_rate: 5.0000e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5378 - loss: 24.5789 - val_accuracy: 0.4447 - val_loss: 24.1821 - learning_rate: 5.0000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5119 - loss: 24.4046 - val_accuracy: 0.4406 - val_loss: 23.8946 - learning_rate: 5.0000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.5473 - loss: 23.8930 - val_accuracy: 0.4303 - val_loss: 23.5994 - learning_rate: 5.0000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4955 - loss: 23.7308 - val_accuracy: 0.4344 - val_loss: 23.3556 - learning_rate: 5.0000e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5401 - loss: 23.2881 - val_accuracy: 0.4324 - val_loss: 23.0233 - learning_rate: 5.0000e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5529 - loss: 23.0353 - val_accuracy: 0.4283 - val_loss: 22.7701 - learning_rate: 5.0000e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.5290 - loss: 22.8271 - val_accuracy: 0.4344 - val_loss: 22.4657 - learning_rate: 5.0000e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.5325 - loss: 22.5836 - val_accuracy: 0.4303 - val_loss: 22.2358 - learning_rate: 5.0000e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5331 - loss: 22.2393 - val_accuracy: 0.4324 - val_loss: 21.9675 - learning_rate: 5.0000e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5334 - loss: 21.9998 - val_accuracy: 0.4221 - val_loss: 21.7239 - learning_rate: 5.0000e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5328 - loss: 21.7893 - val_accuracy: 0.4488 - val_loss: 21.5114 - learning_rate: 5.0000e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5479 - loss: 21.4927 - val_accuracy: 0.4426 - val_loss: 21.2494 - learning_rate: 5.0000e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.5543 - loss: 21.2045 - val_accuracy: 0.4590 - val_loss: 20.9839 - learning_rate: 5.0000e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.5753 - loss: 20.9526 - val_accuracy: 0.4549 - val_loss: 20.7963 - learning_rate: 5.0000e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5557 - loss: 20.8175 - val_accuracy: 0.4467 - val_loss: 20.5596 - learning_rate: 5.0000e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5475 - loss: 20.5514 - val_accuracy: 0.4529 - val_loss: 20.3622 - learning_rate: 5.0000e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.5497 - loss: 20.4553 - val_accuracy: 0.4447 - val_loss: 20.1235 - learning_rate: 5.0000e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.5592 - loss: 20.1271 - val_accuracy: 0.4529 - val_loss: 19.9086 - learning_rate: 5.0000e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.5582 - loss: 19.9771 - val_accuracy: 0.4467 - val_loss: 19.7494 - learning_rate: 5.0000e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5745 - loss: 19.6603 - val_accuracy: 0.4447 - val_loss: 19.5364 - learning_rate: 5.0000e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5779 - loss: 19.4368 - val_accuracy: 0.4467 - val_loss: 19.2888 - learning_rate: 5.0000e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.5942 - loss: 19.2227 - val_accuracy: 0.4283 - val_loss: 19.1133 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_temp,\n",
    "    y_train_temp,\n",
    "    validation_data=(X_val_temp, y_val_temp),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fea979d-29a4-4f22-9fb5-94571a2cc59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# 1. Load your data (replace with your actual data loading)\n",
    "# X, y = load_your_data() \n",
    "\n",
    "# 2. Split data (keeping temporal order)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Scale features (fit only on training)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62274fde-4603-4acd-b1c8-9facc800454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: (1956, 5, 535)\n",
      "Validation sequences: (486, 5, 535)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(features, targets, n_steps=3):\n",
    "    \"\"\"\n",
    "    Creates proper time series sequences for LSTM/CNN\n",
    "    Returns: \n",
    "        X: 3D array of shape (samples, timesteps, features)\n",
    "        y: 1D array of targets\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(features)):\n",
    "        X.append(features[i-n_steps:i])\n",
    "        y.append(targets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "n_steps = 5  # Try 3-10 time steps\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, n_steps)\n",
    "X_val_seq, y_val_val = create_sequences(X_val_scaled, y_val, n_steps)\n",
    "\n",
    "print(f\"Training sequences: {X_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "174bc89b-c1d6-4b86-bd27-e75e9ab88a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout, BatchNormalization\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l1_l2\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      6\u001b[0m     LSTM(\u001b[38;5;241m128\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(n_steps, X_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), \n\u001b[0;32m      7\u001b[0m          kernel_regularizer\u001b[38;5;241m=\u001b[39ml1_l2(l1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m),\n\u001b[0;32m      8\u001b[0m          return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      9\u001b[0m     BatchNormalization(),\n\u001b[0;32m     10\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     LSTM(\u001b[38;5;241m64\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml1_l2(l1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     BatchNormalization(),\n\u001b[0;32m     14\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 17\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(classes), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m ])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(n_steps, X_train_scaled.shape[1]), \n",
    "         kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "         return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(64, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0003),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        min_delta=0.005,\n",
    "        baseline=0.5,  # Minimum acceptable accuracy\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9bfdb6-6981-4178-ac14-b1e6201e1cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - accuracy: 0.1475 - loss: 2.5612 - val_accuracy: 0.1173 - val_loss: 2.0873 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.1638 - loss: 2.2141 - val_accuracy: 0.1255 - val_loss: 2.0897 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.1810 - loss: 2.1227 - val_accuracy: 0.1420 - val_loss: 2.0902 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2072 - loss: 2.0496 - val_accuracy: 0.1337 - val_loss: 2.0917 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.2367 - loss: 1.9725 - val_accuracy: 0.1235 - val_loss: 2.0929 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2220 - loss: 1.9522 - val_accuracy: 0.1255 - val_loss: 2.1080 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.2573 - loss: 1.9135 - val_accuracy: 0.1502 - val_loss: 2.1004 - learning_rate: 2.5000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.2496 - loss: 1.9148 - val_accuracy: 0.1235 - val_loss: 2.1205 - learning_rate: 2.5000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.2935 - loss: 1.8032 - val_accuracy: 0.1317 - val_loss: 2.1269 - learning_rate: 2.5000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.2897 - loss: 1.8152 - val_accuracy: 0.1214 - val_loss: 2.1415 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3072 - loss: 1.8141 - val_accuracy: 0.1317 - val_loss: 2.1382 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.3280 - loss: 1.7639 - val_accuracy: 0.1255 - val_loss: 2.1469 - learning_rate: 1.2500e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3417 - loss: 1.7817 - val_accuracy: 0.1420 - val_loss: 2.1673 - learning_rate: 1.2500e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3405 - loss: 1.7581 - val_accuracy: 0.1379 - val_loss: 2.1785 - learning_rate: 1.2500e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3556 - loss: 1.6888 - val_accuracy: 0.1420 - val_loss: 2.1915 - learning_rate: 1.2500e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.3697 - loss: 1.6715 - val_accuracy: 0.1358 - val_loss: 2.2025 - learning_rate: 1.2500e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3473 - loss: 1.6747 - val_accuracy: 0.1379 - val_loss: 2.2092 - learning_rate: 6.2500e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3910 - loss: 1.6561 - val_accuracy: 0.1296 - val_loss: 2.2126 - learning_rate: 6.2500e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.3577 - loss: 1.6509 - val_accuracy: 0.1317 - val_loss: 2.2185 - learning_rate: 6.2500e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3460 - loss: 1.6595 - val_accuracy: 0.1276 - val_loss: 2.2278 - learning_rate: 6.2500e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.3704 - loss: 1.6274 - val_accuracy: 0.1276 - val_loss: 2.2316 - learning_rate: 6.2500e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3810 - loss: 1.6351 - val_accuracy: 0.1337 - val_loss: 2.2335 - learning_rate: 3.1250e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3865 - loss: 1.6263 - val_accuracy: 0.1379 - val_loss: 2.2375 - learning_rate: 3.1250e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3599 - loss: 1.6291 - val_accuracy: 0.1379 - val_loss: 2.2407 - learning_rate: 3.1250e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.4013 - loss: 1.5877 - val_accuracy: 0.1379 - val_loss: 2.2409 - learning_rate: 3.1250e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.3724 - loss: 1.6182 - val_accuracy: 0.1399 - val_loss: 2.2441 - learning_rate: 3.1250e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3915 - loss: 1.6058 - val_accuracy: 0.1420 - val_loss: 2.2440 - learning_rate: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8923083d-8c30-45b7-9386-f019e90e2302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (Train): [300 300 153 300 301 148 301 153]\n",
      "Class distribution (Val): [74 75 38 73 75 38 75 38]\n",
      "NaN in X_train_seq: 0\n",
      "Inf in X_train_seq: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution\n",
    "print(\"Class distribution (Train):\", np.bincount(y_train_seq))\n",
    "print(\"Class distribution (Val):\", np.bincount(y_val_val))\n",
    "\n",
    "# Check for NaN/inf values\n",
    "print(\"NaN in X_train_seq:\", np.isnan(X_train_seq).sum())\n",
    "print(\"Inf in X_train_seq:\", np.isinf(X_train_seq).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00297baa-9110-45dc-8864-61a7bfd5f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2452, 535), Labels: (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SAFE loading from now on\n",
    "X = np.load(\"X_advanced_features.npy\")\n",
    "y = np.load(\"y_emotion_labels_encoded.npy\")  # No allow_pickle needed!\n",
    "\n",
    "print(f\"Features: {X.shape}, Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b491dee2-536e-4ffb-a6cd-701e96186f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: {0: 376, 1: 376, 2: 192, 3: 376, 4: 376, 5: 188, 6: 376, 7: 192}\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Class distribution: {dict(zip(unique, counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b9abd5-0306-42e6-aa1c-b1e9b78a89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape if needed (for CNNs/LSTMs)\n",
    "# Example: If X is 2D but needs to be 3D for LSTM (samples, timesteps, features)\n",
    "if len(X_train.shape) == 2:\n",
    "    X_train = np.expand_dims(X_train, axis=-1)  # Add channel dimension\n",
    "    X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf54cce6-b1e5-4e96-a4d0-402005aeed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(256, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88716f6d-f39c-46a0-a869-bd2b883a68f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.1839 - loss: 2.5181 - val_accuracy: 0.2648 - val_loss: 1.8995\n",
      "Epoch 2/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.3175 - loss: 1.8013 - val_accuracy: 0.4134 - val_loss: 1.5795\n",
      "Epoch 3/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.4745 - loss: 1.4454 - val_accuracy: 0.5031 - val_loss: 1.3562\n",
      "Epoch 4/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.5657 - loss: 1.1069 - val_accuracy: 0.5316 - val_loss: 1.2440\n",
      "Epoch 5/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.6890 - loss: 0.8958 - val_accuracy: 0.5316 - val_loss: 1.3086\n",
      "Epoch 6/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.7340 - loss: 0.7529 - val_accuracy: 0.5560 - val_loss: 1.2556\n",
      "Epoch 7/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.7982 - loss: 0.6198 - val_accuracy: 0.5662 - val_loss: 1.2953\n",
      "Epoch 8/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.8553 - loss: 0.4785 - val_accuracy: 0.6069 - val_loss: 1.2930\n",
      "Epoch 9/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.8956 - loss: 0.3323 - val_accuracy: 0.6069 - val_loss: 1.4044\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if y is one-hot\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff3a505-cb26-4a08-9ac2-2022060931e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6394 - loss: 1.2956\n",
      "Test Accuracy: 0.6069\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e573b2ff-faa6-4471-bf86-564aae41601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight * 1.2 for i, weight in enumerate(class_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9385c0c3-2d7d-4356-9db0-058c16edabbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.8983 - loss: 0.3442 - val_accuracy: 0.5947 - val_loss: 1.4567\n",
      "Epoch 2/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9633 - loss: 0.1666 - val_accuracy: 0.6090 - val_loss: 1.6285\n",
      "Epoch 3/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9826 - loss: 0.1020 - val_accuracy: 0.5866 - val_loss: 1.8063\n",
      "Epoch 4/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9900 - loss: 0.0670 - val_accuracy: 0.6090 - val_loss: 1.9736\n",
      "Epoch 5/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9952 - loss: 0.0435 - val_accuracy: 0.6130 - val_loss: 1.9341\n",
      "Epoch 6/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9926 - loss: 0.0415 - val_accuracy: 0.6029 - val_loss: 2.0024\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if y is one-hot\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    class_weight=class_weight_dict,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8cf993-3f33-4b87-bb93-756e4896e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6029 - loss: 1.9123\n",
      "Test Accuracy: 0.6029\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4fd27f-c22f-4c8e-9bf1-84533ec324fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', \n",
    "          kernel_regularizer=l2(0.1),  # Extreme L2 penalty\n",
    "          input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.6),\n",
    "    Dense(32, activation='relu',\n",
    "          kernel_regularizer=l2(0.05)),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d59562b9-0f27-414f-9700-9a474c4a3c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1610 - loss: 14.6429 - val_accuracy: 0.1914 - val_loss: 10.9576\n",
      "Epoch 2/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1908 - loss: 10.1384 - val_accuracy: 0.2525 - val_loss: 7.7734\n",
      "Epoch 3/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2332 - loss: 7.1900 - val_accuracy: 0.3279 - val_loss: 5.6615\n",
      "Epoch 4/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2965 - loss: 5.2972 - val_accuracy: 0.3646 - val_loss: 4.2826\n",
      "Epoch 5/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3293 - loss: 4.0522 - val_accuracy: 0.3747 - val_loss: 3.4276\n",
      "Epoch 6/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3379 - loss: 3.2833 - val_accuracy: 0.3870 - val_loss: 2.8917\n",
      "Epoch 7/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3781 - loss: 2.7898 - val_accuracy: 0.4257 - val_loss: 2.5302\n",
      "Epoch 8/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3851 - loss: 2.4807 - val_accuracy: 0.3971 - val_loss: 2.3133\n",
      "Epoch 9/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3865 - loss: 2.2907 - val_accuracy: 0.4318 - val_loss: 2.1550\n",
      "Epoch 10/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4038 - loss: 2.1426 - val_accuracy: 0.4399 - val_loss: 2.0654\n",
      "Epoch 11/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3985 - loss: 2.0638 - val_accuracy: 0.4399 - val_loss: 1.9944\n",
      "Epoch 12/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4150 - loss: 1.9829 - val_accuracy: 0.4481 - val_loss: 1.9421\n",
      "Epoch 13/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4389 - loss: 1.9428 - val_accuracy: 0.4196 - val_loss: 1.9070\n",
      "Epoch 14/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4148 - loss: 1.9119 - val_accuracy: 0.4236 - val_loss: 1.8885\n",
      "Epoch 15/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4320 - loss: 1.8785 - val_accuracy: 0.4745 - val_loss: 1.8513\n",
      "Epoch 16/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4399 - loss: 1.8716 - val_accuracy: 0.4623 - val_loss: 1.8189\n",
      "Epoch 17/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4571 - loss: 1.8271 - val_accuracy: 0.4786 - val_loss: 1.8167\n",
      "Epoch 18/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4364 - loss: 1.8355 - val_accuracy: 0.4725 - val_loss: 1.7854\n",
      "Epoch 19/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4393 - loss: 1.8195 - val_accuracy: 0.4868 - val_loss: 1.7856\n",
      "Epoch 20/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4706 - loss: 1.7967 - val_accuracy: 0.4969 - val_loss: 1.7689\n",
      "Epoch 21/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4567 - loss: 1.7716 - val_accuracy: 0.4929 - val_loss: 1.7589\n",
      "Epoch 22/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4565 - loss: 1.8107 - val_accuracy: 0.4705 - val_loss: 1.7567\n",
      "Epoch 23/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4509 - loss: 1.7809 - val_accuracy: 0.4705 - val_loss: 1.7576\n",
      "Epoch 24/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4670 - loss: 1.7440 - val_accuracy: 0.4766 - val_loss: 1.7444\n",
      "Epoch 25/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4775 - loss: 1.7529 - val_accuracy: 0.5010 - val_loss: 1.7461\n",
      "Epoch 26/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4743 - loss: 1.7250 - val_accuracy: 0.5051 - val_loss: 1.7091\n",
      "Epoch 27/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4810 - loss: 1.7164 - val_accuracy: 0.5112 - val_loss: 1.6992\n",
      "Epoch 28/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4665 - loss: 1.7512 - val_accuracy: 0.4929 - val_loss: 1.7100\n",
      "Epoch 29/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4508 - loss: 1.7645 - val_accuracy: 0.4725 - val_loss: 1.7304\n",
      "Epoch 30/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4768 - loss: 1.7093 - val_accuracy: 0.5051 - val_loss: 1.6893\n",
      "Epoch 31/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4913 - loss: 1.7095 - val_accuracy: 0.5193 - val_loss: 1.6871\n",
      "Epoch 32/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4549 - loss: 1.7616 - val_accuracy: 0.4868 - val_loss: 1.6947\n",
      "Epoch 33/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4753 - loss: 1.6973 - val_accuracy: 0.4949 - val_loss: 1.6915\n",
      "Epoch 34/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5007 - loss: 1.6919 - val_accuracy: 0.5255 - val_loss: 1.6746\n",
      "Epoch 35/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 1.6943 - val_accuracy: 0.5092 - val_loss: 1.6692\n",
      "Epoch 36/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4730 - loss: 1.6857 - val_accuracy: 0.5193 - val_loss: 1.6721\n",
      "Epoch 37/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5089 - loss: 1.6747 - val_accuracy: 0.5173 - val_loss: 1.6661\n",
      "Epoch 38/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5109 - loss: 1.6442 - val_accuracy: 0.5295 - val_loss: 1.6627\n",
      "Epoch 39/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4819 - loss: 1.6887 - val_accuracy: 0.5010 - val_loss: 1.6590\n",
      "Epoch 40/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4958 - loss: 1.6937 - val_accuracy: 0.5092 - val_loss: 1.6791\n",
      "Epoch 41/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4876 - loss: 1.6938 - val_accuracy: 0.5173 - val_loss: 1.6696\n",
      "Epoch 42/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5030 - loss: 1.6745 - val_accuracy: 0.5214 - val_loss: 1.6600\n",
      "Epoch 43/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4682 - loss: 1.6664 - val_accuracy: 0.4929 - val_loss: 1.6689\n",
      "Epoch 44/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4760 - loss: 1.6594 - val_accuracy: 0.5255 - val_loss: 1.6378\n",
      "Epoch 45/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5131 - loss: 1.6273 - val_accuracy: 0.5132 - val_loss: 1.6573\n",
      "Epoch 46/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4829 - loss: 1.6656 - val_accuracy: 0.5010 - val_loss: 1.6532\n",
      "Epoch 47/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4923 - loss: 1.6608 - val_accuracy: 0.5031 - val_loss: 1.6551\n",
      "Epoch 48/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5102 - loss: 1.6479 - val_accuracy: 0.5377 - val_loss: 1.6433\n",
      "Epoch 49/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4816 - loss: 1.6501 - val_accuracy: 0.5356 - val_loss: 1.6149\n",
      "Epoch 50/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5008 - loss: 1.6591 - val_accuracy: 0.5112 - val_loss: 1.6379\n",
      "Epoch 51/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4856 - loss: 1.6863 - val_accuracy: 0.5112 - val_loss: 1.6471\n",
      "Epoch 52/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4783 - loss: 1.6591 - val_accuracy: 0.5438 - val_loss: 1.6158\n",
      "Epoch 53/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4935 - loss: 1.6509 - val_accuracy: 0.5010 - val_loss: 1.6416\n",
      "Epoch 54/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4986 - loss: 1.6338 - val_accuracy: 0.5519 - val_loss: 1.6086\n",
      "Epoch 55/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4859 - loss: 1.6580 - val_accuracy: 0.4888 - val_loss: 1.6572\n",
      "Epoch 56/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4835 - loss: 1.6705 - val_accuracy: 0.5214 - val_loss: 1.6067\n",
      "Epoch 57/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5183 - loss: 1.6242 - val_accuracy: 0.5193 - val_loss: 1.6018\n",
      "Epoch 58/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4856 - loss: 1.6147 - val_accuracy: 0.5214 - val_loss: 1.6127\n",
      "Epoch 59/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5164 - loss: 1.6176 - val_accuracy: 0.5560 - val_loss: 1.5802\n",
      "Epoch 60/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5182 - loss: 1.6056 - val_accuracy: 0.5397 - val_loss: 1.5843\n",
      "Epoch 61/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4862 - loss: 1.6320 - val_accuracy: 0.5092 - val_loss: 1.6105\n",
      "Epoch 62/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5051 - loss: 1.6274 - val_accuracy: 0.5153 - val_loss: 1.6033\n",
      "Epoch 63/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4960 - loss: 1.6325 - val_accuracy: 0.5397 - val_loss: 1.5912\n",
      "Epoch 64/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5212 - loss: 1.5952 - val_accuracy: 0.5234 - val_loss: 1.5986\n",
      "Epoch 65/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4858 - loss: 1.6709 - val_accuracy: 0.5397 - val_loss: 1.6059\n",
      "Epoch 66/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4930 - loss: 1.6306 - val_accuracy: 0.5580 - val_loss: 1.5842\n",
      "Epoch 67/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5130 - loss: 1.6272 - val_accuracy: 0.5499 - val_loss: 1.5786\n",
      "Epoch 68/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5281 - loss: 1.6006 - val_accuracy: 0.5316 - val_loss: 1.6123\n",
      "Epoch 69/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4833 - loss: 1.6652 - val_accuracy: 0.5153 - val_loss: 1.6087\n",
      "Epoch 70/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5268 - loss: 1.5887 - val_accuracy: 0.5316 - val_loss: 1.5846\n",
      "Epoch 71/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5276 - loss: 1.5974 - val_accuracy: 0.5356 - val_loss: 1.5898\n",
      "Epoch 72/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4941 - loss: 1.6360 - val_accuracy: 0.5397 - val_loss: 1.5888\n",
      "Epoch 73/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5368 - loss: 1.5712 - val_accuracy: 0.5316 - val_loss: 1.5995\n",
      "Epoch 74/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5360 - loss: 1.6014 - val_accuracy: 0.5234 - val_loss: 1.5880\n",
      "Epoch 75/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5121 - loss: 1.6042 - val_accuracy: 0.4949 - val_loss: 1.6322\n",
      "Epoch 76/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4895 - loss: 1.6399 - val_accuracy: 0.5316 - val_loss: 1.5980\n",
      "Epoch 77/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5174 - loss: 1.5974 - val_accuracy: 0.5397 - val_loss: 1.5743\n",
      "Epoch 78/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5039 - loss: 1.6221 - val_accuracy: 0.5255 - val_loss: 1.5783\n",
      "Epoch 79/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5386 - loss: 1.5633 - val_accuracy: 0.5316 - val_loss: 1.5872\n",
      "Epoch 80/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5021 - loss: 1.5940 - val_accuracy: 0.5356 - val_loss: 1.5653\n",
      "Epoch 81/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5314 - loss: 1.5591 - val_accuracy: 0.5438 - val_loss: 1.5628\n",
      "Epoch 82/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5320 - loss: 1.5855 - val_accuracy: 0.5112 - val_loss: 1.5657\n",
      "Epoch 83/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5276 - loss: 1.5392 - val_accuracy: 0.5356 - val_loss: 1.5790\n",
      "Epoch 84/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5067 - loss: 1.5981 - val_accuracy: 0.5356 - val_loss: 1.5629\n",
      "Epoch 85/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5094 - loss: 1.5807 - val_accuracy: 0.5031 - val_loss: 1.5864\n",
      "Epoch 86/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5442 - loss: 1.5673 - val_accuracy: 0.5642 - val_loss: 1.5406\n",
      "Epoch 87/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5162 - loss: 1.5674 - val_accuracy: 0.5295 - val_loss: 1.5649\n",
      "Epoch 88/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5513 - loss: 1.5518 - val_accuracy: 0.5255 - val_loss: 1.5751\n",
      "Epoch 89/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5146 - loss: 1.6028 - val_accuracy: 0.5458 - val_loss: 1.5500\n",
      "Epoch 90/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5285 - loss: 1.5659 - val_accuracy: 0.5295 - val_loss: 1.5734\n",
      "Epoch 91/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5290 - loss: 1.5733 - val_accuracy: 0.5560 - val_loss: 1.5703\n",
      "Epoch 92/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5049 - loss: 1.5690 - val_accuracy: 0.5377 - val_loss: 1.5454\n",
      "Epoch 93/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5641 - loss: 1.5212 - val_accuracy: 0.5377 - val_loss: 1.5730\n",
      "Epoch 94/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5176 - loss: 1.5634 - val_accuracy: 0.5418 - val_loss: 1.5433\n",
      "Epoch 95/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5210 - loss: 1.5574 - val_accuracy: 0.5540 - val_loss: 1.5382\n",
      "Epoch 96/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5410 - loss: 1.5510 - val_accuracy: 0.5071 - val_loss: 1.5839\n",
      "Epoch 97/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5408 - loss: 1.5505 - val_accuracy: 0.5316 - val_loss: 1.5454\n",
      "Epoch 98/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5348 - loss: 1.5230 - val_accuracy: 0.5173 - val_loss: 1.5427\n",
      "Epoch 99/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5409 - loss: 1.5576 - val_accuracy: 0.5336 - val_loss: 1.5575\n",
      "Epoch 100/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5141 - loss: 1.5936 - val_accuracy: 0.5560 - val_loss: 1.5330\n",
      "Epoch 101/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5496 - loss: 1.5187 - val_accuracy: 0.5418 - val_loss: 1.5592\n",
      "Epoch 102/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5139 - loss: 1.5789 - val_accuracy: 0.5377 - val_loss: 1.5667\n",
      "Epoch 103/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5555 - loss: 1.5435 - val_accuracy: 0.5295 - val_loss: 1.5769\n",
      "Epoch 104/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5399 - loss: 1.5389 - val_accuracy: 0.5438 - val_loss: 1.5606\n",
      "Epoch 105/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5176 - loss: 1.5457 - val_accuracy: 0.5479 - val_loss: 1.5246\n",
      "Epoch 106/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5335 - loss: 1.5576 - val_accuracy: 0.5499 - val_loss: 1.5712\n",
      "Epoch 107/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5339 - loss: 1.5705 - val_accuracy: 0.5438 - val_loss: 1.5423\n",
      "Epoch 108/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5515 - loss: 1.5346 - val_accuracy: 0.5580 - val_loss: 1.5291\n",
      "Epoch 109/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5277 - loss: 1.5601 - val_accuracy: 0.5418 - val_loss: 1.5477\n",
      "Epoch 110/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5222 - loss: 1.5510 - val_accuracy: 0.5418 - val_loss: 1.5344\n",
      "Epoch 111/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5173 - loss: 1.5662 - val_accuracy: 0.5662 - val_loss: 1.5303\n",
      "Epoch 112/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5234 - loss: 1.5677 - val_accuracy: 0.5295 - val_loss: 1.5636\n",
      "Epoch 113/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5233 - loss: 1.5740 - val_accuracy: 0.5193 - val_loss: 1.5742\n",
      "Epoch 114/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5292 - loss: 1.5407 - val_accuracy: 0.5214 - val_loss: 1.5661\n",
      "Epoch 115/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5149 - loss: 1.5634 - val_accuracy: 0.5540 - val_loss: 1.5419\n",
      "Epoch 116/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5324 - loss: 1.5315 - val_accuracy: 0.5540 - val_loss: 1.5193\n",
      "Epoch 117/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5496 - loss: 1.5224 - val_accuracy: 0.5397 - val_loss: 1.5309\n",
      "Epoch 118/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5276 - loss: 1.5389 - val_accuracy: 0.5132 - val_loss: 1.5639\n",
      "Epoch 119/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5548 - loss: 1.5442 - val_accuracy: 0.5438 - val_loss: 1.5324\n",
      "Epoch 120/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5586 - loss: 1.5094 - val_accuracy: 0.5275 - val_loss: 1.5963\n",
      "Epoch 121/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 1.5551 - val_accuracy: 0.5438 - val_loss: 1.5347\n",
      "Epoch 122/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5250 - loss: 1.5447 - val_accuracy: 0.5214 - val_loss: 1.5514\n",
      "Epoch 123/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5193 - loss: 1.5603 - val_accuracy: 0.5193 - val_loss: 1.5726\n",
      "Epoch 124/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5420 - loss: 1.5264 - val_accuracy: 0.5519 - val_loss: 1.5297\n",
      "Epoch 125/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5427 - loss: 1.5629 - val_accuracy: 0.5580 - val_loss: 1.5397\n",
      "Epoch 126/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5358 - loss: 1.5596 - val_accuracy: 0.5458 - val_loss: 1.5185\n",
      "Epoch 127/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5470 - loss: 1.5071 - val_accuracy: 0.5519 - val_loss: 1.5191\n",
      "Epoch 128/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5528 - loss: 1.5332 - val_accuracy: 0.5540 - val_loss: 1.5055\n",
      "Epoch 129/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5612 - loss: 1.5285 - val_accuracy: 0.5397 - val_loss: 1.5333\n",
      "Epoch 130/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5156 - loss: 1.5594 - val_accuracy: 0.4847 - val_loss: 1.5829\n",
      "Epoch 131/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5265 - loss: 1.5875 - val_accuracy: 0.5397 - val_loss: 1.5420\n",
      "Epoch 132/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5389 - loss: 1.5250 - val_accuracy: 0.5255 - val_loss: 1.5539\n",
      "Epoch 133/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5264 - loss: 1.5377 - val_accuracy: 0.5682 - val_loss: 1.5074\n",
      "Epoch 134/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5510 - loss: 1.5232 - val_accuracy: 0.5336 - val_loss: 1.5347\n",
      "Epoch 135/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5500 - loss: 1.5148 - val_accuracy: 0.5662 - val_loss: 1.5188\n",
      "Epoch 136/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5230 - loss: 1.5320 - val_accuracy: 0.5438 - val_loss: 1.5381\n",
      "Epoch 137/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5410 - loss: 1.5351 - val_accuracy: 0.5540 - val_loss: 1.5264\n",
      "Epoch 138/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5346 - loss: 1.5357 - val_accuracy: 0.5458 - val_loss: 1.5337\n",
      "Epoch 139/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5662 - loss: 1.4963 - val_accuracy: 0.5438 - val_loss: 1.5515\n",
      "Epoch 140/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5302 - loss: 1.5582 - val_accuracy: 0.5377 - val_loss: 1.5204\n",
      "Epoch 141/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5392 - loss: 1.5283 - val_accuracy: 0.5316 - val_loss: 1.5339\n",
      "Epoch 142/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5560 - loss: 1.5483 - val_accuracy: 0.5438 - val_loss: 1.5447\n",
      "Epoch 143/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5221 - loss: 1.5566 - val_accuracy: 0.5458 - val_loss: 1.5149\n",
      "Epoch 144/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5506 - loss: 1.4909 - val_accuracy: 0.5316 - val_loss: 1.5541\n",
      "Epoch 145/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5041 - loss: 1.5810 - val_accuracy: 0.5418 - val_loss: 1.5184\n",
      "Epoch 146/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5160 - loss: 1.5413 - val_accuracy: 0.5601 - val_loss: 1.5071\n",
      "Epoch 147/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5512 - loss: 1.4987 - val_accuracy: 0.5214 - val_loss: 1.5708\n",
      "Epoch 148/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5234 - loss: 1.5426 - val_accuracy: 0.5377 - val_loss: 1.5127\n",
      "Epoch 149/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5410 - loss: 1.5082 - val_accuracy: 0.5662 - val_loss: 1.5052\n",
      "Epoch 150/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5356 - loss: 1.5162 - val_accuracy: 0.5621 - val_loss: 1.5040\n",
      "Epoch 151/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5475 - loss: 1.5052 - val_accuracy: 0.5377 - val_loss: 1.5162\n",
      "Epoch 152/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5615 - loss: 1.4944 - val_accuracy: 0.5356 - val_loss: 1.5239\n",
      "Epoch 153/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5600 - loss: 1.5039 - val_accuracy: 0.5418 - val_loss: 1.5300\n",
      "Epoch 154/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5239 - loss: 1.5494 - val_accuracy: 0.5540 - val_loss: 1.5048\n",
      "Epoch 155/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5436 - loss: 1.4943 - val_accuracy: 0.5031 - val_loss: 1.5512\n",
      "Epoch 156/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5052 - loss: 1.5774 - val_accuracy: 0.5438 - val_loss: 1.5010\n",
      "Epoch 157/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5261 - loss: 1.5306 - val_accuracy: 0.5458 - val_loss: 1.5194\n",
      "Epoch 158/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5420 - loss: 1.5211 - val_accuracy: 0.5377 - val_loss: 1.5122\n",
      "Epoch 159/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5576 - loss: 1.4667 - val_accuracy: 0.5580 - val_loss: 1.4939\n",
      "Epoch 160/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5385 - loss: 1.5414 - val_accuracy: 0.5356 - val_loss: 1.5470\n",
      "Epoch 161/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5421 - loss: 1.5400 - val_accuracy: 0.5377 - val_loss: 1.5077\n",
      "Epoch 162/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5484 - loss: 1.5001 - val_accuracy: 0.5255 - val_loss: 1.5172\n",
      "Epoch 163/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5490 - loss: 1.5105 - val_accuracy: 0.5479 - val_loss: 1.5364\n",
      "Epoch 164/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5434 - loss: 1.5227 - val_accuracy: 0.5499 - val_loss: 1.5161\n",
      "Epoch 165/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5508 - loss: 1.5383 - val_accuracy: 0.4990 - val_loss: 1.5907\n",
      "Epoch 166/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5429 - loss: 1.5393 - val_accuracy: 0.5662 - val_loss: 1.5083\n",
      "Epoch 167/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5555 - loss: 1.5052 - val_accuracy: 0.5499 - val_loss: 1.5012\n",
      "Epoch 168/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5377 - loss: 1.5247 - val_accuracy: 0.5499 - val_loss: 1.5028\n",
      "Epoch 169/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5479 - loss: 1.5249 - val_accuracy: 0.5499 - val_loss: 1.5317\n",
      "Epoch 170/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5621 - loss: 1.5186 - val_accuracy: 0.5214 - val_loss: 1.5410\n",
      "Epoch 171/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5766 - loss: 1.4757 - val_accuracy: 0.5479 - val_loss: 1.5180\n",
      "Epoch 172/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4941 - loss: 1.5781 - val_accuracy: 0.5356 - val_loss: 1.5266\n",
      "Epoch 173/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5460 - loss: 1.5078 - val_accuracy: 0.5458 - val_loss: 1.4994\n",
      "Epoch 174/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5326 - loss: 1.5190 - val_accuracy: 0.5336 - val_loss: 1.4986\n",
      "Epoch 175/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5495 - loss: 1.4926 - val_accuracy: 0.5193 - val_loss: 1.5210\n",
      "Epoch 176/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5331 - loss: 1.5206 - val_accuracy: 0.5642 - val_loss: 1.4947\n",
      "Epoch 177/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5641 - loss: 1.4667 - val_accuracy: 0.5438 - val_loss: 1.4944\n",
      "Epoch 178/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5700 - loss: 1.4758 - val_accuracy: 0.5356 - val_loss: 1.5057\n",
      "Epoch 179/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5165 - loss: 1.5526 - val_accuracy: 0.5804 - val_loss: 1.4948\n",
      "Epoch 180/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5417 - loss: 1.4988 - val_accuracy: 0.5621 - val_loss: 1.4968\n",
      "Epoch 181/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5754 - loss: 1.4600 - val_accuracy: 0.5540 - val_loss: 1.4977\n",
      "Epoch 182/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5408 - loss: 1.5234 - val_accuracy: 0.5418 - val_loss: 1.5010\n",
      "Epoch 183/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5344 - loss: 1.5318 - val_accuracy: 0.5682 - val_loss: 1.4994\n",
      "Epoch 184/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5205 - loss: 1.5561 - val_accuracy: 0.5580 - val_loss: 1.4790\n",
      "Epoch 185/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5741 - loss: 1.4666 - val_accuracy: 0.5173 - val_loss: 1.5252\n",
      "Epoch 186/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5576 - loss: 1.4965 - val_accuracy: 0.5397 - val_loss: 1.5288\n",
      "Epoch 187/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5329 - loss: 1.5081 - val_accuracy: 0.5377 - val_loss: 1.5106\n",
      "Epoch 188/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5597 - loss: 1.4917 - val_accuracy: 0.5438 - val_loss: 1.5145\n",
      "Epoch 189/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5294 - loss: 1.4966 - val_accuracy: 0.5601 - val_loss: 1.4951\n",
      "Epoch 190/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5472 - loss: 1.4957 - val_accuracy: 0.5499 - val_loss: 1.5024\n",
      "Epoch 191/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5547 - loss: 1.5053 - val_accuracy: 0.5540 - val_loss: 1.4836\n",
      "Epoch 192/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5630 - loss: 1.4855 - val_accuracy: 0.5438 - val_loss: 1.4988\n",
      "Epoch 193/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5423 - loss: 1.5058 - val_accuracy: 0.5112 - val_loss: 1.5530\n",
      "Epoch 194/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5604 - loss: 1.5034 - val_accuracy: 0.5499 - val_loss: 1.4970\n",
      "Epoch 195/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5460 - loss: 1.4880 - val_accuracy: 0.5540 - val_loss: 1.4913\n",
      "Epoch 196/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5441 - loss: 1.4844 - val_accuracy: 0.5540 - val_loss: 1.4988\n",
      "Epoch 197/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5477 - loss: 1.5082 - val_accuracy: 0.5703 - val_loss: 1.5004\n",
      "Epoch 198/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5322 - loss: 1.5095 - val_accuracy: 0.5580 - val_loss: 1.5068\n",
      "Epoch 199/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5711 - loss: 1.4967 - val_accuracy: 0.5703 - val_loss: 1.4858\n",
      "Epoch 200/200\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5456 - loss: 1.5024 - val_accuracy: 0.5499 - val_loss: 1.5049\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),  # Lower than default\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Longer patience for regularized models\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,  # Let early stopping decide\n",
    "    batch_size=32,  # Smaller batches help regularization\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbb0531-8106-4a4e-897e-bddde50e740f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.axhline(y=max(history.history['val_accuracy']), \n",
    "            color='r', linestyle='--', \n",
    "            label=f'Best Val: {max(history.history[\"val_accuracy\"]):.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0e3b357-3815-427f-a273-2b7ef361ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature stats - Mean: 1.5062380383177214e-17 Std: 0.9990649834327573\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature stats - Mean:\", X_train.mean(), \"Std:\", X_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a869be4c-5a57-4ec3-bd2d-90e14be32bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(96, activation='relu', \n",
    "          kernel_regularizer=l1_l2(l1=0.001, l2=0.01),\n",
    "          input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu',\n",
    "          kernel_regularizer=l1_l2(l1=0.001, l2=0.005)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ec2122-f9ef-49d4-afaa-1cf7118df191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1175 - loss: 7.4433 - val_accuracy: 0.2138 - val_loss: 6.6714 - learning_rate: 8.0000e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2033 - loss: 6.5626 - val_accuracy: 0.2749 - val_loss: 5.9770 - learning_rate: 8.0000e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2686 - loss: 5.8247 - val_accuracy: 0.3177 - val_loss: 5.3480 - learning_rate: 8.0000e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2952 - loss: 5.2097 - val_accuracy: 0.3605 - val_loss: 4.8073 - learning_rate: 8.0000e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3650 - loss: 4.6643 - val_accuracy: 0.4420 - val_loss: 4.3626 - learning_rate: 8.0000e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3908 - loss: 4.2372 - val_accuracy: 0.4807 - val_loss: 3.9980 - learning_rate: 8.0000e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4733 - loss: 3.8603 - val_accuracy: 0.4908 - val_loss: 3.7024 - learning_rate: 8.0000e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4809 - loss: 3.5367 - val_accuracy: 0.5132 - val_loss: 3.4128 - learning_rate: 8.0000e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5127 - loss: 3.2935 - val_accuracy: 0.5336 - val_loss: 3.1755 - learning_rate: 8.0000e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5082 - loss: 3.0993 - val_accuracy: 0.5356 - val_loss: 2.9888 - learning_rate: 8.0000e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5431 - loss: 2.8791 - val_accuracy: 0.5479 - val_loss: 2.8273 - learning_rate: 8.0000e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5642 - loss: 2.6706 - val_accuracy: 0.5377 - val_loss: 2.6996 - learning_rate: 8.0000e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5794 - loss: 2.5867 - val_accuracy: 0.5642 - val_loss: 2.5770 - learning_rate: 8.0000e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6013 - loss: 2.4243 - val_accuracy: 0.5560 - val_loss: 2.4497 - learning_rate: 8.0000e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6260 - loss: 2.2925 - val_accuracy: 0.5703 - val_loss: 2.3552 - learning_rate: 8.0000e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6254 - loss: 2.1974 - val_accuracy: 0.5743 - val_loss: 2.2782 - learning_rate: 8.0000e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6324 - loss: 2.1284 - val_accuracy: 0.5723 - val_loss: 2.2551 - learning_rate: 8.0000e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6415 - loss: 2.0232 - val_accuracy: 0.5845 - val_loss: 2.1549 - learning_rate: 8.0000e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6487 - loss: 1.9839 - val_accuracy: 0.5927 - val_loss: 2.0999 - learning_rate: 8.0000e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6266 - loss: 1.9485 - val_accuracy: 0.5662 - val_loss: 2.0679 - learning_rate: 8.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6626 - loss: 1.8537 - val_accuracy: 0.5784 - val_loss: 2.0092 - learning_rate: 8.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6668 - loss: 1.8260 - val_accuracy: 0.5906 - val_loss: 1.9591 - learning_rate: 8.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6853 - loss: 1.7337 - val_accuracy: 0.5743 - val_loss: 1.9613 - learning_rate: 8.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 1.7252 - val_accuracy: 0.5662 - val_loss: 1.9381 - learning_rate: 8.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6883 - loss: 1.6843 - val_accuracy: 0.5845 - val_loss: 1.8516 - learning_rate: 8.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6871 - loss: 1.6360 - val_accuracy: 0.5886 - val_loss: 1.8387 - learning_rate: 8.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 1.5945 - val_accuracy: 0.5988 - val_loss: 1.8030 - learning_rate: 8.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6742 - loss: 1.5992 - val_accuracy: 0.5927 - val_loss: 1.8061 - learning_rate: 8.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6888 - loss: 1.5406 - val_accuracy: 0.6151 - val_loss: 1.7648 - learning_rate: 8.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 1.4875 - val_accuracy: 0.5886 - val_loss: 1.7562 - learning_rate: 8.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6982 - loss: 1.5011 - val_accuracy: 0.6090 - val_loss: 1.7414 - learning_rate: 8.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6966 - loss: 1.4775 - val_accuracy: 0.6029 - val_loss: 1.7147 - learning_rate: 8.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7054 - loss: 1.4820 - val_accuracy: 0.6029 - val_loss: 1.7111 - learning_rate: 8.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7125 - loss: 1.4544 - val_accuracy: 0.5967 - val_loss: 1.6834 - learning_rate: 8.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7075 - loss: 1.3987 - val_accuracy: 0.5866 - val_loss: 1.7447 - learning_rate: 8.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6836 - loss: 1.4779 - val_accuracy: 0.6090 - val_loss: 1.6741 - learning_rate: 8.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7212 - loss: 1.3744 - val_accuracy: 0.6191 - val_loss: 1.6550 - learning_rate: 8.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7501 - loss: 1.3171 - val_accuracy: 0.6008 - val_loss: 1.6655 - learning_rate: 8.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7330 - loss: 1.3386 - val_accuracy: 0.6008 - val_loss: 1.6607 - learning_rate: 8.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7120 - loss: 1.3753 - val_accuracy: 0.6151 - val_loss: 1.6668 - learning_rate: 8.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7156 - loss: 1.3703 - val_accuracy: 0.6191 - val_loss: 1.6527 - learning_rate: 8.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 1.2879 - val_accuracy: 0.6008 - val_loss: 1.6365 - learning_rate: 8.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7199 - loss: 1.3166 - val_accuracy: 0.6191 - val_loss: 1.6051 - learning_rate: 8.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7324 - loss: 1.2841 - val_accuracy: 0.6008 - val_loss: 1.6993 - learning_rate: 8.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7275 - loss: 1.2877 - val_accuracy: 0.6049 - val_loss: 1.6437 - learning_rate: 8.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7472 - loss: 1.2284 - val_accuracy: 0.6191 - val_loss: 1.6091 - learning_rate: 8.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7406 - loss: 1.2528 - val_accuracy: 0.6151 - val_loss: 1.6102 - learning_rate: 8.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7276 - loss: 1.2558 - val_accuracy: 0.6008 - val_loss: 1.6367 - learning_rate: 8.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7346 - loss: 1.2811 - val_accuracy: 0.6090 - val_loss: 1.6024 - learning_rate: 8.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7184 - loss: 1.3062 - val_accuracy: 0.6171 - val_loss: 1.6073 - learning_rate: 8.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7525 - loss: 1.2047 - val_accuracy: 0.5967 - val_loss: 1.6256 - learning_rate: 8.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7726 - loss: 1.1652 - val_accuracy: 0.6069 - val_loss: 1.6247 - learning_rate: 8.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7196 - loss: 1.2756 - val_accuracy: 0.5988 - val_loss: 1.5855 - learning_rate: 8.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7480 - loss: 1.2147 - val_accuracy: 0.5947 - val_loss: 1.6400 - learning_rate: 8.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7503 - loss: 1.1953 - val_accuracy: 0.5988 - val_loss: 1.5945 - learning_rate: 8.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7444 - loss: 1.2151 - val_accuracy: 0.6314 - val_loss: 1.5422 - learning_rate: 8.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7461 - loss: 1.2163 - val_accuracy: 0.6130 - val_loss: 1.5428 - learning_rate: 8.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7645 - loss: 1.1803 - val_accuracy: 0.6191 - val_loss: 1.5964 - learning_rate: 8.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7500 - loss: 1.1812 - val_accuracy: 0.6212 - val_loss: 1.5969 - learning_rate: 8.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7756 - loss: 1.1521 - val_accuracy: 0.5967 - val_loss: 1.6535 - learning_rate: 8.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7522 - loss: 1.1576 - val_accuracy: 0.6354 - val_loss: 1.5692 - learning_rate: 8.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7532 - loss: 1.1946 - val_accuracy: 0.6090 - val_loss: 1.6035 - learning_rate: 8.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7635 - loss: 1.1706 - val_accuracy: 0.6334 - val_loss: 1.5659 - learning_rate: 8.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7789 - loss: 1.1283 - val_accuracy: 0.6354 - val_loss: 1.5926 - learning_rate: 8.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7528 - loss: 1.1879 - val_accuracy: 0.6151 - val_loss: 1.5904 - learning_rate: 8.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7728 - loss: 1.1409 - val_accuracy: 0.6375 - val_loss: 1.5842 - learning_rate: 8.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7757 - loss: 1.1272 - val_accuracy: 0.6415 - val_loss: 1.5263 - learning_rate: 4.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8056 - loss: 1.0329 - val_accuracy: 0.6477 - val_loss: 1.5264 - learning_rate: 4.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8341 - loss: 1.0029 - val_accuracy: 0.6354 - val_loss: 1.5319 - learning_rate: 4.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8025 - loss: 1.0174 - val_accuracy: 0.6395 - val_loss: 1.5064 - learning_rate: 4.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8418 - loss: 0.9106 - val_accuracy: 0.6538 - val_loss: 1.5203 - learning_rate: 4.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8317 - loss: 0.9320 - val_accuracy: 0.6436 - val_loss: 1.5013 - learning_rate: 4.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8367 - loss: 0.9229 - val_accuracy: 0.6456 - val_loss: 1.5375 - learning_rate: 4.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8288 - loss: 0.9666 - val_accuracy: 0.6395 - val_loss: 1.5261 - learning_rate: 4.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8347 - loss: 0.9445 - val_accuracy: 0.6456 - val_loss: 1.5212 - learning_rate: 4.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8395 - loss: 0.8941 - val_accuracy: 0.6578 - val_loss: 1.4866 - learning_rate: 4.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8409 - loss: 0.8986 - val_accuracy: 0.6171 - val_loss: 1.5523 - learning_rate: 4.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8142 - loss: 0.9322 - val_accuracy: 0.6415 - val_loss: 1.5527 - learning_rate: 4.0000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8461 - loss: 0.8783 - val_accuracy: 0.6456 - val_loss: 1.5346 - learning_rate: 4.0000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8428 - loss: 0.8939 - val_accuracy: 0.6395 - val_loss: 1.5316 - learning_rate: 4.0000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8273 - loss: 0.8995 - val_accuracy: 0.6538 - val_loss: 1.5232 - learning_rate: 4.0000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8439 - loss: 0.8746 - val_accuracy: 0.6497 - val_loss: 1.5089 - learning_rate: 4.0000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8417 - loss: 0.8934 - val_accuracy: 0.6334 - val_loss: 1.5272 - learning_rate: 4.0000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8363 - loss: 0.8905 - val_accuracy: 0.6517 - val_loss: 1.4836 - learning_rate: 4.0000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8297 - loss: 0.9162 - val_accuracy: 0.6477 - val_loss: 1.4686 - learning_rate: 4.0000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8610 - loss: 0.8515 - val_accuracy: 0.6191 - val_loss: 1.5553 - learning_rate: 4.0000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8524 - loss: 0.8688 - val_accuracy: 0.6558 - val_loss: 1.5004 - learning_rate: 4.0000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8326 - loss: 0.9115 - val_accuracy: 0.6375 - val_loss: 1.5506 - learning_rate: 4.0000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8351 - loss: 0.8752 - val_accuracy: 0.6415 - val_loss: 1.5660 - learning_rate: 4.0000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8637 - loss: 0.8527 - val_accuracy: 0.6701 - val_loss: 1.5266 - learning_rate: 4.0000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8493 - loss: 0.8912 - val_accuracy: 0.6558 - val_loss: 1.5398 - learning_rate: 4.0000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8494 - loss: 0.8557 - val_accuracy: 0.6578 - val_loss: 1.5472 - learning_rate: 4.0000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8516 - loss: 0.8143 - val_accuracy: 0.6680 - val_loss: 1.5193 - learning_rate: 4.0000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8430 - loss: 0.8613 - val_accuracy: 0.6456 - val_loss: 1.5158 - learning_rate: 4.0000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8615 - loss: 0.8325 - val_accuracy: 0.6415 - val_loss: 1.5695 - learning_rate: 4.0000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8696 - loss: 0.8104 - val_accuracy: 0.6660 - val_loss: 1.5089 - learning_rate: 2.0000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8775 - loss: 0.7494 - val_accuracy: 0.6619 - val_loss: 1.5176 - learning_rate: 2.0000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8736 - loss: 0.7590 - val_accuracy: 0.6517 - val_loss: 1.5341 - learning_rate: 2.0000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8743 - loss: 0.7561 - val_accuracy: 0.6782 - val_loss: 1.4956 - learning_rate: 2.0000e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8979 - loss: 0.7188 - val_accuracy: 0.6762 - val_loss: 1.4775 - learning_rate: 2.0000e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8898 - loss: 0.7082 - val_accuracy: 0.6517 - val_loss: 1.5384 - learning_rate: 2.0000e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8984 - loss: 0.6988 - val_accuracy: 0.6701 - val_loss: 1.5303 - learning_rate: 2.0000e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8972 - loss: 0.7138 - val_accuracy: 0.6334 - val_loss: 1.5638 - learning_rate: 2.0000e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8907 - loss: 0.7101 - val_accuracy: 0.6538 - val_loss: 1.5398 - learning_rate: 2.0000e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8971 - loss: 0.7035 - val_accuracy: 0.6701 - val_loss: 1.4923 - learning_rate: 2.0000e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8966 - loss: 0.7057 - val_accuracy: 0.6619 - val_loss: 1.5129 - learning_rate: 1.0000e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9065 - loss: 0.6648 - val_accuracy: 0.6741 - val_loss: 1.5050 - learning_rate: 1.0000e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9183 - loss: 0.6359 - val_accuracy: 0.6701 - val_loss: 1.5127 - learning_rate: 1.0000e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9072 - loss: 0.6486 - val_accuracy: 0.6640 - val_loss: 1.5072 - learning_rate: 1.0000e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9146 - loss: 0.6419 - val_accuracy: 0.6660 - val_loss: 1.5131 - learning_rate: 1.0000e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9138 - loss: 0.6306 - val_accuracy: 0.6741 - val_loss: 1.5096 - learning_rate: 1.0000e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9264 - loss: 0.6345 - val_accuracy: 0.6741 - val_loss: 1.5017 - learning_rate: 1.0000e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.6551 - val_accuracy: 0.6517 - val_loss: 1.5401 - learning_rate: 1.0000e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9339 - loss: 0.6001 - val_accuracy: 0.6741 - val_loss: 1.5237 - learning_rate: 1.0000e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9226 - loss: 0.6111 - val_accuracy: 0.6680 - val_loss: 1.5352 - learning_rate: 1.0000e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9222 - loss: 0.6116 - val_accuracy: 0.6578 - val_loss: 1.5360 - learning_rate: 5.0000e-05\n",
      "Epoch 117/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9272 - loss: 0.5890 - val_accuracy: 0.6640 - val_loss: 1.5228 - learning_rate: 5.0000e-05\n",
      "Epoch 118/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9281 - loss: 0.5911 - val_accuracy: 0.6782 - val_loss: 1.5286 - learning_rate: 5.0000e-05\n",
      "Epoch 119/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9287 - loss: 0.5799 - val_accuracy: 0.6680 - val_loss: 1.5316 - learning_rate: 5.0000e-05\n",
      "Epoch 120/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.5699 - val_accuracy: 0.6741 - val_loss: 1.5357 - learning_rate: 5.0000e-05\n",
      "Epoch 121/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.5776 - val_accuracy: 0.6782 - val_loss: 1.5275 - learning_rate: 5.0000e-05\n",
      "Epoch 122/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9285 - loss: 0.5839 - val_accuracy: 0.6660 - val_loss: 1.5194 - learning_rate: 5.0000e-05\n",
      "Epoch 123/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9301 - loss: 0.5747 - val_accuracy: 0.6578 - val_loss: 1.5256 - learning_rate: 5.0000e-05\n",
      "Epoch 124/300\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9430 - loss: 0.5604 - val_accuracy: 0.6701 - val_loss: 1.5366 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "optimizer = Nadam(learning_rate=0.0008, clipnorm=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=25, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "029a4179-9357-4153-9bc9-339fd1bab1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, GaussianNoise\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),        # Define input shape here\n",
    "    GaussianNoise(0.1),                      # Add noise *after* input\n",
    "    Dense(96, activation='relu', kernel_regularizer=l1_l2(0.002, 0.02)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(0.001, 0.01)),\n",
    "    GaussianNoise(0.05),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74433d22-c019-4a79-8910-2c7d9e6aada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def triangular_lr(epoch):\n",
    "    base_lr = 0.0005\n",
    "    max_lr = 0.002\n",
    "    step_size = 8\n",
    "    cycle = np.floor(1 + epoch/(2*step_size))\n",
    "    x = np.abs(epoch/step_size - 2*cycle + 1)\n",
    "    return base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "\n",
    "callbacks.append(LearningRateScheduler(triangular_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fca2e93-6036-47e3-b3f4-d4322c8ec41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1961, 535, 1)\n",
      "y_train shape: (1961,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, timesteps, features)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d96ef3-9d60-40c0-8f93-5ecc171ccd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1961, 535, 1)\n",
      "Sample data: [ 0.07534863  0.64952045 -0.08281574]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape:\", X_train.shape)  # Must be (1961, 535, 1)\n",
    "print(\"Sample data:\", X_train[0, :3, 0])  # First sample, first 3 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdc27fc-b2f9-4dee-b64d-bfeafcbd8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56ab5258-e7f1-4496-b8d3-b03a9a1aa974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 1. Define num_classes if not already\n",
    "num_classes = len(np.unique(y_train))  # or just `y_train` if you're using original labels\n",
    "\n",
    "# ✅ 2. Define model with Input() layer (instead of input_shape=...)\n",
    "model = Sequential([\n",
    "    Input(shape=(535,)),  # 👈 Use Input here\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2696281-89a2-4547-a9b0-d43b311f8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.1698 - loss: 2.8721 - val_accuracy: 0.3462 - val_loss: 1.7988\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3174 - loss: 2.0067 - val_accuracy: 0.3971 - val_loss: 1.6931\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3400 - loss: 1.8232 - val_accuracy: 0.4196 - val_loss: 1.6185\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3860 - loss: 1.7123 - val_accuracy: 0.4807 - val_loss: 1.5063\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4240 - loss: 1.5219 - val_accuracy: 0.4664 - val_loss: 1.4186\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4258 - loss: 1.5245 - val_accuracy: 0.4969 - val_loss: 1.3463\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4683 - loss: 1.4030 - val_accuracy: 0.5458 - val_loss: 1.3501\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4791 - loss: 1.3229 - val_accuracy: 0.5275 - val_loss: 1.2977\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5165 - loss: 1.2966 - val_accuracy: 0.5316 - val_loss: 1.2383\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5319 - loss: 1.2567 - val_accuracy: 0.5255 - val_loss: 1.2510\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5398 - loss: 1.2116 - val_accuracy: 0.5560 - val_loss: 1.2165\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5335 - loss: 1.2130 - val_accuracy: 0.5560 - val_loss: 1.2044\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5819 - loss: 1.1103 - val_accuracy: 0.5377 - val_loss: 1.2080\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5794 - loss: 1.0626 - val_accuracy: 0.5356 - val_loss: 1.1789\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5857 - loss: 1.1187 - val_accuracy: 0.5662 - val_loss: 1.1628\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6012 - loss: 1.0652 - val_accuracy: 0.5601 - val_loss: 1.1590\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6126 - loss: 1.0602 - val_accuracy: 0.5906 - val_loss: 1.1387\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6372 - loss: 0.9694 - val_accuracy: 0.5580 - val_loss: 1.1608\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6296 - loss: 0.9801 - val_accuracy: 0.5682 - val_loss: 1.1379\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6272 - loss: 0.9897 - val_accuracy: 0.5723 - val_loss: 1.1587\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6298 - loss: 0.9620 - val_accuracy: 0.5804 - val_loss: 1.1097\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6831 - loss: 0.8731 - val_accuracy: 0.5886 - val_loss: 1.0991\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6564 - loss: 0.9206 - val_accuracy: 0.5723 - val_loss: 1.1071\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6824 - loss: 0.8417 - val_accuracy: 0.5825 - val_loss: 1.1322\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6731 - loss: 0.8868 - val_accuracy: 0.6029 - val_loss: 1.1135\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6954 - loss: 0.8275 - val_accuracy: 0.5662 - val_loss: 1.0955\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6758 - loss: 0.8609 - val_accuracy: 0.6130 - val_loss: 1.0573\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6929 - loss: 0.8248 - val_accuracy: 0.5906 - val_loss: 1.0815\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7042 - loss: 0.8276 - val_accuracy: 0.6069 - val_loss: 1.0575\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7032 - loss: 0.8243 - val_accuracy: 0.5886 - val_loss: 1.0774\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6975 - loss: 0.7814 - val_accuracy: 0.6008 - val_loss: 1.0941\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7193 - loss: 0.7803 - val_accuracy: 0.6029 - val_loss: 1.0785\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7189 - loss: 0.7398 - val_accuracy: 0.6008 - val_loss: 1.0947\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7084 - loss: 0.8146 - val_accuracy: 0.5845 - val_loss: 1.1524\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6702 - loss: 0.8941 - val_accuracy: 0.5906 - val_loss: 1.0884\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7339 - loss: 0.7256 - val_accuracy: 0.6130 - val_loss: 1.0956\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7215 - loss: 0.7661 - val_accuracy: 0.6151 - val_loss: 1.0981\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7477 - loss: 0.6989 - val_accuracy: 0.6130 - val_loss: 1.1138\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7430 - loss: 0.6580 - val_accuracy: 0.6110 - val_loss: 1.0888\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7366 - loss: 0.7297 - val_accuracy: 0.6008 - val_loss: 1.1236\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7415 - loss: 0.7188 - val_accuracy: 0.6395 - val_loss: 1.0521\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7486 - loss: 0.7000 - val_accuracy: 0.6171 - val_loss: 1.0843\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7531 - loss: 0.6712 - val_accuracy: 0.6049 - val_loss: 1.1170\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7637 - loss: 0.6208 - val_accuracy: 0.6273 - val_loss: 1.0956\n",
      "Epoch 45/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7730 - loss: 0.6693 - val_accuracy: 0.6151 - val_loss: 1.1354\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7530 - loss: 0.6954 - val_accuracy: 0.6191 - val_loss: 1.1040\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.6020 - val_accuracy: 0.6008 - val_loss: 1.1508\n",
      "Epoch 48/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7768 - loss: 0.5957 - val_accuracy: 0.6191 - val_loss: 1.1158\n",
      "Epoch 49/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7787 - loss: 0.6134 - val_accuracy: 0.6253 - val_loss: 1.1506\n",
      "Epoch 50/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7796 - loss: 0.5965 - val_accuracy: 0.6151 - val_loss: 1.1905\n",
      "Epoch 51/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7634 - loss: 0.6656 - val_accuracy: 0.6212 - val_loss: 1.1881\n",
      "Epoch 52/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7606 - loss: 0.6576 - val_accuracy: 0.6191 - val_loss: 1.1518\n",
      "Epoch 53/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7736 - loss: 0.6224 - val_accuracy: 0.6253 - val_loss: 1.1747\n",
      "Epoch 54/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7842 - loss: 0.6423 - val_accuracy: 0.6171 - val_loss: 1.1391\n",
      "Epoch 55/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7941 - loss: 0.5898 - val_accuracy: 0.6253 - val_loss: 1.1343\n",
      "Epoch 56/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7697 - loss: 0.6344 - val_accuracy: 0.6212 - val_loss: 1.1883\n",
      "Epoch 57/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7821 - loss: 0.5991 - val_accuracy: 0.6456 - val_loss: 1.1844\n",
      "Epoch 58/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8075 - loss: 0.5363 - val_accuracy: 0.6130 - val_loss: 1.1836\n",
      "Epoch 59/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8193 - loss: 0.5304 - val_accuracy: 0.6253 - val_loss: 1.1983\n",
      "Epoch 60/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7994 - loss: 0.5523 - val_accuracy: 0.6171 - val_loss: 1.1702\n",
      "Epoch 61/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8041 - loss: 0.5331 - val_accuracy: 0.6293 - val_loss: 1.1832\n",
      "Epoch 62/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8013 - loss: 0.5199 - val_accuracy: 0.6171 - val_loss: 1.2332\n",
      "Epoch 63/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7941 - loss: 0.5406 - val_accuracy: 0.6253 - val_loss: 1.2235\n",
      "Epoch 64/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7821 - loss: 0.5945 - val_accuracy: 0.6130 - val_loss: 1.2347\n",
      "Epoch 65/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8214 - loss: 0.4723 - val_accuracy: 0.6334 - val_loss: 1.2335\n",
      "Epoch 66/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8184 - loss: 0.5270 - val_accuracy: 0.6171 - val_loss: 1.2369\n",
      "Epoch 67/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8184 - loss: 0.5103 - val_accuracy: 0.6212 - val_loss: 1.3065\n",
      "Epoch 68/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8076 - loss: 0.4975 - val_accuracy: 0.6232 - val_loss: 1.2310\n",
      "Epoch 69/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8138 - loss: 0.5226 - val_accuracy: 0.6375 - val_loss: 1.2282\n",
      "Epoch 70/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8120 - loss: 0.5123 - val_accuracy: 0.6375 - val_loss: 1.2131\n",
      "Epoch 71/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8359 - loss: 0.4509 - val_accuracy: 0.6415 - val_loss: 1.2414\n",
      "Epoch 72/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7937 - loss: 0.5405 - val_accuracy: 0.6354 - val_loss: 1.2762\n",
      "Epoch 73/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8312 - loss: 0.5133 - val_accuracy: 0.6375 - val_loss: 1.2605\n",
      "Epoch 74/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8339 - loss: 0.4641 - val_accuracy: 0.6517 - val_loss: 1.2463\n",
      "Epoch 75/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8299 - loss: 0.4951 - val_accuracy: 0.6171 - val_loss: 1.2582\n",
      "Epoch 76/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8368 - loss: 0.4640 - val_accuracy: 0.6314 - val_loss: 1.2741\n",
      "Epoch 77/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8326 - loss: 0.5051 - val_accuracy: 0.6395 - val_loss: 1.2739\n",
      "Epoch 78/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8285 - loss: 0.4740 - val_accuracy: 0.6212 - val_loss: 1.3672\n",
      "Epoch 79/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8380 - loss: 0.4727 - val_accuracy: 0.6517 - val_loss: 1.2652\n",
      "Epoch 80/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8443 - loss: 0.4517 - val_accuracy: 0.6354 - val_loss: 1.2816\n",
      "Epoch 81/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8526 - loss: 0.4299 - val_accuracy: 0.6375 - val_loss: 1.2382\n",
      "Epoch 82/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8475 - loss: 0.4411 - val_accuracy: 0.6232 - val_loss: 1.2888\n",
      "Epoch 83/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8489 - loss: 0.4109 - val_accuracy: 0.6253 - val_loss: 1.3343\n",
      "Epoch 84/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8561 - loss: 0.4516 - val_accuracy: 0.6375 - val_loss: 1.2785\n",
      "Epoch 85/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8256 - loss: 0.5210 - val_accuracy: 0.6415 - val_loss: 1.2665\n",
      "Epoch 86/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8512 - loss: 0.4243 - val_accuracy: 0.6456 - val_loss: 1.2554\n",
      "Epoch 87/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8432 - loss: 0.4412 - val_accuracy: 0.6721 - val_loss: 1.2194\n",
      "Epoch 88/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8499 - loss: 0.4454 - val_accuracy: 0.6436 - val_loss: 1.2353\n",
      "Epoch 89/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8396 - loss: 0.4253 - val_accuracy: 0.6314 - val_loss: 1.2751\n",
      "Epoch 90/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8394 - loss: 0.4339 - val_accuracy: 0.6375 - val_loss: 1.3263\n",
      "Epoch 91/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8553 - loss: 0.4048 - val_accuracy: 0.6415 - val_loss: 1.2852\n",
      "Epoch 92/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8618 - loss: 0.4330 - val_accuracy: 0.6232 - val_loss: 1.3831\n",
      "Epoch 93/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8564 - loss: 0.4406 - val_accuracy: 0.6415 - val_loss: 1.3139\n",
      "Epoch 94/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8430 - loss: 0.4443 - val_accuracy: 0.6477 - val_loss: 1.2797\n",
      "Epoch 95/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8676 - loss: 0.3799 - val_accuracy: 0.6273 - val_loss: 1.3069\n",
      "Epoch 96/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8611 - loss: 0.3709 - val_accuracy: 0.6293 - val_loss: 1.3157\n",
      "Epoch 97/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8726 - loss: 0.3608 - val_accuracy: 0.6538 - val_loss: 1.3094\n",
      "Epoch 98/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8594 - loss: 0.3905 - val_accuracy: 0.6558 - val_loss: 1.3487\n",
      "Epoch 99/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8760 - loss: 0.3789 - val_accuracy: 0.6538 - val_loss: 1.3117\n",
      "Epoch 100/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8746 - loss: 0.3418 - val_accuracy: 0.6619 - val_loss: 1.2859\n"
     ]
    }
   ],
   "source": [
    "# 3. Train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cf7cea-df55-4a0f-b59c-41e194001306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [151] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 300 features (reduces overfitting)\n",
    "selector = SelectKBest(f_classif, k=300)\n",
    "X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "X_test_sel = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d2c480a-ea6b-426c-b36a-91ea19d2bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_sel.shape[1],)),  # ✅ Replaces input_shape\n",
    "    Dense(512, activation='relu',\n",
    "          kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    Dense(256, activation='relu',\n",
    "          kernel_regularizer=regularizers.l2(0.005)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba92efc8-a895-4efe-9e2d-1f2748643b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1655 - loss: 15.2825 - val_accuracy: 0.2546 - val_loss: 12.7232 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2445 - loss: 12.9102 - val_accuracy: 0.2892 - val_loss: 11.2233 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2859 - loss: 11.2867 - val_accuracy: 0.3483 - val_loss: 9.8116 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3310 - loss: 9.8234 - val_accuracy: 0.3951 - val_loss: 8.5546 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3825 - loss: 8.5583 - val_accuracy: 0.3910 - val_loss: 7.5361 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4029 - loss: 7.6169 - val_accuracy: 0.4481 - val_loss: 6.6029 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4142 - loss: 6.7101 - val_accuracy: 0.4399 - val_loss: 5.9039 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4469 - loss: 6.0171 - val_accuracy: 0.4460 - val_loss: 5.2953 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4410 - loss: 5.4776 - val_accuracy: 0.4786 - val_loss: 4.8132 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4546 - loss: 5.0233 - val_accuracy: 0.4705 - val_loss: 4.4335 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4640 - loss: 4.6360 - val_accuracy: 0.4807 - val_loss: 4.1067 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4809 - loss: 4.3102 - val_accuracy: 0.4969 - val_loss: 3.8094 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4914 - loss: 4.0174 - val_accuracy: 0.4603 - val_loss: 3.6584 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4746 - loss: 3.8391 - val_accuracy: 0.5010 - val_loss: 3.4694 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4794 - loss: 3.6744 - val_accuracy: 0.4745 - val_loss: 3.3291 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4923 - loss: 3.4690 - val_accuracy: 0.4521 - val_loss: 3.2232 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5050 - loss: 3.4010 - val_accuracy: 0.4786 - val_loss: 3.0821 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5005 - loss: 3.2425 - val_accuracy: 0.5071 - val_loss: 2.9505 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4993 - loss: 3.1707 - val_accuracy: 0.4807 - val_loss: 2.9067 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4902 - loss: 3.1211 - val_accuracy: 0.4053 - val_loss: 2.9422 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5083 - loss: 3.0872 - val_accuracy: 0.4725 - val_loss: 2.8156 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4904 - loss: 2.9789 - val_accuracy: 0.4460 - val_loss: 2.8332 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5132 - loss: 2.9177 - val_accuracy: 0.4908 - val_loss: 2.6983 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5069 - loss: 2.8541 - val_accuracy: 0.4664 - val_loss: 2.7157 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4851 - loss: 2.8792 - val_accuracy: 0.4786 - val_loss: 2.6836 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5160 - loss: 2.8366 - val_accuracy: 0.5092 - val_loss: 2.6119 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5170 - loss: 2.7891 - val_accuracy: 0.4990 - val_loss: 2.5383 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5283 - loss: 2.7474 - val_accuracy: 0.5051 - val_loss: 2.5249 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4999 - loss: 2.7613 - val_accuracy: 0.4868 - val_loss: 2.5167 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5257 - loss: 2.6402 - val_accuracy: 0.4664 - val_loss: 2.5352 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5165 - loss: 2.7006 - val_accuracy: 0.4847 - val_loss: 2.4557 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5154 - loss: 2.6354 - val_accuracy: 0.4929 - val_loss: 2.5029 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5434 - loss: 2.5864 - val_accuracy: 0.4786 - val_loss: 2.5050 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5142 - loss: 2.6004 - val_accuracy: 0.5031 - val_loss: 2.4374 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5261 - loss: 2.5643 - val_accuracy: 0.4216 - val_loss: 2.5224 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5297 - loss: 2.5633 - val_accuracy: 0.4908 - val_loss: 2.4009 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5001 - loss: 2.5610 - val_accuracy: 0.5112 - val_loss: 2.4111 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5287 - loss: 2.5371 - val_accuracy: 0.4847 - val_loss: 2.4043 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5395 - loss: 2.4716 - val_accuracy: 0.4216 - val_loss: 2.4867 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5173 - loss: 2.5169 - val_accuracy: 0.4868 - val_loss: 2.3723 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5179 - loss: 2.5198 - val_accuracy: 0.4949 - val_loss: 2.3682 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5159 - loss: 2.5793 - val_accuracy: 0.5193 - val_loss: 2.3319 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5214 - loss: 2.5042 - val_accuracy: 0.4725 - val_loss: 2.4021 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5161 - loss: 2.5203 - val_accuracy: 0.4827 - val_loss: 2.3709 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5264 - loss: 2.4607 - val_accuracy: 0.4949 - val_loss: 2.3706 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5101 - loss: 2.5558 - val_accuracy: 0.4908 - val_loss: 2.3313 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5211 - loss: 2.5002 - val_accuracy: 0.4949 - val_loss: 2.3056 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5297 - loss: 2.4436 - val_accuracy: 0.5071 - val_loss: 2.3434 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5448 - loss: 2.4322 - val_accuracy: 0.4766 - val_loss: 2.3355 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5238 - loss: 2.4796 - val_accuracy: 0.4745 - val_loss: 2.3564 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5249 - loss: 2.4362 - val_accuracy: 0.4990 - val_loss: 2.2774 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5312 - loss: 2.4094 - val_accuracy: 0.5112 - val_loss: 2.3027 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5351 - loss: 2.4269 - val_accuracy: 0.4990 - val_loss: 2.3699 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5326 - loss: 2.4862 - val_accuracy: 0.4725 - val_loss: 2.3565 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5265 - loss: 2.4365 - val_accuracy: 0.4766 - val_loss: 2.2926 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5317 - loss: 2.3968 - val_accuracy: 0.4969 - val_loss: 2.3037 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5524 - loss: 2.3414 - val_accuracy: 0.4908 - val_loss: 2.1718 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5580 - loss: 2.2379 - val_accuracy: 0.5071 - val_loss: 2.1076 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5660 - loss: 2.1959 - val_accuracy: 0.5336 - val_loss: 2.0573 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5713 - loss: 2.1389 - val_accuracy: 0.5234 - val_loss: 2.0259 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5889 - loss: 2.0766 - val_accuracy: 0.5295 - val_loss: 1.9787 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5927 - loss: 1.9951 - val_accuracy: 0.5132 - val_loss: 2.0024 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5677 - loss: 2.1003 - val_accuracy: 0.5173 - val_loss: 2.0313 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5561 - loss: 2.1480 - val_accuracy: 0.5234 - val_loss: 2.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5836 - loss: 2.0125 - val_accuracy: 0.5051 - val_loss: 2.0104 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5915 - loss: 1.9968 - val_accuracy: 0.5255 - val_loss: 1.9965 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5847 - loss: 1.9705 - val_accuracy: 0.5397 - val_loss: 1.9019 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6166 - loss: 1.8818 - val_accuracy: 0.5418 - val_loss: 1.8727 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6230 - loss: 1.7845 - val_accuracy: 0.5479 - val_loss: 1.8344 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6368 - loss: 1.7612 - val_accuracy: 0.5031 - val_loss: 1.8453 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6157 - loss: 1.8186 - val_accuracy: 0.5275 - val_loss: 1.8306 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6110 - loss: 1.7851 - val_accuracy: 0.5071 - val_loss: 1.8537 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6138 - loss: 1.7855 - val_accuracy: 0.5479 - val_loss: 1.8034 - learning_rate: 2.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6338 - loss: 1.7426 - val_accuracy: 0.5356 - val_loss: 1.7945 - learning_rate: 2.5000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6314 - loss: 1.7266 - val_accuracy: 0.5438 - val_loss: 1.7967 - learning_rate: 2.5000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6153 - loss: 1.7547 - val_accuracy: 0.5397 - val_loss: 1.7770 - learning_rate: 2.5000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6183 - loss: 1.7138 - val_accuracy: 0.5112 - val_loss: 1.8178 - learning_rate: 2.5000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6232 - loss: 1.7383 - val_accuracy: 0.5418 - val_loss: 1.7635 - learning_rate: 2.5000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6308 - loss: 1.7066 - val_accuracy: 0.5336 - val_loss: 1.7828 - learning_rate: 2.5000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6407 - loss: 1.7180 - val_accuracy: 0.5153 - val_loss: 1.8050 - learning_rate: 2.5000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6351 - loss: 1.7134 - val_accuracy: 0.5275 - val_loss: 1.8128 - learning_rate: 2.5000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6219 - loss: 1.6937 - val_accuracy: 0.5499 - val_loss: 1.7738 - learning_rate: 2.5000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6438 - loss: 1.6726 - val_accuracy: 0.5418 - val_loss: 1.7579 - learning_rate: 2.5000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6775 - loss: 1.5997 - val_accuracy: 0.5356 - val_loss: 1.7584 - learning_rate: 2.5000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6344 - loss: 1.7151 - val_accuracy: 0.5234 - val_loss: 1.7759 - learning_rate: 2.5000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6342 - loss: 1.6846 - val_accuracy: 0.5621 - val_loss: 1.7362 - learning_rate: 2.5000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6490 - loss: 1.6647 - val_accuracy: 0.5336 - val_loss: 1.7689 - learning_rate: 2.5000e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6249 - loss: 1.7000 - val_accuracy: 0.5377 - val_loss: 1.7632 - learning_rate: 2.5000e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6369 - loss: 1.6557 - val_accuracy: 0.5336 - val_loss: 1.7785 - learning_rate: 2.5000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6226 - loss: 1.6766 - val_accuracy: 0.4949 - val_loss: 1.8504 - learning_rate: 2.5000e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6172 - loss: 1.6790 - val_accuracy: 0.5377 - val_loss: 1.7400 - learning_rate: 2.5000e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6420 - loss: 1.5776 - val_accuracy: 0.5458 - val_loss: 1.7219 - learning_rate: 1.2500e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6573 - loss: 1.6099 - val_accuracy: 0.5377 - val_loss: 1.6829 - learning_rate: 1.2500e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6651 - loss: 1.5024 - val_accuracy: 0.5275 - val_loss: 1.7150 - learning_rate: 1.2500e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6493 - loss: 1.5134 - val_accuracy: 0.5336 - val_loss: 1.7349 - learning_rate: 1.2500e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6822 - loss: 1.4624 - val_accuracy: 0.5458 - val_loss: 1.6750 - learning_rate: 1.2500e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6994 - loss: 1.4225 - val_accuracy: 0.5418 - val_loss: 1.6691 - learning_rate: 1.2500e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6878 - loss: 1.4425 - val_accuracy: 0.5479 - val_loss: 1.6552 - learning_rate: 1.2500e-04\n",
      "Epoch 99/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6946 - loss: 1.4530 - val_accuracy: 0.5499 - val_loss: 1.6630 - learning_rate: 1.2500e-04\n",
      "Epoch 100/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6509 - loss: 1.5450 - val_accuracy: 0.5295 - val_loss: 1.7094 - learning_rate: 1.2500e-04\n",
      "Epoch 101/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6857 - loss: 1.4121 - val_accuracy: 0.5377 - val_loss: 1.7303 - learning_rate: 1.2500e-04\n",
      "Epoch 102/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6617 - loss: 1.4765 - val_accuracy: 0.5499 - val_loss: 1.6762 - learning_rate: 1.2500e-04\n",
      "Epoch 103/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7029 - loss: 1.4006 - val_accuracy: 0.5601 - val_loss: 1.6357 - learning_rate: 1.2500e-04\n",
      "Epoch 104/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6914 - loss: 1.4300 - val_accuracy: 0.5418 - val_loss: 1.7175 - learning_rate: 1.2500e-04\n",
      "Epoch 105/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6998 - loss: 1.4086 - val_accuracy: 0.5621 - val_loss: 1.6561 - learning_rate: 1.2500e-04\n",
      "Epoch 106/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6982 - loss: 1.4097 - val_accuracy: 0.5499 - val_loss: 1.6598 - learning_rate: 1.2500e-04\n",
      "Epoch 107/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6919 - loss: 1.3877 - val_accuracy: 0.5560 - val_loss: 1.6533 - learning_rate: 1.2500e-04\n",
      "Epoch 108/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7014 - loss: 1.3557 - val_accuracy: 0.5397 - val_loss: 1.6604 - learning_rate: 1.2500e-04\n",
      "Epoch 109/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7168 - loss: 1.3533 - val_accuracy: 0.5682 - val_loss: 1.6138 - learning_rate: 6.2500e-05\n",
      "Epoch 110/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7010 - loss: 1.3289 - val_accuracy: 0.5784 - val_loss: 1.6287 - learning_rate: 6.2500e-05\n",
      "Epoch 111/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7008 - loss: 1.3226 - val_accuracy: 0.5621 - val_loss: 1.6652 - learning_rate: 6.2500e-05\n",
      "Epoch 112/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7268 - loss: 1.3131 - val_accuracy: 0.5580 - val_loss: 1.6308 - learning_rate: 6.2500e-05\n",
      "Epoch 113/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7233 - loss: 1.3065 - val_accuracy: 0.5703 - val_loss: 1.6284 - learning_rate: 6.2500e-05\n",
      "Epoch 114/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6997 - loss: 1.3087 - val_accuracy: 0.5703 - val_loss: 1.6091 - learning_rate: 6.2500e-05\n",
      "Epoch 115/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7154 - loss: 1.2986 - val_accuracy: 0.5662 - val_loss: 1.6254 - learning_rate: 6.2500e-05\n",
      "Epoch 116/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7189 - loss: 1.2665 - val_accuracy: 0.5723 - val_loss: 1.6340 - learning_rate: 6.2500e-05\n",
      "Epoch 117/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7251 - loss: 1.2818 - val_accuracy: 0.5723 - val_loss: 1.6378 - learning_rate: 6.2500e-05\n",
      "Epoch 118/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7238 - loss: 1.2690 - val_accuracy: 0.5703 - val_loss: 1.6174 - learning_rate: 6.2500e-05\n",
      "Epoch 119/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7125 - loss: 1.2659 - val_accuracy: 0.5723 - val_loss: 1.6314 - learning_rate: 6.2500e-05\n",
      "Epoch 120/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7454 - loss: 1.2392 - val_accuracy: 0.5703 - val_loss: 1.6141 - learning_rate: 3.1250e-05\n",
      "Epoch 121/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7322 - loss: 1.2429 - val_accuracy: 0.5642 - val_loss: 1.6174 - learning_rate: 3.1250e-05\n",
      "Epoch 122/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7376 - loss: 1.1902 - val_accuracy: 0.5703 - val_loss: 1.6033 - learning_rate: 3.1250e-05\n",
      "Epoch 123/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7485 - loss: 1.1940 - val_accuracy: 0.5764 - val_loss: 1.5927 - learning_rate: 3.1250e-05\n",
      "Epoch 124/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7494 - loss: 1.1677 - val_accuracy: 0.5743 - val_loss: 1.6218 - learning_rate: 3.1250e-05\n",
      "Epoch 125/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7531 - loss: 1.1803 - val_accuracy: 0.5784 - val_loss: 1.5969 - learning_rate: 3.1250e-05\n",
      "Epoch 126/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7406 - loss: 1.2037 - val_accuracy: 0.5743 - val_loss: 1.5962 - learning_rate: 3.1250e-05\n",
      "Epoch 127/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7448 - loss: 1.2022 - val_accuracy: 0.5703 - val_loss: 1.6047 - learning_rate: 3.1250e-05\n",
      "Epoch 128/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7347 - loss: 1.2265 - val_accuracy: 0.5682 - val_loss: 1.6095 - learning_rate: 3.1250e-05\n",
      "Epoch 129/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7580 - loss: 1.1635 - val_accuracy: 0.5723 - val_loss: 1.5892 - learning_rate: 1.5625e-05\n",
      "Epoch 130/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7572 - loss: 1.1608 - val_accuracy: 0.5723 - val_loss: 1.5929 - learning_rate: 1.5625e-05\n",
      "Epoch 131/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7410 - loss: 1.1578 - val_accuracy: 0.5682 - val_loss: 1.5878 - learning_rate: 1.5625e-05\n",
      "Epoch 132/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7563 - loss: 1.1440 - val_accuracy: 0.5703 - val_loss: 1.5856 - learning_rate: 1.5625e-05\n",
      "Epoch 133/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7603 - loss: 1.1584 - val_accuracy: 0.5723 - val_loss: 1.5985 - learning_rate: 1.5625e-05\n",
      "Epoch 134/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 1.1677 - val_accuracy: 0.5642 - val_loss: 1.5880 - learning_rate: 1.5625e-05\n",
      "Epoch 135/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7554 - loss: 1.1436 - val_accuracy: 0.5743 - val_loss: 1.5848 - learning_rate: 1.5625e-05\n",
      "Epoch 136/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7515 - loss: 1.1501 - val_accuracy: 0.5642 - val_loss: 1.5931 - learning_rate: 1.5625e-05\n",
      "Epoch 137/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7493 - loss: 1.1506 - val_accuracy: 0.5723 - val_loss: 1.5807 - learning_rate: 1.5625e-05\n",
      "Epoch 138/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7545 - loss: 1.1394 - val_accuracy: 0.5723 - val_loss: 1.5761 - learning_rate: 1.5625e-05\n",
      "Epoch 139/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7719 - loss: 1.1046 - val_accuracy: 0.5743 - val_loss: 1.5876 - learning_rate: 1.5625e-05\n",
      "Epoch 140/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7599 - loss: 1.1032 - val_accuracy: 0.5784 - val_loss: 1.5731 - learning_rate: 1.5625e-05\n",
      "Epoch 141/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7659 - loss: 1.1224 - val_accuracy: 0.5866 - val_loss: 1.5862 - learning_rate: 1.5625e-05\n",
      "Epoch 142/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7681 - loss: 1.1147 - val_accuracy: 0.5845 - val_loss: 1.5751 - learning_rate: 1.5625e-05\n",
      "Epoch 143/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7455 - loss: 1.1617 - val_accuracy: 0.5743 - val_loss: 1.5801 - learning_rate: 1.5625e-05\n",
      "Epoch 144/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7740 - loss: 1.0986 - val_accuracy: 0.5743 - val_loss: 1.5840 - learning_rate: 1.5625e-05\n",
      "Epoch 145/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7642 - loss: 1.1187 - val_accuracy: 0.5845 - val_loss: 1.5759 - learning_rate: 1.5625e-05\n",
      "Epoch 146/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7573 - loss: 1.1064 - val_accuracy: 0.5825 - val_loss: 1.5736 - learning_rate: 7.8125e-06\n",
      "Epoch 147/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7704 - loss: 1.1469 - val_accuracy: 0.5845 - val_loss: 1.5764 - learning_rate: 7.8125e-06\n",
      "Epoch 148/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7693 - loss: 1.1082 - val_accuracy: 0.5886 - val_loss: 1.5801 - learning_rate: 7.8125e-06\n",
      "Epoch 149/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7661 - loss: 1.1434 - val_accuracy: 0.5703 - val_loss: 1.5748 - learning_rate: 7.8125e-06\n",
      "Epoch 150/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7669 - loss: 1.0900 - val_accuracy: 0.5804 - val_loss: 1.5756 - learning_rate: 7.8125e-06\n",
      "Epoch 151/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7751 - loss: 1.1145 - val_accuracy: 0.5825 - val_loss: 1.5756 - learning_rate: 3.9063e-06\n",
      "Epoch 152/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7683 - loss: 1.0873 - val_accuracy: 0.5764 - val_loss: 1.5798 - learning_rate: 3.9063e-06\n",
      "Epoch 153/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7663 - loss: 1.0876 - val_accuracy: 0.5804 - val_loss: 1.5819 - learning_rate: 3.9063e-06\n",
      "Epoch 154/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7525 - loss: 1.1287 - val_accuracy: 0.5784 - val_loss: 1.5770 - learning_rate: 3.9063e-06\n",
      "Epoch 155/200\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7709 - loss: 1.1053 - val_accuracy: 0.5804 - val_loss: 1.5769 - learning_rate: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Weight-decay optimized Adam\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Dynamic learning rate reduction\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_sel, y_train,\n",
    "    validation_data=(X_test_sel, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    class_weight=class_weight_dict  # Only if imbalance exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6f4234c-c971-425e-b726-ff46404618f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "TTA Val Accuracy: 0.5743380855397149\n"
     ]
    }
   ],
   "source": [
    "def TTA_predict(model, X, n_iter=5):\n",
    "    preds = []\n",
    "    for _ in range(n_iter):\n",
    "        # Add small noise during prediction\n",
    "        preds.append(model.predict(X * np.random.normal(1, 0.05, X.shape)))\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "y_probs = TTA_predict(model, X_test_sel)\n",
    "y_pred = y_probs.argmax(axis=1)\n",
    "print(\"TTA Val Accuracy:\", np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e1269db-d0c7-43b4-9bc0-e4d365610b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The estimator Sequential should be a classifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 14\u001b[0m\n\u001b[0;32m      8\u001b[0m model3 \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m     10\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m     11\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdnn\u001b[39m\u001b[38;5;124m'\u001b[39m, model1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, model2), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m'\u001b[39m, model3)],\n\u001b[0;32m     12\u001b[0m     voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mfit(X_train_sel, y_train)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble Val Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensemble\u001b[38;5;241m.\u001b[39mscore(X_test_sel, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:419\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_estimators()\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:234\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    236\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    237\u001b[0m             )\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "\u001b[1;31mValueError\u001b[0m: The estimator Sequential should be a classifier."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "\n",
    "# Train 3 different models\n",
    "model1 = model\n",
    "model2 = SVC(kernel='rbf', probability=True)\n",
    "model3 = GradientBoostingClassifier()\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('dnn', model1), ('svm', model2), ('gb', model3)],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train_sel, y_train)\n",
    "print(\"Ensemble Val Accuracy:\", ensemble.score(X_test_sel, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f6f7ef5-b1e9-4f58-8eb1-cc80df0c29ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikeras) (3.7.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikeras) (1.6.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras) (24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.0)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8caa54-4873-472b-8272-097f1b434500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_sel.shape[1],)),\n",
    "        Dense(512, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.005)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model1 = KerasClassifier(model=build_model, epochs=30, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd49ace9-5e40-4a67-93a5-fdb1ff8371ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m model3 \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m      7\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m      8\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdnn\u001b[39m\u001b[38;5;124m'\u001b[39m, model1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, model2), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m'\u001b[39m, model3)],\n\u001b[0;32m      9\u001b[0m     voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mfit(X_train_sel, y_train)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble Val Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensemble\u001b[38;5;241m.\u001b[39mscore(X_test_sel, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:419\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_estimators()\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:233\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m is_estimator_type \u001b[38;5;241m=\u001b[39m is_classifier \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m is_regressor\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    236\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    237\u001b[0m             )\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_tags(estimator)\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__sklearn_tags__()\n\u001b[0;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model2 = SVC(kernel='rbf', probability=True)\n",
    "model3 = GradientBoostingClassifier()\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('dnn', model1), ('svm', model2), ('gb', model3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble.fit(X_train_sel, y_train)\n",
    "print(\"Ensemble Val Accuracy:\", ensemble.score(X_test_sel, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb698af-f8c0-4e3b-8326-4ef51bb934e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasClassifier(\n",
      "\tmodel=<function build_model at 0x0000015E66BD2F20>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=32\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=0\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=30\n",
      "\tclass_weight=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35044e59-5fca-47d1-85a2-1847fef0caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1c64e5f-1e44-40b3-876f-96308acffef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_sel.shape[1],)),\n",
    "        Dense(512, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.005)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a9b938-6a60-45c9-befb-cb592398058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KerasClassifier(model=build_model, epochs=30, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2939ec0b-bfc4-4bb8-bea2-521dcf246705",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m model3 \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m      4\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m      5\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdnn\u001b[39m\u001b[38;5;124m'\u001b[39m, model1),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mfit(X_train_sel, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:419\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_estimators()\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:233\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m is_estimator_type \u001b[38;5;241m=\u001b[39m is_classifier \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m is_regressor\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    236\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    237\u001b[0m             )\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_tags(estimator)\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__sklearn_tags__()\n\u001b[0;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "model2 = SVC(kernel='rbf', probability=True)\n",
    "model3 = GradientBoostingClassifier()\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('dnn', model1),\n",
    "        ('svm', model2),\n",
    "        ('gb', model3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble.fit(X_train_sel, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9c356a-8ffe-4861-a54a-07931451ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      2\u001b[0m     Dense(\u001b[38;5;241m512\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m           kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.001\u001b[39m),  \u001b[38;5;66;03m# Reduced from 0.01\u001b[39;00m\n\u001b[0;32m      4\u001b[0m           input_shape\u001b[38;5;241m=\u001b[39m(X_train_sel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m      5\u001b[0m     BatchNormalization(),\n\u001b[0;32m      6\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),  \u001b[38;5;66;03m# Reduced from 0.6\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \n\u001b[0;32m      8\u001b[0m     Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m           kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.0005\u001b[39m)),  \u001b[38;5;66;03m# Reduced from 0.005\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     Dropout(\u001b[38;5;241m0.2\u001b[39m),  \u001b[38;5;66;03m# Reduced from 0.5\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 13\u001b[0m     Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', \n",
    "          kernel_regularizer=regularizers.l2(0.001),  # Reduced from 0.01\n",
    "          input_shape=(X_train_sel.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  # Reduced from 0.6\n",
    "    \n",
    "    Dense(256, activation='relu',\n",
    "          kernel_regularizer=regularizers.l2(0.0005)),  # Reduced from 0.005\n",
    "    Dropout(0.2),  # Reduced from 0.5\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3108f913-f9aa-4338-b719-44819e19ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2452, 535), Labels: (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SAFE loading from now on\n",
    "X = np.load(\"X_advanced_features.npy\")\n",
    "y = np.load(\"y_emotion_labels_encoded.npy\")  # No allow_pickle needed!\n",
    "\n",
    "print(f\"Features: {X.shape}, Labels: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a39ccd-6e2c-40a3-aad8-c9708e6d0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization, \n",
    "                                    MaxPooling1D, GlobalAveragePooling1D,\n",
    "                                    Dense, Dropout, Attention, Multiply)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint, TensorBoard)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f53c59c8-6778-42a9-a382-0945ce004b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TARGET_SR = 22050  # Sample rate\n",
    "DURATION = 3.0     # Seconds\n",
    "TARGET_LENGTH = 128  # Time steps\n",
    "N_MFCC = 40        # Number of MFCC coefficients\n",
    "N_FFT = 2048       # FFT window size\n",
    "HOP_LENGTH = 512   # Hop length for STFT\n",
    "\n",
    "# Paths (modify these)\n",
    "SONG_PATH = \"audio_song_actors_01-24\"\n",
    "SPEECH_PATH = \"audio_speech_actors_01-24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f71a464e-1a03-4b6b-b734-7beccf54add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion_features(file_path, target_length=TARGET_LENGTH):\n",
    "    \"\"\"\n",
    "    Extract time-preserved audio features optimized for emotion recognition\n",
    "    Returns features shaped [n_features, time_steps]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio with consistent duration\n",
    "        y, sr = librosa.load(file_path, sr=TARGET_SR, duration=DURATION)\n",
    "        \n",
    "        # Pad if shorter than duration\n",
    "        if len(y) < int(DURATION * TARGET_SR):\n",
    "            y = np.pad(y, (0, max(0, int(DURATION * TARGET_SR) - len(y))), mode='constant')\n",
    "        \n",
    "        # 1. MFCCs with deltas (time-preserved)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, \n",
    "                                   n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        \n",
    "        # 2. Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, \n",
    "                                                            n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, \n",
    "                                                              n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, \n",
    "                                                            n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        \n",
    "        # 3. Chroma and tonal features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        \n",
    "        # 4. Rhythm features\n",
    "        tempogram = librosa.feature.tempogram(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "        \n",
    "        # Stack all features vertically\n",
    "        features = np.vstack([\n",
    "            mfcc,\n",
    "            mfcc_delta,\n",
    "            mfcc_delta2,\n",
    "            spectral_centroid,\n",
    "            spectral_bandwidth,\n",
    "            spectral_contrast,\n",
    "            chroma,\n",
    "            tonnetz,\n",
    "            tempogram\n",
    "        ])\n",
    "        \n",
    "        # Ensure consistent time length\n",
    "        if features.shape[1] > target_length:\n",
    "            features = features[:, :target_length]\n",
    "        else:\n",
    "            pad_width = ((0, 0), (0, target_length - features.shape[1]))\n",
    "            features = np.pad(features, pad_width, mode='constant')\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        # Return zero array with same shape as successful extraction\n",
    "        return np.zeros((N_MFCC*3 + 7 + 6 + 384, target_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed10e50c-3148-44d4-92ff-5e0989a06659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def create_dataset(base_path):\n",
    "    \"\"\"Create DataFrame with file paths and emotions\"\"\"\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy',\n",
    "        '04': 'sad', '05': 'angry', '06': 'fearful',\n",
    "        '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    filepaths = []\n",
    "    emotions = []\n",
    "    \n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                try:\n",
    "                    emotion_code = file.split('-')[2]\n",
    "                    emotion = emotion_map.get(emotion_code, 'unknown')\n",
    "                    filepaths.append(os.path.join(root, file))\n",
    "                    emotions.append(emotion)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame({'filepath': filepaths, 'emotion': emotions})\n",
    "\n",
    "# Combine song and speech data\n",
    "song_df = create_dataset(SONG_PATH)\n",
    "speech_df = create_dataset(SPEECH_PATH)\n",
    "full_df = pd.concat([song_df, speech_df], ignore_index=True)\n",
    "\n",
    "# Remove unknown emotions\n",
    "full_df = full_df[full_df['emotion'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2ec0e6-b74a-4d50-804b-bfefe48447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio_features(features):\n",
    "    \"\"\"Apply random augmentations to feature array\"\"\"\n",
    "    # Time masking\n",
    "    if np.random.rand() > 0.5:\n",
    "        max_mask = int(TARGET_LENGTH * 0.2)  # Max 20% masking\n",
    "        mask_length = np.random.randint(1, max_mask)\n",
    "        mask_start = np.random.randint(0, TARGET_LENGTH - mask_length)\n",
    "        features[:, mask_start:mask_start+mask_length] = 0\n",
    "    \n",
    "    # Frequency masking\n",
    "    if np.random.rand() > 0.5:\n",
    "        max_mask = int(features.shape[0] * 0.1)  # Max 10% freq bands\n",
    "        mask_length = np.random.randint(1, max_mask)\n",
    "        mask_start = np.random.randint(0, features.shape[0] - mask_length)\n",
    "        features[mask_start:mask_start+mask_length, :] = 0\n",
    "    \n",
    "    # Random gain\n",
    "    features = features * np.random.uniform(0.9, 1.1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c642cada-b36c-49ee-b3bb-414edfbd1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emotion_model(input_shape, num_classes):\n",
    "    \"\"\"Build 1D CNN with attention for emotion classification\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Channel expansion\n",
    "    x = Conv1D(128, 5, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    # Attention block\n",
    "    attention = Conv1D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    attention = Multiply()([x, attention])\n",
    "    \n",
    "    # Feature processing\n",
    "    x = Conv1D(256, 3, padding='same')(attention)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv1D(512, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a9bb453-ee5d-4d21-a100-e0e084a2da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 2452\n",
      "File 0: audio_song_actors_01-24\\Actor_01\\03-02-01-01-01-01-01.wav - Exists: True\n",
      "File 1: audio_song_actors_01-24\\Actor_01\\03-02-01-01-01-02-01.wav - Exists: True\n",
      "File 2: audio_song_actors_01-24\\Actor_01\\03-02-01-01-02-01-01.wav - Exists: True\n",
      "File 3: audio_song_actors_01-24\\Actor_01\\03-02-01-01-02-02-01.wav - Exists: True\n",
      "File 4: audio_song_actors_01-24\\Actor_01\\03-02-02-01-01-01-01.wav - Exists: True\n"
     ]
    }
   ],
   "source": [
    "# After creating the full_df, check if files exist\n",
    "print(f\"Total files found: {len(full_df)}\")\n",
    "\n",
    "# Check if files are accessible\n",
    "for i, row in full_df[:5].iterrows():\n",
    "    print(f\"File {i}: {row['filepath']} - Exists: {os.path.exists(row['filepath'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789431b4-e6c1-4f76-aa96-0f56f34761a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking: audio_song_actors_01-24\n",
      "Found 1012 WAV files\n",
      "First 5 files:\n",
      " - 03-02-01-01-01-01-01.wav\n",
      " - 03-02-01-01-01-02-01.wav\n",
      " - 03-02-01-01-02-01-01.wav\n",
      " - 03-02-01-01-02-02-01.wav\n",
      " - 03-02-02-01-01-01-01.wav\n",
      "\n",
      "Checking: audio_speech_actors_01-24\n",
      "Found 1440 WAV files\n",
      "First 5 files:\n",
      " - 03-01-01-01-01-01-01.wav\n",
      " - 03-01-01-01-01-02-01.wav\n",
      " - 03-01-01-01-02-01-01.wav\n",
      " - 03-01-01-01-02-02-01.wav\n",
      " - 03-01-02-01-01-01-01.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Update these paths to match your actual directory structure\n",
    "SONG_PATH = \"audio_song_actors_01-24\"  # or full path like \"C:/your/path/audio_song_actors_01_2024\"\n",
    "SPEECH_PATH = \"audio_speech_actors_01-24\"\n",
    "\n",
    "# Check what files exist\n",
    "def check_files(directory):\n",
    "    print(f\"\\nChecking: {directory}\")\n",
    "    wav_files = glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "    print(f\"Found {len(wav_files)} WAV files\")\n",
    "    if wav_files:\n",
    "        print(\"First 5 files:\")\n",
    "        for f in wav_files[:5]:\n",
    "            print(f\" - {os.path.basename(f)}\")\n",
    "    return wav_files\n",
    "\n",
    "song_files = check_files(SONG_PATH)\n",
    "speech_files = check_files(SPEECH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402461c-6474-4e3b-bdce-aa22addffac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7aa665c-1240-43da-ae3c-bbc7d733b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2452/2452 [11:13<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 215ms/step - accuracy: 0.1362 - loss: 2.1215 - val_accuracy: 0.1853 - val_loss: 2.0331 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 237ms/step - accuracy: 0.1693 - loss: 2.0378 - val_accuracy: 0.1935 - val_loss: 2.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 255ms/step - accuracy: 0.1823 - loss: 1.9832 - val_accuracy: 0.1670 - val_loss: 2.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 288ms/step - accuracy: 0.2061 - loss: 1.9670 - val_accuracy: 0.1283 - val_loss: 2.1499 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 315ms/step - accuracy: 0.2304 - loss: 1.8964 - val_accuracy: 0.1955 - val_loss: 2.0046 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 298ms/step - accuracy: 0.2760 - loss: 1.8041 - val_accuracy: 0.1935 - val_loss: 2.5964 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 317ms/step - accuracy: 0.3060 - loss: 1.6849 - val_accuracy: 0.2077 - val_loss: 2.1122 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 281ms/step - accuracy: 0.3627 - loss: 1.5443 - val_accuracy: 0.2301 - val_loss: 2.0095 - learning_rate: 2.5000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 238ms/step - accuracy: 0.4228 - loss: 1.3945 - val_accuracy: 0.1690 - val_loss: 2.6823 - learning_rate: 2.5000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 269ms/step - accuracy: 0.4406 - loss: 1.3575 - val_accuracy: 0.1609 - val_loss: 2.7461 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.4403 - loss: 1.2872 - val_accuracy: 0.2729 - val_loss: 1.8994 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 335ms/step - accuracy: 0.4856 - loss: 1.2360 - val_accuracy: 0.2627 - val_loss: 2.0967 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 192ms/step - accuracy: 0.5096 - loss: 1.1539 - val_accuracy: 0.2546 - val_loss: 2.6448 - learning_rate: 2.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 196ms/step - accuracy: 0.5310 - loss: 1.1006 - val_accuracy: 0.2200 - val_loss: 2.4423 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 288ms/step - accuracy: 0.5540 - loss: 1.0253 - val_accuracy: 0.2974 - val_loss: 2.3584 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 257ms/step - accuracy: 0.5873 - loss: 0.9453 - val_accuracy: 0.2261 - val_loss: 3.2922 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 334ms/step - accuracy: 0.6200 - loss: 0.8549 - val_accuracy: 0.2200 - val_loss: 4.0898 - learning_rate: 1.2500e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 367ms/step - accuracy: 0.6405 - loss: 0.8029 - val_accuracy: 0.2566 - val_loss: 2.8272 - learning_rate: 1.2500e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 370ms/step - accuracy: 0.6580 - loss: 0.7787 - val_accuracy: 0.3544 - val_loss: 2.0128 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 303ms/step - accuracy: 0.6857 - loss: 0.6975 - val_accuracy: 0.2485 - val_loss: 3.1405 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 304ms/step - accuracy: 0.7111 - loss: 0.6731 - val_accuracy: 0.3585 - val_loss: 1.8840 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 315ms/step - accuracy: 0.7380 - loss: 0.6195 - val_accuracy: 0.3727 - val_loss: 2.1484 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 291ms/step - accuracy: 0.7255 - loss: 0.5955 - val_accuracy: 0.1161 - val_loss: 5.9058 - learning_rate: 1.2500e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 279ms/step - accuracy: 0.7294 - loss: 0.5818 - val_accuracy: 0.3096 - val_loss: 2.2318 - learning_rate: 1.2500e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 298ms/step - accuracy: 0.7768 - loss: 0.5079 - val_accuracy: 0.2688 - val_loss: 3.7805 - learning_rate: 1.2500e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 277ms/step - accuracy: 0.7819 - loss: 0.4827 - val_accuracy: 0.2363 - val_loss: 4.1701 - learning_rate: 1.2500e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 329ms/step - accuracy: 0.8202 - loss: 0.4472 - val_accuracy: 0.4114 - val_loss: 2.3553 - learning_rate: 6.2500e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 271ms/step - accuracy: 0.8445 - loss: 0.3907 - val_accuracy: 0.4684 - val_loss: 1.8387 - learning_rate: 6.2500e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 235ms/step - accuracy: 0.8453 - loss: 0.3912 - val_accuracy: 0.4664 - val_loss: 1.9264 - learning_rate: 6.2500e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 296ms/step - accuracy: 0.8613 - loss: 0.3599 - val_accuracy: 0.4969 - val_loss: 1.7750 - learning_rate: 6.2500e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 280ms/step - accuracy: 0.8719 - loss: 0.3379 - val_accuracy: 0.4705 - val_loss: 1.7522 - learning_rate: 6.2500e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 290ms/step - accuracy: 0.8801 - loss: 0.3182 - val_accuracy: 0.4623 - val_loss: 2.0057 - learning_rate: 6.2500e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 311ms/step - accuracy: 0.8792 - loss: 0.3130 - val_accuracy: 0.5010 - val_loss: 2.0779 - learning_rate: 6.2500e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 324ms/step - accuracy: 0.8729 - loss: 0.3157 - val_accuracy: 0.4725 - val_loss: 1.8737 - learning_rate: 6.2500e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 293ms/step - accuracy: 0.9016 - loss: 0.2763 - val_accuracy: 0.4216 - val_loss: 2.5669 - learning_rate: 6.2500e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 305ms/step - accuracy: 0.9003 - loss: 0.2748 - val_accuracy: 0.4786 - val_loss: 2.1235 - learning_rate: 6.2500e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9051 - loss: 0.2660 - val_accuracy: 0.4969 - val_loss: 1.8766 - learning_rate: 3.1250e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 257ms/step - accuracy: 0.9079 - loss: 0.2517 - val_accuracy: 0.5214 - val_loss: 1.7208 - learning_rate: 3.1250e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 252ms/step - accuracy: 0.9098 - loss: 0.2430 - val_accuracy: 0.5193 - val_loss: 1.7781 - learning_rate: 3.1250e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 325ms/step - accuracy: 0.9212 - loss: 0.2219 - val_accuracy: 0.5336 - val_loss: 1.8232 - learning_rate: 3.1250e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 273ms/step - accuracy: 0.9332 - loss: 0.2109 - val_accuracy: 0.4358 - val_loss: 2.3976 - learning_rate: 3.1250e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 258ms/step - accuracy: 0.9236 - loss: 0.2153 - val_accuracy: 0.4542 - val_loss: 2.0062 - learning_rate: 3.1250e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 305ms/step - accuracy: 0.9280 - loss: 0.2118 - val_accuracy: 0.5071 - val_loss: 1.9128 - learning_rate: 3.1250e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 289ms/step - accuracy: 0.9334 - loss: 0.2064 - val_accuracy: 0.5438 - val_loss: 1.7911 - learning_rate: 1.5625e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 293ms/step - accuracy: 0.9334 - loss: 0.1937 - val_accuracy: 0.5458 - val_loss: 1.7451 - learning_rate: 1.5625e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 265ms/step - accuracy: 0.9280 - loss: 0.2040 - val_accuracy: 0.5438 - val_loss: 1.7534 - learning_rate: 1.5625e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 298ms/step - accuracy: 0.9280 - loss: 0.2029 - val_accuracy: 0.5418 - val_loss: 1.7608 - learning_rate: 1.5625e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 277ms/step - accuracy: 0.9366 - loss: 0.1868 - val_accuracy: 0.5438 - val_loss: 1.8099 - learning_rate: 1.5625e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 265ms/step - accuracy: 0.9405 - loss: 0.1903 - val_accuracy: 0.5377 - val_loss: 1.7762 - learning_rate: 7.8125e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9443 - loss: 0.1808 - val_accuracy: 0.5540 - val_loss: 1.7760 - learning_rate: 7.8125e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 257ms/step - accuracy: 0.9428 - loss: 0.1812 - val_accuracy: 0.5580 - val_loss: 1.7785 - learning_rate: 7.8125e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 302ms/step - accuracy: 0.9384 - loss: 0.1859 - val_accuracy: 0.5458 - val_loss: 1.7745 - learning_rate: 7.8125e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 294ms/step - accuracy: 0.9414 - loss: 0.1819 - val_accuracy: 0.5458 - val_loss: 1.7561 - learning_rate: 7.8125e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 252ms/step - accuracy: 0.9433 - loss: 0.1717 - val_accuracy: 0.5458 - val_loss: 1.7703 - learning_rate: 3.9063e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 267ms/step - accuracy: 0.9482 - loss: 0.1793 - val_accuracy: 0.5458 - val_loss: 1.7851 - learning_rate: 3.9063e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 272ms/step - accuracy: 0.9487 - loss: 0.1751 - val_accuracy: 0.5499 - val_loss: 1.7870 - learning_rate: 3.9063e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 267ms/step - accuracy: 0.9370 - loss: 0.1813 - val_accuracy: 0.5540 - val_loss: 1.7819 - learning_rate: 3.9063e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 271ms/step - accuracy: 0.9472 - loss: 0.1719 - val_accuracy: 0.5560 - val_loss: 1.7845 - learning_rate: 3.9063e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 297ms/step - accuracy: 0.9461 - loss: 0.1736 - val_accuracy: 0.5499 - val_loss: 1.7728 - learning_rate: 1.9531e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 246ms/step - accuracy: 0.9419 - loss: 0.1764 - val_accuracy: 0.5540 - val_loss: 1.7708 - learning_rate: 1.9531e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 257ms/step - accuracy: 0.9419 - loss: 0.1660 - val_accuracy: 0.5499 - val_loss: 1.7705 - learning_rate: 1.9531e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 275ms/step - accuracy: 0.9484 - loss: 0.1685 - val_accuracy: 0.5560 - val_loss: 1.7730 - learning_rate: 1.9531e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 304ms/step - accuracy: 0.9522 - loss: 0.1706 - val_accuracy: 0.5560 - val_loss: 1.7813 - learning_rate: 1.9531e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 328ms/step - accuracy: 0.9473 - loss: 0.1710 - val_accuracy: 0.5580 - val_loss: 1.7752 - learning_rate: 1.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 288ms/step - accuracy: 0.9501 - loss: 0.1681 - val_accuracy: 0.5580 - val_loss: 1.7771 - learning_rate: 1.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 296ms/step - accuracy: 0.9531 - loss: 0.1632 - val_accuracy: 0.5560 - val_loss: 1.7717 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X = np.array([extract_emotion_features(fp) for fp in tqdm(full_df['filepath'])])\n",
    "\n",
    "y = full_df['emotion'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Apply augmentation to training set\n",
    "X_train_aug = np.array([augment_audio_features(x) for x in X_train])\n",
    "X_train = np.concatenate([X_train, X_train_aug])\n",
    "y_train = np.concatenate([y_train, y_train])\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Build model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_emotion_model(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "   ModelCheckpoint('best_emotion_model.keras', save_best_only=True),\n",
    "    TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fe189b2-fd6f-4e51-a8b9-a900619d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_features.npy\", X)\n",
    "np.save(\"y_labels.npy\", y_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6871e9-14a5-4322-8017-1f35cfd26623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2452, 531, 128)\n",
      "Label vector shape: (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Load your saved features and labels\n",
    "X = np.load(\"X_features.npy\")\n",
    "y_encoded = np.load(\"y_labels.npy\")\n",
    "\n",
    "# 2. Verify shapes\n",
    "print(\"Feature matrix shape:\", X.shape)  # Should be (n_samples, n_features, time_steps)\n",
    "print(\"Label vector shape:\", y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece393fa-23af-4a56-ae36-8ae5bcd89b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.8143687707641196, 1: 0.8143687707641196, 2: 1.602124183006536, 3: 0.8143687707641196, 4: 0.8143687707641196, 5: 1.6341666666666668, 6: 0.8143687707641196, 7: 1.602124183006536}\n"
     ]
    }
   ],
   "source": [
    "# 3. Train-test split with stratification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization, \n",
    "                                   MaxPooling1D, GlobalAveragePooling1D,\n",
    "                                   Dense, Dropout, Multiply)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint)\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    stratify=y_encoded,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae9982a1-430d-4dba-8c21-1b464f01c2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m82,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_9 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m98,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_10 (\u001b[38;5;33mReLU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m393,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_11 (\u001b[38;5;33mReLU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m2,056\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,304</span> (2.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,304\u001b[0m (2.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">709,512</span> (2.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m709,512\u001b[0m (2.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization, \n",
    "                                   MaxPooling1D, GlobalAveragePooling1D,\n",
    "                                   Dense, Dropout, Multiply)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint)\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def build_regularized_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Channel expansion with regularization\n",
    "    x = Conv1D(128, 5, padding='same', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature processing\n",
    "    x = Conv1D(256, 3, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Conv1D(512, 3, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    \n",
    "    # Classifier\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 6. Compile the model\n",
    "model = build_regularized_model(X_train.shape[1:], len(np.unique(y_encoded)))\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 7. Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.01\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "# 8. Convert labels to categorical\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87efa9df-8977-465b-97b5-42b296de5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 171ms/step - accuracy: 0.1300 - loss: 2.6906 - val_accuracy: 0.0794 - val_loss: 2.6554 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 168ms/step - accuracy: 0.1135 - loss: 2.6071 - val_accuracy: 0.1405 - val_loss: 2.5222 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.1531 - loss: 2.4984 - val_accuracy: 0.1711 - val_loss: 2.4390 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 244ms/step - accuracy: 0.1447 - loss: 2.4234 - val_accuracy: 0.1670 - val_loss: 2.3858 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 266ms/step - accuracy: 0.1677 - loss: 2.3813 - val_accuracy: 0.1813 - val_loss: 2.3201 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 259ms/step - accuracy: 0.1933 - loss: 2.2915 - val_accuracy: 0.1772 - val_loss: 2.2856 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 208ms/step - accuracy: 0.2026 - loss: 2.2758 - val_accuracy: 0.1894 - val_loss: 2.2501 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 0.1963 - loss: 2.2410 - val_accuracy: 0.2240 - val_loss: 2.2261 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 235ms/step - accuracy: 0.1889 - loss: 2.1959 - val_accuracy: 0.2138 - val_loss: 2.2019 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 339ms/step - accuracy: 0.2231 - loss: 2.1544 - val_accuracy: 0.2077 - val_loss: 2.1605 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 324ms/step - accuracy: 0.1942 - loss: 2.1429 - val_accuracy: 0.2179 - val_loss: 2.1716 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 295ms/step - accuracy: 0.2231 - loss: 2.1177 - val_accuracy: 0.2261 - val_loss: 2.1194 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 289ms/step - accuracy: 0.2179 - loss: 2.1086 - val_accuracy: 0.2057 - val_loss: 2.1258 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.2566 - loss: 2.0589 - val_accuracy: 0.2159 - val_loss: 2.0853 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 299ms/step - accuracy: 0.2398 - loss: 2.0727 - val_accuracy: 0.3014 - val_loss: 2.0462 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 286ms/step - accuracy: 0.2385 - loss: 2.0519 - val_accuracy: 0.2709 - val_loss: 2.0353 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 285ms/step - accuracy: 0.2522 - loss: 1.9985 - val_accuracy: 0.2281 - val_loss: 2.0301 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 244ms/step - accuracy: 0.2909 - loss: 1.9607 - val_accuracy: 0.2933 - val_loss: 2.0073 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 250ms/step - accuracy: 0.2959 - loss: 1.9282 - val_accuracy: 0.2831 - val_loss: 1.9921 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 269ms/step - accuracy: 0.2793 - loss: 1.9794 - val_accuracy: 0.2933 - val_loss: 1.9799 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 302ms/step - accuracy: 0.3172 - loss: 1.9427 - val_accuracy: 0.3422 - val_loss: 1.9140 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 294ms/step - accuracy: 0.3224 - loss: 1.8954 - val_accuracy: 0.2892 - val_loss: 1.9685 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 313ms/step - accuracy: 0.2962 - loss: 1.9003 - val_accuracy: 0.3625 - val_loss: 1.8870 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 286ms/step - accuracy: 0.3465 - loss: 1.8427 - val_accuracy: 0.3646 - val_loss: 1.8942 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 259ms/step - accuracy: 0.3339 - loss: 1.8577 - val_accuracy: 0.3707 - val_loss: 1.8871 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 330ms/step - accuracy: 0.3474 - loss: 1.8288 - val_accuracy: 0.3727 - val_loss: 1.8938 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 288ms/step - accuracy: 0.3284 - loss: 1.8292 - val_accuracy: 0.3870 - val_loss: 1.8673 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 295ms/step - accuracy: 0.3650 - loss: 1.8250 - val_accuracy: 0.3646 - val_loss: 1.8799 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 312ms/step - accuracy: 0.3406 - loss: 1.8284 - val_accuracy: 0.3829 - val_loss: 1.8716 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 307ms/step - accuracy: 0.3559 - loss: 1.8215 - val_accuracy: 0.3259 - val_loss: 1.8662 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 314ms/step - accuracy: 0.3638 - loss: 1.8368 - val_accuracy: 0.3340 - val_loss: 1.8700 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 340ms/step - accuracy: 0.3539 - loss: 1.8286 - val_accuracy: 0.4012 - val_loss: 1.8203 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 303ms/step - accuracy: 0.3554 - loss: 1.8013 - val_accuracy: 0.4033 - val_loss: 1.8265 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 319ms/step - accuracy: 0.3614 - loss: 1.7959 - val_accuracy: 0.3483 - val_loss: 1.8527 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 319ms/step - accuracy: 0.3799 - loss: 1.7675 - val_accuracy: 0.4155 - val_loss: 1.8176 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 322ms/step - accuracy: 0.3668 - loss: 1.7872 - val_accuracy: 0.3768 - val_loss: 1.8243 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 314ms/step - accuracy: 0.3745 - loss: 1.7551 - val_accuracy: 0.3462 - val_loss: 1.8443 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 284ms/step - accuracy: 0.3859 - loss: 1.7546 - val_accuracy: 0.3442 - val_loss: 1.8555 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 314ms/step - accuracy: 0.3869 - loss: 1.7535 - val_accuracy: 0.4073 - val_loss: 1.7912 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 297ms/step - accuracy: 0.3891 - loss: 1.7399 - val_accuracy: 0.3910 - val_loss: 1.8347 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 328ms/step - accuracy: 0.3846 - loss: 1.7367 - val_accuracy: 0.4134 - val_loss: 1.7924 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 330ms/step - accuracy: 0.3884 - loss: 1.7455 - val_accuracy: 0.4155 - val_loss: 1.7815 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 305ms/step - accuracy: 0.4072 - loss: 1.7328 - val_accuracy: 0.3809 - val_loss: 1.8020 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 316ms/step - accuracy: 0.3787 - loss: 1.7422 - val_accuracy: 0.3523 - val_loss: 1.8543 - learning_rate: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 301ms/step - accuracy: 0.3808 - loss: 1.7329 - val_accuracy: 0.3707 - val_loss: 1.8158 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 330ms/step - accuracy: 0.3913 - loss: 1.7332 - val_accuracy: 0.4257 - val_loss: 1.7824 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 317ms/step - accuracy: 0.3974 - loss: 1.7609 - val_accuracy: 0.3890 - val_loss: 1.8457 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 315ms/step - accuracy: 0.4107 - loss: 1.6943 - val_accuracy: 0.4257 - val_loss: 1.7689 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 284ms/step - accuracy: 0.4426 - loss: 1.6420 - val_accuracy: 0.4033 - val_loss: 1.7797 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 310ms/step - accuracy: 0.4098 - loss: 1.6600 - val_accuracy: 0.3992 - val_loss: 1.7949 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 300ms/step - accuracy: 0.4293 - loss: 1.6281 - val_accuracy: 0.4216 - val_loss: 1.7738 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 294ms/step - accuracy: 0.4275 - loss: 1.6180 - val_accuracy: 0.3890 - val_loss: 1.7860 - learning_rate: 2.5000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 294ms/step - accuracy: 0.4489 - loss: 1.6388 - val_accuracy: 0.4358 - val_loss: 1.7591 - learning_rate: 2.5000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.4488 - loss: 1.5993 - val_accuracy: 0.3870 - val_loss: 1.8036 - learning_rate: 2.5000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 299ms/step - accuracy: 0.4504 - loss: 1.5698 - val_accuracy: 0.4277 - val_loss: 1.7578 - learning_rate: 2.5000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 303ms/step - accuracy: 0.4309 - loss: 1.6387 - val_accuracy: 0.4236 - val_loss: 1.7634 - learning_rate: 2.5000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.4637 - loss: 1.5838 - val_accuracy: 0.3951 - val_loss: 1.7909 - learning_rate: 2.5000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 328ms/step - accuracy: 0.4675 - loss: 1.5937 - val_accuracy: 0.4440 - val_loss: 1.7610 - learning_rate: 2.5000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 173ms/step - accuracy: 0.4563 - loss: 1.5910 - val_accuracy: 0.4236 - val_loss: 1.7611 - learning_rate: 2.5000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 281ms/step - accuracy: 0.4550 - loss: 1.5884 - val_accuracy: 0.3727 - val_loss: 1.8856 - learning_rate: 2.5000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.4690 - loss: 1.5506 - val_accuracy: 0.4440 - val_loss: 1.7364 - learning_rate: 1.2500e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 303ms/step - accuracy: 0.4931 - loss: 1.5334 - val_accuracy: 0.4094 - val_loss: 1.7830 - learning_rate: 1.2500e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.4951 - loss: 1.5082 - val_accuracy: 0.4175 - val_loss: 1.7794 - learning_rate: 1.2500e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.4738 - loss: 1.5152 - val_accuracy: 0.4257 - val_loss: 1.7377 - learning_rate: 1.2500e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.4908 - loss: 1.5500 - val_accuracy: 0.4440 - val_loss: 1.7422 - learning_rate: 1.2500e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 255ms/step - accuracy: 0.4964 - loss: 1.4965 - val_accuracy: 0.4175 - val_loss: 1.7392 - learning_rate: 1.2500e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 259ms/step - accuracy: 0.4933 - loss: 1.4741 - val_accuracy: 0.4236 - val_loss: 1.7513 - learning_rate: 6.2500e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.5296 - loss: 1.4810 - val_accuracy: 0.4257 - val_loss: 1.7440 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "# 9. Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0533518b-d1e6-4884-8985-68d60daf1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Stem with larger filters\n",
    "    x = Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    # Parallel branches\n",
    "    branch1 = Conv1D(128, 3, padding='same', activation='relu')(x)\n",
    "    branch2 = Conv1D(128, 5, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.concatenate([branch1, branch2])\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    # Feature refinement\n",
    "    x = Conv1D(256, 3, dilation_rate=2, activation='relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Classifier\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = build_enhanced_model(X_train.shape[1:], len(np.unique(y_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca225bf-6c2c-497a-8831-990d11d1214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">57,408</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">531</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m57,408\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m531\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m24,704\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m41,088\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m265\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │      \u001b[38;5;34m1,032\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">354,248</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m354,248\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">354,120</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m354,120\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# 6. Compile the model\n",
    "# model = build_regularized_model(X_train.shape[1:], len(np.unique(y_encoded)))\n",
    "# Custom cyclical learning rate\n",
    "def lr_schedule(epoch):\n",
    "    base_lr = 0.001\n",
    "    max_lr = 0.01\n",
    "    step_size = 8\n",
    "    cycle = np.floor(1 + epoch/(2*step_size))\n",
    "    x = np.abs(epoch/step_size - 2*cycle + 1)\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "    return lr\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # Changed for integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# 7. Define callbacks\n",
    "callbacks = [\n",
    "    lr_scheduler,\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.01\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "# 8. Convert labels to categorical\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7439e1f-0e03-4626-8665-a84e3361c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.1144 - loss: 2.0891 - val_accuracy: 0.1446 - val_loss: 2.0980 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.002125.\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - accuracy: 0.1522 - loss: 2.0530 - val_accuracy: 0.2098 - val_loss: 2.0312 - learning_rate: 0.0021\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.1825 - loss: 2.0130 - val_accuracy: 0.2587 - val_loss: 1.9619 - learning_rate: 0.0033\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 150ms/step - accuracy: 0.2613 - loss: 1.9071 - val_accuracy: 0.2566 - val_loss: 1.9301 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 152ms/step - accuracy: 0.2838 - loss: 1.7825 - val_accuracy: 0.3177 - val_loss: 1.7305 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 166ms/step - accuracy: 0.3182 - loss: 1.7498 - val_accuracy: 0.2790 - val_loss: 1.7846 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 165ms/step - accuracy: 0.2982 - loss: 1.7356 - val_accuracy: 0.2831 - val_loss: 1.7596 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.3053 - loss: 1.7542 - val_accuracy: 0.3564 - val_loss: 1.7122 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 181ms/step - accuracy: 0.3458 - loss: 1.7022 - val_accuracy: 0.2872 - val_loss: 1.6794 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 165ms/step - accuracy: 0.3364 - loss: 1.6256 - val_accuracy: 0.3707 - val_loss: 1.6328 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 197ms/step - accuracy: 0.3718 - loss: 1.5441 - val_accuracy: 0.3218 - val_loss: 1.7011 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 187ms/step - accuracy: 0.4077 - loss: 1.4586 - val_accuracy: 0.3849 - val_loss: 1.5593 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 13/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 185ms/step - accuracy: 0.4239 - loss: 1.3780 - val_accuracy: 0.4012 - val_loss: 1.6911 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 14/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.4374 - loss: 1.3500 - val_accuracy: 0.3564 - val_loss: 1.6663 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 15/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 161ms/step - accuracy: 0.4960 - loss: 1.1843 - val_accuracy: 0.4684 - val_loss: 1.4201 - learning_rate: 0.0033\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.002125.\n",
      "Epoch 16/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 188ms/step - accuracy: 0.5659 - loss: 1.0477 - val_accuracy: 0.4725 - val_loss: 1.4064 - learning_rate: 0.0021\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.5920 - loss: 0.9827 - val_accuracy: 0.4888 - val_loss: 1.3738 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.002125.\n",
      "Epoch 18/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 242ms/step - accuracy: 0.5913 - loss: 0.9417 - val_accuracy: 0.5112 - val_loss: 1.3591 - learning_rate: 0.0021\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 19/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 209ms/step - accuracy: 0.6177 - loss: 0.9318 - val_accuracy: 0.5316 - val_loss: 1.3102 - learning_rate: 0.0033\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 20/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 187ms/step - accuracy: 0.6196 - loss: 0.8864 - val_accuracy: 0.5682 - val_loss: 1.2440 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 21/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.6654 - loss: 0.8118 - val_accuracy: 0.5397 - val_loss: 1.2235 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 22/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 153ms/step - accuracy: 0.6451 - loss: 0.8627 - val_accuracy: 0.5988 - val_loss: 1.2021 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 23/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 178ms/step - accuracy: 0.6708 - loss: 0.8317 - val_accuracy: 0.4705 - val_loss: 1.5952 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 24/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 151ms/step - accuracy: 0.6508 - loss: 0.8583 - val_accuracy: 0.5601 - val_loss: 1.3391 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 25/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 163ms/step - accuracy: 0.6251 - loss: 0.9037 - val_accuracy: 0.5642 - val_loss: 1.2822 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 26/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 177ms/step - accuracy: 0.6645 - loss: 0.7831 - val_accuracy: 0.5682 - val_loss: 1.2580 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 27/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 147ms/step - accuracy: 0.7652 - loss: 0.5819 - val_accuracy: 0.5642 - val_loss: 1.4383 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 28/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 178ms/step - accuracy: 0.7806 - loss: 0.5179 - val_accuracy: 0.6375 - val_loss: 1.1894 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 29/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.8304 - loss: 0.3783 - val_accuracy: 0.6191 - val_loss: 1.2693 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 30/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 178ms/step - accuracy: 0.8568 - loss: 0.3179 - val_accuracy: 0.6253 - val_loss: 1.2761 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 31/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.8997 - loss: 0.2302 - val_accuracy: 0.6436 - val_loss: 1.2543 - learning_rate: 0.0033\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.002125.\n",
      "Epoch 32/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 154ms/step - accuracy: 0.9238 - loss: 0.1693 - val_accuracy: 0.6660 - val_loss: 1.3018 - learning_rate: 0.0021\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 33/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 174ms/step - accuracy: 0.9349 - loss: 0.1607 - val_accuracy: 0.6619 - val_loss: 1.3361 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.002125.\n",
      "Epoch 34/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 170ms/step - accuracy: 0.9614 - loss: 0.1165 - val_accuracy: 0.6477 - val_loss: 1.4152 - learning_rate: 0.0021\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 35/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9498 - loss: 0.1304 - val_accuracy: 0.6375 - val_loss: 1.5541 - learning_rate: 0.0033\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 36/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - accuracy: 0.9354 - loss: 0.1472 - val_accuracy: 0.6232 - val_loss: 1.6359 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 37/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 176ms/step - accuracy: 0.9172 - loss: 0.1945 - val_accuracy: 0.6619 - val_loss: 1.4894 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 38/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 194ms/step - accuracy: 0.8544 - loss: 0.3362 - val_accuracy: 0.5743 - val_loss: 1.7166 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 39/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7920 - loss: 0.4880 - val_accuracy: 0.6640 - val_loss: 1.2852 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 40/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 153ms/step - accuracy: 0.9156 - loss: 0.2154 - val_accuracy: 0.6069 - val_loss: 1.5106 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 41/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step - accuracy: 0.8961 - loss: 0.2684 - val_accuracy: 0.5886 - val_loss: 1.6187 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.008875000000000001.\n",
      "Epoch 42/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 168ms/step - accuracy: 0.8215 - loss: 0.4355 - val_accuracy: 0.6538 - val_loss: 1.4182 - learning_rate: 0.0089\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.007750000000000001.\n",
      "Epoch 43/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.9260 - loss: 0.1860 - val_accuracy: 0.6314 - val_loss: 1.4097 - learning_rate: 0.0077\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.006625000000000001.\n",
      "Epoch 44/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 134ms/step - accuracy: 0.9351 - loss: 0.1360 - val_accuracy: 0.6558 - val_loss: 1.6149 - learning_rate: 0.0066\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0055000000000000005.\n",
      "Epoch 45/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 199ms/step - accuracy: 0.9839 - loss: 0.0498 - val_accuracy: 0.6578 - val_loss: 1.7972 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.004375.\n",
      "Epoch 46/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 192ms/step - accuracy: 0.9839 - loss: 0.0491 - val_accuracy: 0.6701 - val_loss: 1.7812 - learning_rate: 0.0044\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0032500000000000003.\n",
      "Epoch 47/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 189ms/step - accuracy: 0.9993 - loss: 0.0154 - val_accuracy: 0.6680 - val_loss: 1.8298 - learning_rate: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# 9. Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466bdc5a-55fb-4f2b-b0bd-ec7c52721194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac10306-42e7-481a-9f04-df5bae5849dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 46\n",
      "Best validation accuracy: 0.6701\n",
      "Corresponding training accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "best_val_acc = history.history['val_accuracy'][best_epoch]\n",
    "best_train_acc = history.history['accuracy'][best_epoch]\n",
    "\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Corresponding training accuracy: {best_train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef51adf-dbae-4c2a-96a9-cb8cdc878377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy with best model: 0.6701\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest accuracy with best model: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e560e303-9c1d-4684-9327-9dad1d27f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 128, 531)\n",
      "(491, 531, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d14770-30e6-4a43-b9e5-020ae33e95e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (491,531,128) (531,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     X_test_norm \u001b[38;5;241m=\u001b[39m (X_test \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m (stds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train_norm, X_test_norm\n\u001b[1;32m----> 9\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m normalize_data(X_train, X_test)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Verify label distribution\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y_train))\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m X_train_norm \u001b[38;5;241m=\u001b[39m (X_train \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m (stds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m X_test_norm \u001b[38;5;241m=\u001b[39m (X_test \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m (stds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_norm, X_test_norm\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (491,531,128) (531,) "
     ]
    }
   ],
   "source": [
    "# 1. Normalize features per channel\n",
    "def normalize_data(X_train, X_test):\n",
    "    means = np.mean(X_train, axis=(0, 1))\n",
    "    stds = np.std(X_train, axis=(0, 1))\n",
    "    X_train_norm = (X_train - means) / (stds + 1e-8)\n",
    "    X_test_norm = (X_test - means) / (stds + 1e-8)\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "X_train, X_test = normalize_data(X_train, X_test)\n",
    "\n",
    "# 2. Verify label distribution\n",
    "print(\"Label distribution:\", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4232fe-f05a-48d9-8ecb-b4b447aa5f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "X_train: (1961, 128, 531)\n",
      "X_test: (491, 531, 128)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (491,531,128) (1,1,531) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Apply normalization\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m normalize_data(X_train, X_test)\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X_train_norm \u001b[38;5;241m=\u001b[39m (X_train \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m (stds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m X_test_norm \u001b[38;5;241m=\u001b[39m (X_test \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m (stds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_norm, X_test_norm\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (491,531,128) (1,1,531) "
     ]
    }
   ],
   "source": [
    "def normalize_data(X_train, X_test):\n",
    "    # Calculate stats along features dimension (assuming channels_last format)\n",
    "    means = np.mean(X_train, axis=(0, 1))  # Shape: (features,)\n",
    "    stds = np.std(X_train, axis=(0, 1))    # Shape: (features,)\n",
    "    \n",
    "    # Reshape for broadcasting (1,1,features)\n",
    "    means = means.reshape((1, 1, -1))\n",
    "    stds = stds.reshape((1, 1, -1))\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_norm = (X_train - means) / (stds + 1e-8)\n",
    "    X_test_norm = (X_test - means) / (stds + 1e-8)\n",
    "    \n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "# Verify shapes first\n",
    "print(\"Original shapes:\")\n",
    "print(\"X_train:\", X_train.shape)  # Should be (samples, timesteps, features)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "# Apply normalization\n",
    "X_train, X_test = normalize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b037373c-d462-46c4-9608-ad4f52f6637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2452, 531, 128)\n",
      "Label vector shape: (2452,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Load your saved features and labels\n",
    "X = np.load(\"X_features.npy\")\n",
    "y_encoded = np.load(\"y_labels.npy\")\n",
    "\n",
    "# 2. Verify shapes\n",
    "print(\"Feature matrix shape:\", X.shape)   # Should be (n_samples, n_features, time_steps)\n",
    "print(\"Label vector shape:\", y_encoded.shape)  # Should be (n_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc9fd88d-a822-4ca5-a20a-ce404adcddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing audio_song_actors_01-24\\Actor_01\\03-02-01-01-01-01-01.wav: name 'librosa' is not defined\n",
      "Feature shape: (517, 128)\n"
     ]
    }
   ],
   "source": [
    "# Pick one audio file\n",
    "sample_path = full_df['filepath'].iloc[0]\n",
    "\n",
    "# Extract features using your current function\n",
    "features = extract_emotion_features(sample_path)\n",
    "\n",
    "# Print the shape\n",
    "print(\"Feature shape:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3690e805-4540-4aeb-b81c-ee4900ce30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.unique(y_encoded)\n",
    "print(\"Unique labels:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1c9e13b-592f-4350-a0a6-e98e892cbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean: 9.048914623725071\n",
      "Global std: 178.08872218406646\n"
     ]
    }
   ],
   "source": [
    "global_mean = np.mean(X)\n",
    "global_std = np.std(X)\n",
    "\n",
    "print(\"Global mean:\", global_mean)\n",
    "print(\"Global std:\", global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3accccf1-cd7c-4d2e-bff3-b50978a08233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling each feature independently...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(X):\n",
    "    \"\"\"Scale each feature across all samples and timesteps\"\"\"\n",
    "    print(\"Scaling each feature independently...\")\n",
    "    n_samples, n_features, n_timesteps = X.shape\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    \n",
    "    for feature_idx in range(n_features):\n",
    "        # Extract all timesteps for this feature across all samples\n",
    "        feature_data = X[:, feature_idx, :]  # shape: (2452, 128)\n",
    "        scaler = StandardScaler()\n",
    "        # Scale across samples and timesteps\n",
    "        X_scaled[:, feature_idx, :] = scaler.fit_transform(feature_data.reshape(-1, 1)).reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "X = scale_features(X)  # Use this instead of the commented version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c4362b9-10e7-436f-b0f5-62275a3d16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2452, 531, 128)\n",
      "(2452,)\n"
     ]
    }
   ],
   "source": [
    "y=y_encoded\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fc22927-0d57-4dea-a95c-8d6c6931bf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.unique(y)\n",
    "print(\"Unique labels:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "682f4583-ab25-4c09-aec9-42d1dd805cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing labels...\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.1407 - loss: 3.6140 - precision: 0.1733 - recall: 0.0798 - val_accuracy: 0.1711 - val_loss: 2.0459 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.1583 - loss: 2.0623 - precision: 0.2772 - recall: 9.8672e-04 - val_accuracy: 0.1976 - val_loss: 2.0214 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.1544 - loss: 2.0493 - precision: 0.3126 - recall: 0.0014 - val_accuracy: 0.1935 - val_loss: 2.0062 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step - accuracy: 0.1618 - loss: 2.0331 - precision: 0.4497 - recall: 0.0026 - val_accuracy: 0.1650 - val_loss: 2.0035 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.1696 - loss: 2.0311 - precision: 0.2311 - recall: 9.0983e-04 - val_accuracy: 0.1813 - val_loss: 1.9789 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.1464 - loss: 2.0286 - precision: 0.4762 - recall: 0.0014 - val_accuracy: 0.1894 - val_loss: 1.9620 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.1755 - loss: 2.0000 - precision: 0.4292 - recall: 0.0058 - val_accuracy: 0.2138 - val_loss: 1.9463 - val_precision: 1.0000 - val_recall: 0.0020\n",
      "Epoch 8/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.1846 - loss: 1.9668 - precision: 0.6056 - recall: 0.0119 - val_accuracy: 0.2057 - val_loss: 1.9223 - val_precision: 1.0000 - val_recall: 0.0081\n",
      "Epoch 9/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - accuracy: 0.1866 - loss: 1.9549 - precision: 0.4510 - recall: 0.0136 - val_accuracy: 0.2016 - val_loss: 1.9343 - val_precision: 0.8750 - val_recall: 0.0143\n",
      "Epoch 10/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 103ms/step - accuracy: 0.1756 - loss: 1.9479 - precision: 0.4066 - recall: 0.0154 - val_accuracy: 0.2077 - val_loss: 1.9215 - val_precision: 0.8889 - val_recall: 0.0163\n",
      "Epoch 11/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.1917 - loss: 1.8907 - precision: 0.5763 - recall: 0.0272 - val_accuracy: 0.2301 - val_loss: 1.8879 - val_precision: 0.5625 - val_recall: 0.0367\n",
      "Epoch 12/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.2159 - loss: 1.8772 - precision: 0.5124 - recall: 0.0276 - val_accuracy: 0.2363 - val_loss: 1.8538 - val_precision: 0.9091 - val_recall: 0.0204\n",
      "Epoch 13/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.2265 - loss: 1.8458 - precision: 0.6524 - recall: 0.0427 - val_accuracy: 0.2342 - val_loss: 1.8489 - val_precision: 0.5952 - val_recall: 0.0509\n",
      "Epoch 14/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.2150 - loss: 1.8568 - precision: 0.5760 - recall: 0.0529 - val_accuracy: 0.2648 - val_loss: 1.8378 - val_precision: 0.6296 - val_recall: 0.0346\n",
      "Epoch 15/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 125ms/step - accuracy: 0.2308 - loss: 1.8020 - precision: 0.5508 - recall: 0.0450 - val_accuracy: 0.2587 - val_loss: 1.8213 - val_precision: 0.8571 - val_recall: 0.0244\n",
      "Epoch 16/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 117ms/step - accuracy: 0.2560 - loss: 1.8098 - precision: 0.6600 - recall: 0.0591 - val_accuracy: 0.3340 - val_loss: 1.7337 - val_precision: 0.8400 - val_recall: 0.0428\n",
      "Epoch 17/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.2393 - loss: 1.7556 - precision: 0.6572 - recall: 0.0771 - val_accuracy: 0.3157 - val_loss: 1.7529 - val_precision: 0.7111 - val_recall: 0.0652\n",
      "Epoch 18/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 123ms/step - accuracy: 0.2589 - loss: 1.7614 - precision: 0.6663 - recall: 0.0649 - val_accuracy: 0.3686 - val_loss: 1.7349 - val_precision: 0.7800 - val_recall: 0.0794\n",
      "Epoch 19/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 150ms/step - accuracy: 0.2556 - loss: 1.7169 - precision: 0.6534 - recall: 0.0765 - val_accuracy: 0.3992 - val_loss: 1.6784 - val_precision: 0.6508 - val_recall: 0.0835\n",
      "Epoch 20/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.2820 - loss: 1.6706 - precision: 0.6341 - recall: 0.0846 - val_accuracy: 0.3442 - val_loss: 1.7054 - val_precision: 0.7308 - val_recall: 0.0774\n",
      "Epoch 21/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 141ms/step - accuracy: 0.2808 - loss: 1.6646 - precision: 0.6584 - recall: 0.0860 - val_accuracy: 0.3299 - val_loss: 1.7006 - val_precision: 0.7463 - val_recall: 0.1018\n",
      "Epoch 22/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 115ms/step - accuracy: 0.2836 - loss: 1.6660 - precision: 0.6192 - recall: 0.0790 - val_accuracy: 0.3523 - val_loss: 1.6759 - val_precision: 0.7368 - val_recall: 0.0855\n",
      "Epoch 23/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 135ms/step - accuracy: 0.3002 - loss: 1.6053 - precision: 0.7277 - recall: 0.0909 - val_accuracy: 0.3747 - val_loss: 1.6479 - val_precision: 0.6250 - val_recall: 0.0916\n",
      "Epoch 24/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 112ms/step - accuracy: 0.3057 - loss: 1.6237 - precision: 0.6275 - recall: 0.0761 - val_accuracy: 0.3360 - val_loss: 1.6731 - val_precision: 0.8571 - val_recall: 0.0733\n",
      "Epoch 25/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 150ms/step - accuracy: 0.3118 - loss: 1.5964 - precision: 0.6341 - recall: 0.0868 - val_accuracy: 0.3483 - val_loss: 1.6498 - val_precision: 0.8367 - val_recall: 0.0835\n",
      "Epoch 26/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 134ms/step - accuracy: 0.3043 - loss: 1.5641 - precision: 0.6884 - recall: 0.1012 - val_accuracy: 0.4033 - val_loss: 1.5920 - val_precision: 0.8372 - val_recall: 0.0733\n",
      "Epoch 27/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.3330 - loss: 1.5393 - precision: 0.6977 - recall: 0.1164 - val_accuracy: 0.3666 - val_loss: 1.5939 - val_precision: 0.8444 - val_recall: 0.0774\n",
      "Epoch 28/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 164ms/step - accuracy: 0.3336 - loss: 1.4929 - precision: 0.7391 - recall: 0.1076 - val_accuracy: 0.3646 - val_loss: 1.5852 - val_precision: 0.8704 - val_recall: 0.0957\n",
      "Epoch 29/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 150ms/step - accuracy: 0.3483 - loss: 1.4998 - precision: 0.6390 - recall: 0.1101 - val_accuracy: 0.3870 - val_loss: 1.5756 - val_precision: 0.8519 - val_recall: 0.0937\n",
      "Epoch 30/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 147ms/step - accuracy: 0.3354 - loss: 1.5311 - precision: 0.7737 - recall: 0.1102 - val_accuracy: 0.4134 - val_loss: 1.5372 - val_precision: 0.7759 - val_recall: 0.0916\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"Processing labels...\")\n",
    "y = to_categorical(y, num_classes=8)  # Convert to one-hot encoding\n",
    "\n",
    "# 4. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 5. Optional: Class Weighting (for imbalanced data)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# 6. Model Definition\n",
    "def create_multiclass_cnn(input_shape=(531, 128), num_classes=8):\n",
    "    print(\"Building model...\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Conv Block 1\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Conv Block 2\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Conv Block 3\n",
    "    model.add(Conv1D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Dense Layers\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 7. Model Compilation\n",
    "model = create_multiclass_cnn(input_shape=(531, 128))\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# 8. Model Training\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weights,  # Optional: remove if not needed\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a668d840-fff1-409c-b7de-a6f67a223858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5372\n",
      "Test Accuracy: 41.34%\n",
      "Test Precision: 0.7759\n",
      "Test Recall: 0.0916\n",
      "Model saved as 'multiclass_cnn_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# 9. Model Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]*100:.2f}%\")\n",
    "print(f\"Test Precision: {results[2]:.4f}\")\n",
    "print(f\"Test Recall: {results[3]:.4f}\")\n",
    "\n",
    "# 10. Save Model\n",
    "model.save('multiclass_cnn_model.h5')\n",
    "print(\"Model saved as 'multiclass_cnn_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d713733-fffc-428a-b35e-ec5fb915850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (2452,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "981a6d2f-eb4c-4b52-9a5b-a2e91c7e904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels:\n",
      " [5 5 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 labels:\\n\", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84b0e453-9aef-4127-b35a-e8105fa29103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (2452,)\n",
      "y sample: [5 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"y shape:\", y.shape)\n",
    "print(\"y sample:\", y[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e78dc1e6-fa25-48a6-98d3-b7b0bc003da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 393ms/step - accuracy: 0.1317 - loss: 15.8805 - val_accuracy: 0.0998 - val_loss: 7.3267\n",
      "Epoch 2/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 423ms/step - accuracy: 0.1300 - loss: 12.8474 - val_accuracy: 0.1568 - val_loss: 7.1656\n",
      "Epoch 3/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 455ms/step - accuracy: 0.1421 - loss: 11.1837 - val_accuracy: 0.1731 - val_loss: 7.0954\n",
      "Epoch 4/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 460ms/step - accuracy: 0.1448 - loss: 9.4982 - val_accuracy: 0.1853 - val_loss: 6.9607\n",
      "Epoch 5/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 480ms/step - accuracy: 0.1514 - loss: 7.8772 - val_accuracy: 0.2057 - val_loss: 6.8078\n",
      "Epoch 6/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 473ms/step - accuracy: 0.1566 - loss: 7.1253 - val_accuracy: 0.1548 - val_loss: 6.6798\n",
      "Epoch 7/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 460ms/step - accuracy: 0.1702 - loss: 6.7443 - val_accuracy: 0.1690 - val_loss: 6.5389\n",
      "Epoch 8/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.1404 - loss: 6.5540 - val_accuracy: 0.1711 - val_loss: 6.3958\n",
      "Epoch 9/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 485ms/step - accuracy: 0.1555 - loss: 6.3674 - val_accuracy: 0.1670 - val_loss: 6.2864\n",
      "Epoch 10/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 478ms/step - accuracy: 0.1517 - loss: 6.2626 - val_accuracy: 0.1568 - val_loss: 6.1963\n",
      "Epoch 11/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 478ms/step - accuracy: 0.1673 - loss: 6.1880 - val_accuracy: 0.1731 - val_loss: 6.1284\n",
      "Epoch 12/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 495ms/step - accuracy: 0.1424 - loss: 6.1235 - val_accuracy: 0.1752 - val_loss: 6.0859\n",
      "Epoch 13/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 483ms/step - accuracy: 0.1579 - loss: 6.0685 - val_accuracy: 0.1752 - val_loss: 6.0491\n",
      "Epoch 14/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 494ms/step - accuracy: 0.1725 - loss: 5.9934 - val_accuracy: 0.1853 - val_loss: 6.0158\n",
      "Epoch 15/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 490ms/step - accuracy: 0.1589 - loss: 6.0087 - val_accuracy: 0.1752 - val_loss: 6.0033\n",
      "Epoch 16/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 491ms/step - accuracy: 0.1771 - loss: 6.0012 - val_accuracy: 0.1752 - val_loss: 5.9980\n",
      "Epoch 17/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 496ms/step - accuracy: 0.1726 - loss: 5.9895 - val_accuracy: 0.1731 - val_loss: 5.9875\n",
      "Epoch 18/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.1854 - loss: 5.9504 - val_accuracy: 0.1731 - val_loss: 5.9705\n",
      "Epoch 19/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 480ms/step - accuracy: 0.1680 - loss: 5.9488 - val_accuracy: 0.1711 - val_loss: 5.9371\n",
      "Epoch 20/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 492ms/step - accuracy: 0.1631 - loss: 5.8994 - val_accuracy: 0.1996 - val_loss: 5.8914\n",
      "Epoch 21/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 511ms/step - accuracy: 0.1757 - loss: 5.8551 - val_accuracy: 0.1772 - val_loss: 5.8518\n",
      "Epoch 22/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 508ms/step - accuracy: 0.1809 - loss: 5.8260 - val_accuracy: 0.1731 - val_loss: 5.7869\n",
      "Epoch 23/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 442ms/step - accuracy: 0.1524 - loss: 5.7871 - val_accuracy: 0.1772 - val_loss: 5.7198\n",
      "Epoch 24/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 476ms/step - accuracy: 0.1616 - loss: 5.7390 - val_accuracy: 0.1650 - val_loss: 5.6582\n",
      "Epoch 25/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 491ms/step - accuracy: 0.1933 - loss: 5.6507 - val_accuracy: 0.1792 - val_loss: 5.6065\n",
      "Epoch 26/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 477ms/step - accuracy: 0.1936 - loss: 5.5674 - val_accuracy: 0.1670 - val_loss: 5.5620\n",
      "Epoch 27/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 483ms/step - accuracy: 0.1927 - loss: 5.5381 - val_accuracy: 0.1833 - val_loss: 5.5225\n",
      "Epoch 28/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.1938 - loss: 5.4713 - val_accuracy: 0.1874 - val_loss: 5.4922\n",
      "Epoch 29/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 519ms/step - accuracy: 0.1601 - loss: 5.4612 - val_accuracy: 0.1874 - val_loss: 5.4633\n",
      "Epoch 30/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 478ms/step - accuracy: 0.1968 - loss: 5.4682 - val_accuracy: 0.1955 - val_loss: 5.4486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== STEP 1: CLR Callback =====\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=1e-5, max_lr=1e-3, step_size=2000., mode='triangular'):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        # ✅ Use assign instead of K.set_value\n",
    "        self.model.optimizer.learning_rate.assign(self.base_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        new_lr = self.clr()\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "\n",
    "# ===== STEP 2: Label prep (Assuming y already loaded and shape = (2452, 8)) =====\n",
    "# STEP 2: Prepare labels correctly\n",
    "y_labels = y  # shape = (2452,)\n",
    "y_onehot = to_categorical(y_labels, num_classes=8)  # for model training\n",
    "\n",
    "# STEP 3: Compute weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_labels)\n",
    "\n",
    "# STEP 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test, train_index, _ = train_test_split(\n",
    "    X, y_onehot, np.arange(len(y_onehot)), test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "\n",
    "# ===== STEP 5: Model Definition =====\n",
    "def create_improved_model(input_shape=(531, 128)):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(128, 5, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Conv1D(256, 5, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Conv1D(512, 3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        Dense(512, activation='relu', kernel_regularizer='l2'),\n",
    "        Dropout(0.6),\n",
    "        Dense(8, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ===== STEP 6: Compile Model =====\n",
    "model = create_improved_model()\n",
    "clr = CyclicLR(\n",
    "    base_lr=1e-6,\n",
    "    max_lr=1e-4,\n",
    "    step_size=8 * (len(X_train) // 32)\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,        # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with best value\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ===== STEP 7: Train the model =====\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[clr,early_stop],\n",
    "    sample_weight=sample_weights[train_index],  # aligned with train data\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3969458a-ba0f-481e-8a96-a60c0e9d17d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 5 y values:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print 5 random samples from y\n",
    "random_indices = np.random.choice(len(y), size=5, replace=False)\n",
    "print(\"Random 5 y values:\")\n",
    "for idx in random_indices:\n",
    "    print(y[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "674a82c6-f927-403c-aaec-0b24ade7decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (2452, 8)\n",
      "Unique labels in y: [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"y shape:\", y.shape)\n",
    "print(\"Unique labels in y:\", np.unique(np.argmax(y, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "481c002d-df58-44e3-8711-46388be5d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: (2452,)\n",
      "Unique class labels: (array([0], dtype=int64), array([2452], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# If your y was one-hot\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "print(\"Label shape:\", y_labels.shape)\n",
    "print(\"Unique class labels:\", np.unique(y_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1deb478-1fd2-4083-bb0e-a98fad7c0b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\user\n",
      "\n",
      "Folders in current directory:\n",
      "📁 .anaconda\n",
      "📁 .cisco\n",
      "📁 .conda\n",
      "📁 .continuum\n",
      "📁 .ipynb_checkpoints\n",
      "📁 .ipython\n",
      "📁 .jupyter\n",
      "📁 .keras\n",
      "📁 .matplotlib\n",
      "📁 .virtual_documents\n",
      "📁 .vscode\n",
      "📁 3D Objects\n",
      "📁 anaconda3\n",
      "📁 AppData\n",
      "📁 Application Data\n",
      "📁 Audio_Song_Actors_01-24\n",
      "📁 Audio_Speech_Actors_01-24\n",
      "📁 Contacts\n",
      "📁 Cookies\n",
      "📁 data\n",
      "📁 Desktop\n",
      "📁 Documents\n",
      "📁 Downloads\n",
      "📁 Favorites\n",
      "📁 Links\n",
      "📁 Local Settings\n",
      "📁 logs\n",
      "📁 Music\n",
      "📁 My Documents\n",
      "📁 NetHood\n",
      "📁 OneDrive\n",
      "📁 Pictures\n",
      "📁 PrintHood\n",
      "📁 Recent\n",
      "📁 Saved Games\n",
      "📁 Searches\n",
      "📁 SendTo\n",
      "📁 Start Menu\n",
      "📁 stl_files\n",
      "📁 Templates\n",
      "📁 Videos\n",
      "📁 WPS Cloud Files\n",
      "📁 __pycache__\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Show current directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# List folders inside your current directory\n",
    "print(\"\\nFolders in current directory:\")\n",
    "for item in os.listdir():\n",
    "    if os.path.isdir(item):\n",
    "        print(\"📁\", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfac7d33-1b3c-4d7e-a317-e22937f3e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech folder path: C:\\Users\\user\\ Audio_Speech_Actors_01-24\n",
      "Song folder path: C:\\Users\\user\\Audio_Song_Actors_01-24\n"
     ]
    }
   ],
   "source": [
    "speech_dir = os.path.abspath(\" Audio_Speech_Actors_01-24\")\n",
    "song_dir = os.path.abspath(\"Audio_Song_Actors_01-24\")\n",
    "\n",
    "print(\"Speech folder path:\", speech_dir)\n",
    "print(\"Song folder path:\", song_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bb0672-32fa-4b37-90b3-25d43af58c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                    Dense, Dropout, Input, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6887f9c-1498-4c98-aeb0-c38e8eaf4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUDIO_DURATION = 94  # ~3 seconds with hop_length=512\n",
    "MEL_BANDS = 32  \n",
    "FEATURE_DIMENSIONS = 2\n",
    "MODEL_ID = \"v6\"  \n",
    "MODEL_TAG = \"enhanced\"  \n",
    "RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0546947-5509-48c1-9cb8-ac521e12dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_array_dimensions(data_array, desired_shape):\n",
    "    \"\"\"Resize array to match target dimensions by padding or trimming\"\"\"\n",
    "    if data_array.size == 0:\n",
    "        return np.zeros(desired_shape)\n",
    "    \n",
    "    current_dims = data_array.shape\n",
    "    \n",
    "    # Process time axis\n",
    "    if len(current_dims) >= 2:\n",
    "        if current_dims[1] > desired_shape[1]:\n",
    "            data_array = data_array[:, :desired_shape[1]]\n",
    "        elif current_dims[1] < desired_shape[1]:\n",
    "            padding = [(0, 0), (0, desired_shape[1] - current_dims[1])]\n",
    "            data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    # Process frequency axis\n",
    "    if current_dims[0] > desired_shape[0]:\n",
    "        data_array = data_array[:desired_shape[0], :]\n",
    "    elif current_dims[0] < desired_shape[0]:\n",
    "        padding = [(0, desired_shape[0] - current_dims[0]), (0, 0)]\n",
    "        data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def standardize_features(feature_matrix):\n",
    "    \"\"\"Normalize feature values with robust handling\"\"\"\n",
    "    if feature_matrix.size == 0:\n",
    "        return feature_matrix\n",
    "    \n",
    "    mean_val = np.mean(feature_matrix)\n",
    "    std_val = np.std(feature_matrix)\n",
    "    \n",
    "    if std_val == 0 or np.isnan(std_val):\n",
    "        return feature_matrix - mean_val\n",
    "    \n",
    "    return (feature_matrix - mean_val) / std_val\n",
    "\n",
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Feature extraction pipeline with consistent output dimensions\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = 32\n",
    "        \n",
    "        try:\n",
    "            # First channel: Mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=mel_bands, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        try:\n",
    "            # Second channel: Combined audio features\n",
    "            mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, \n",
    "                                               n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma_features = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            channel_2 = np.vstack([mfcc_features, chroma_features, spectral_contrast])\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        feature_stack = np.stack([channel_1, channel_2], axis=-1)\n",
    "        \n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b5229d-86e1-4532-b84b-3ade02f4d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Data augmentation for underrepresented classes\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = np.where(label_set == class_id)[0]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            augmentation = np.random.choice(['add_noise', 'time_warp', 'frequency_mask', 'amplitude_scale'])\n",
    "            \n",
    "            if augmentation == 'add_noise':\n",
    "                modified_sample = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif augmentation == 'time_warp':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif augmentation == 'frequency_mask':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            else:\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = class_id\n",
    "            current_position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aad2a2f-cdd5-49fb-ba8f-b74508a1b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Neural network architecture for emotion classification\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bb4a55-87dd-4742-b1e5-0ef5cedbe58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86d7e6f-854f-43fc-8708-e72bba3cec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache'):\n",
    "    \"\"\"Feature processing pipeline with caching\"\"\"\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(base_features, base_labels)\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f7d409-e339-47bd-8bf4-83e136c4cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_audio_dataset_from_two_dirs(speech_dir, song_dir):\n",
    "    \"\"\"Create dataset from audio files in two folders (speech & song) with emotion labels\"\"\"\n",
    "    emotion_codes = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def process_dir(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_codes:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_codes[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Process both folders\n",
    "    process_dir(speech_dir)\n",
    "    process_dir(song_dir)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No valid emotion-labeled files found. Using placeholder labels.\")\n",
    "        all_audio_files = glob.glob(os.path.join(speech_dir, \"*.wav\")) + glob.glob(os.path.join(song_dir, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_audio_files))])\n",
    "        paths.extend(all_audio_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69a8835-613d-4024-8f0e-bc4b74faf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(model_file, audio_file, encoder_file):\n",
    "    \"\"\"Predict emotion from audio using trained model\"\"\"\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    with open(encoder_file, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    features = extract_audio_characteristics(audio_file)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_index]\n",
    "    \n",
    "    emotion_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return emotion_label, confidence\n",
    "\n",
    "def record_training_performance(training_history, model_identifier, train_accuracy, val_accuracy,\n",
    "                              overall_acc, f1_macro, f1_weighted,\n",
    "                              f2_macro, f2_weighted, avg_precision, avg_recall,\n",
    "                              per_class_acc, classification_report):\n",
    "    \"\"\"Save comprehensive training metrics\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_identifier}_performance_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Performance Report for {model_identifier}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model Version: {MODEL_ID}\\n\")\n",
    "        f.write(f\"Model Variant: {MODEL_TAG}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY METRICS:\\n\")\n",
    "        f.write(f\"Training Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F1: {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F2: {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F2: {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "        for class_name, accuracy in per_class_acc.items():\n",
    "            f.write(f\"{class_name}: {accuracy:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(classification_report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Performance report saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57614eae-1f6c-4aa5-be69-aab73e849498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rrfr\n"
     ]
    }
   ],
   "source": [
    "print(\"rrfr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3def77d-2e80-463c-8e67-9559ae17fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Enhanced feature extraction pipeline with delta features and improved spectral analysis\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, 4))  # Now 4 channels\n",
    "\n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, 4))\n",
    "\n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, 4))\n",
    "\n",
    "        # More robust audio trimming/padding\n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        audio_data = librosa.util.fix_length(audio_data, target_samples)\n",
    "\n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        \n",
    "        try:\n",
    "            # Channel 1: Enhanced Mel-spectrogram with delta features\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=MEL_BANDS, \n",
    "                n_fft=window_size, hop_length=frame_shift,\n",
    "                fmin=20, fmax=sr//2)  # Extended frequency range\n",
    "            mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Add temporal derivatives\n",
    "            delta_mel = librosa.feature.delta(mel_db)\n",
    "            delta2_mel = librosa.feature.delta(mel_db, order=2)\n",
    "            \n",
    "            channel_1 = np.stack([mel_db, delta_mel, delta2_mel], axis=-1)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION, 3))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION, 3))\n",
    "\n",
    "        try:\n",
    "            # Channel 2: Enhanced spectral features\n",
    "            mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=20,  # Increased from 13\n",
    "                                      n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, n_bands=6, hop_length=frame_shift)\n",
    "            tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sr)\n",
    "            \n",
    "            channel_2 = np.vstack([mfcc, chroma, contrast, tonnetz])\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "\n",
    "        # Combine channels (now 4 total)\n",
    "        feature_stack = np.concatenate([\n",
    "            channel_1,\n",
    "            channel_2[..., np.newaxis]  # Add new axis for concatenation\n",
    "        ], axis=-1)\n",
    "\n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "186d093b-24cb-4a39-8bfe-368d63963687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa.effects import pitch_shift, time_stretch\n",
    "# Optional: Add these to your config section\n",
    "MAX_PITCH_SHIFT = 1.5  # semitones\n",
    "TIME_STRETCH_RANGE = (0.85, 1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "595ea993-06c4-46fc-afce-05e3f3b63778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the top of your script\n",
    "FEATURE_DIMENSIONS = 4  # Was previously 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72ce0a1a-d7a8-43df-96f3-1649934958d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n",
    "                                      ModelCheckpoint, Callback)\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1788416b-3342-4f84-a224-8599f32a9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class DynamicLRScheduler(Callback):\n",
    "    def __init__(self, monitor='val_accuracy', patience=3, factor=0.5, min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.wait = 0\n",
    "        self.best = -np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        if current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                current_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                print(f\"\\nReducing learning rate to {new_lr:.2e}\")\n",
    "                self.wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6968b42c-4b74-4822-801e-c52e692ccd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Enhanced data augmentation that handles 4-channel features\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = feature_set[label_set == class_id]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            sample = class_samples[np.random.randint(0, len(class_samples))]\n",
    "            \n",
    "            # Modified augmentation to handle 4 channels\n",
    "            aug_type = np.random.choice(['time_warp', 'frequency_mask', 'noise_injection', 'amplitude_scale'])\n",
    "            \n",
    "            if aug_type == 'time_warp':\n",
    "                modified_sample = sample.copy()\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "                \n",
    "            elif aug_type == 'frequency_mask':\n",
    "                modified_sample = sample.copy()\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "                \n",
    "            elif aug_type == 'noise_injection':\n",
    "                noise = np.random.normal(0, 0.02, sample.shape)\n",
    "                modified_sample = sample + noise\n",
    "                \n",
    "            else:  # amplitude_scale\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features.append(modified_sample)\n",
    "            augmented_labels.append(class_id)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    augmented_features = np.array(augmented_features)\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(len(combined_labels))\n",
    "    return combined_features[shuffle_idx], combined_labels[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb905b98-99b3-45b9-9f6e-5cafbdee016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                   Dense, Dropout, Input, GlobalAveragePooling2D,\n",
    "                                   GlobalMaxPooling2D, Multiply, Add, Concatenate,\n",
    "                                   Activation)  # Added Activation here\n",
    "from tensorflow.keras.activations import swish  # For swish activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5d2c03a-7407-4cbf-aa68-1eb8ccc90454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Enhanced emotion classification network with proper residual connections\"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_dims)\n",
    "    x = inputs\n",
    "\n",
    "    # Block 1: Initial convolution\n",
    "    x = Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    residual = Conv2D(64, (1, 1), strides=(2, 2), padding='same')(inputs) if input_dims[-1] != 64 else x\n",
    "\n",
    "    # Block 2: Residual block\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Adjust residual connection if needed\n",
    "    if residual.shape[-1] != x.shape[-1]:\n",
    "        residual = Conv2D(x.shape[-1], (1, 1), padding='same')(residual)\n",
    "    x = Add()([x, residual])\n",
    "    x = Activation('swish')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Block 3: Final convolution\n",
    "    x = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Feature aggregation\n",
    "    gap = GlobalAveragePooling2D()(x)\n",
    "    gmp = GlobalMaxPooling2D()(x)\n",
    "    x = Concatenate()([gap, gmp])\n",
    "\n",
    "    # Classifier\n",
    "    x = Dense(256, activation='swish', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model._name = \"emotion_recognizer_v3\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f68571d-4c92-40b9-bd49-cbe1366ec1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3287a7ec-3aeb-4926-9583-1d90bd031268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution started\n",
      "\n",
      "==================================================\n",
      "Initializing pipeline: emotion_detector_v6_enhanced\n",
      "Model Version: v6 | Variant: enhanced\n",
      "Start Time: 2025-06-23 17:12:58\n",
      "==================================================\n",
      "\n",
      "Configuring GPU...\n",
      "⚠ No GPU devices found, using CPU\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Dataset Statistics:\n",
      "- Total samples: 2,452\n",
      "- Emotion distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "\n",
      "Data Split Successful:\n",
      "- Training samples: 1,961\n",
      "- Validation samples: 491\n",
      "\n",
      "Processing features...\n",
      "Processing training features:\n",
      "Loading cached features from: feature_cache\\bf4fb2261f9d192315d019a1d6d80368.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "Processing validation features:\n",
      "Loading cached features from: feature_cache\\909090482eb64c24550f3f1dfc2b6f82.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Feature Extraction Complete:\n",
      "- Training features shape: (2408, 32, 94, 4)\n",
      "- Validation features shape: (491, 32, 94, 4)\n",
      "\n",
      "Encoding labels...\n",
      "Classes detected: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "Building model...\n",
      "Model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_16… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │ conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_12       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_17… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_13       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_18… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m4\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m,    │      \u001b[38;5;34m1,184\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_16… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │ conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_12       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_17… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m,     │     \u001b[38;5;34m73,856\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m,     │        \u001b[38;5;34m512\u001b[0m │ conv2d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_13       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m11\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m11\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_18… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │      \u001b[38;5;34m2,056\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">199,784</span> (780.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m199,784\u001b[0m (780.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">199,208</span> (778.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m199,208\u001b[0m (778.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> (2.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576\u001b[0m (2.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling model...\n",
      "✔ Model successfully compiled\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "Epoch 1/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.1305 - loss: 3.6521\n",
      "Epoch 1: val_accuracy improved from -inf to 0.15275, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 105ms/step - accuracy: 0.1305 - loss: 3.6473 - val_accuracy: 0.1527 - val_loss: 2.0666 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.1393 - loss: 2.6183 \n",
      "Epoch 2: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 153ms/step - accuracy: 0.1391 - loss: 2.6124 - val_accuracy: 0.1527 - val_loss: 2.0679 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.1424 - loss: 2.2092\n",
      "Epoch 3: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 155ms/step - accuracy: 0.1424 - loss: 2.2088 - val_accuracy: 0.0794 - val_loss: 2.0750 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.1333 - loss: 2.0984 \n",
      "Epoch 4: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 151ms/step - accuracy: 0.1333 - loss: 2.0985 - val_accuracy: 0.1527 - val_loss: 2.0706 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1206 - loss: 2.1058 \n",
      "Epoch 5: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 151ms/step - accuracy: 0.1206 - loss: 2.1056 - val_accuracy: 0.1527 - val_loss: 2.0705 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.1363 - loss: 2.0845 \n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 161ms/step - accuracy: 0.1363 - loss: 2.0843 - val_accuracy: 0.1527 - val_loss: 2.0747 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.1243 - loss: 2.0737\n",
      "Epoch 7: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.1243 - loss: 2.0736 - val_accuracy: 0.1527 - val_loss: 2.0643 - learning_rate: 2.5000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.1311 - loss: 2.0606\n",
      "Epoch 8: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 152ms/step - accuracy: 0.1311 - loss: 2.0605 - val_accuracy: 0.1527 - val_loss: 2.0718 - learning_rate: 2.5000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.1359 - loss: 2.0506 \n",
      "Epoch 9: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 152ms/step - accuracy: 0.1359 - loss: 2.0507 - val_accuracy: 0.1527 - val_loss: 2.0697 - learning_rate: 2.5000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.1415 - loss: 2.0565\n",
      "Epoch 10: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.1414 - loss: 2.0565 - val_accuracy: 0.1527 - val_loss: 2.0667 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.1346 - loss: 2.0510\n",
      "Epoch 11: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 155ms/step - accuracy: 0.1345 - loss: 2.0511 - val_accuracy: 0.1527 - val_loss: 2.0752 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.1405 - loss: 2.0571 \n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.1405 - loss: 2.0571 - val_accuracy: 0.1527 - val_loss: 2.0734 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.1267 - loss: 2.0522 \n",
      "Epoch 13: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 155ms/step - accuracy: 0.1268 - loss: 2.0521 - val_accuracy: 0.1527 - val_loss: 2.0682 - learning_rate: 1.2500e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.1457 - loss: 2.0492 \n",
      "Epoch 14: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 151ms/step - accuracy: 0.1454 - loss: 2.0492 - val_accuracy: 0.1527 - val_loss: 2.0731 - learning_rate: 1.2500e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.1246 - loss: 2.0563\n",
      "Epoch 15: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 118ms/step - accuracy: 0.1249 - loss: 2.0561 - val_accuracy: 0.1527 - val_loss: 2.0686 - learning_rate: 1.2500e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.1385 - loss: 2.0472\n",
      "Epoch 16: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 154ms/step - accuracy: 0.1383 - loss: 2.0472 - val_accuracy: 0.1527 - val_loss: 2.0727 - learning_rate: 1.2500e-04\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "✔ Training completed successfully\n",
      "Script execution completed\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Enhanced execution pipeline with better debugging and output\"\"\"\n",
    "    # 1. Initialization with clear output\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Initializing pipeline: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID} | Variant: {MODEL_TAG}\")\n",
    "    print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # 2. Enhanced GPU configuration with verification\n",
    "    print(\"Configuring GPU...\")\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(f\"✔ GPU configured ({len(gpu_devices)} devices)\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"⚠ GPU configuration warning: {str(e)}\")\n",
    "    else:\n",
    "        print(\"⚠ No GPU devices found, using CPU\")\n",
    "\n",
    "    # 3. Dataset loading with validation\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Verify directories exist\n",
    "    for dir_path, dir_name in [(speech_dir, \"Speech\"), (song_dir, \"Song\")]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"❌ Error: {dir_name} directory not found at {dir_path}\")\n",
    "            return\n",
    "    \n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"❌ Dataset initialization failed\")\n",
    "        return\n",
    "    \n",
    "    # 4. Dataset statistics with better formatting\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"- Total samples: {len(audio_dataset):,}\")\n",
    "    print(\"- Emotion distribution:\")\n",
    "    print(audio_dataset['emotion'].value_counts().to_string())\n",
    "    \n",
    "    # 5. Data splitting with verification\n",
    "    try:\n",
    "        train_data, val_data = train_test_split(\n",
    "            audio_dataset, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=audio_dataset['emotion']\n",
    "        )\n",
    "        print(f\"\\nData Split Successful:\")\n",
    "        print(f\"- Training samples: {len(train_data):,}\")\n",
    "        print(f\"- Validation samples: {len(val_data):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data splitting failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 6. Feature processing with progress tracking\n",
    "    print(\"\\nProcessing features...\")\n",
    "    try:\n",
    "        print(\"Processing training features:\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        print(\"Processing validation features:\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"❌ Feature extraction failed - empty results\")\n",
    "            if len(X_train) == 0:\n",
    "                print(\"- Training features are empty\")\n",
    "            if len(X_val) == 0:\n",
    "                print(\"- Validation features are empty\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nFeature Extraction Complete:\")\n",
    "        print(f\"- Training features shape: {X_train.shape}\")\n",
    "        print(f\"- Validation features shape: {X_val.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Feature processing failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 7. Label processing\n",
    "    print(\"\\nEncoding labels...\")\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train_raw)\n",
    "    y_val = encoder.transform(y_val_raw)\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    y_val_categorical = to_categorical(y_val)\n",
    "    \n",
    "    print(\"Classes detected:\", list(encoder.classes_))\n",
    "\n",
    "    # 8. Model construction\n",
    "    print(\"\\nBuilding model...\")\n",
    "    try:\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        print(\"Model architecture:\")\n",
    "        emotion_model.summary()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model construction failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 9. Model compilation\n",
    "    print(\"\\nCompiling model...\")\n",
    "    try:\n",
    "        optimizer = Adam(\n",
    "            learning_rate=0.0005,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        emotion_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"✔ Model successfully compiled\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model compilation failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 10. Training phase\n",
    "    print(\"\\nStarting training...\")\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "            ModelCheckpoint(f'{RUN_NAME}_top.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "        ]\n",
    "        \n",
    "        history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=calculate_class_importance(y_train),\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"\\n✔ Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # [Rest of your evaluation code remains the same...]\n",
    "    # Make sure to add similar print statements throughout\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script execution started\")\n",
    "    execute_pipeline()\n",
    "    print(\"Script execution completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c63409a9-5fbe-4e25-9838-fb993d0b68b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput stats - Mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(X_train), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mstd(X_train))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(y_train_categorical, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Input stats - Mean:\", np.mean(X_train), \"Std:\", np.std(X_train))\n",
    "print(\"Label distribution:\", np.sum(y_train_categorical, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5aa6e9e7-1f3b-4a65-8cfb-e69ff64db3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution started\n",
      "\n",
      "==================================================\n",
      "Initializing pipeline: emotion_detector_v6_enhanced\n",
      "Model Version: v6 | Variant: enhanced\n",
      "Start Time: 2025-06-23 17:28:09\n",
      "==================================================\n",
      "\n",
      "Configuring GPU...\n",
      "⚠ No GPU devices found, using CPU\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Dataset Statistics:\n",
      "- Total samples: 2,452\n",
      "- Emotion distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "\n",
      "Data Split Successful:\n",
      "- Training samples: 1,961\n",
      "- Validation samples: 491\n",
      "\n",
      "Processing features...\n",
      "Processing training features:\n",
      "Loading cached features from: feature_cache\\bf4fb2261f9d192315d019a1d6d80368.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "Processing validation features:\n",
      "Loading cached features from: feature_cache\\909090482eb64c24550f3f1dfc2b6f82.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Feature Extraction Complete:\n",
      "- Training features shape: (2408, 32, 94, 4)\n",
      "- Validation features shape: (491, 32, 94, 4)\n",
      "\n",
      "Encoding labels...\n",
      "Classes detected: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "Building optimized model...\n",
      "Model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m1,184\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,976</span> (437.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m111,976\u001b[0m (437.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,528</span> (435.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m111,528\u001b[0m (435.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling model...\n",
      "✔ Model successfully compiled\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "Epoch 1/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.1233 - loss: 2.1928 - val_accuracy: 0.1527 - val_loss: 2.0764\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - accuracy: 0.1377 - loss: 2.0764 - val_accuracy: 0.1527 - val_loss: 2.0680\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.1320 - loss: 2.0693 - val_accuracy: 0.1527 - val_loss: 2.0670\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.1433 - loss: 2.0590 - val_accuracy: 0.1527 - val_loss: 2.0733\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.1410 - loss: 2.0451 - val_accuracy: 0.1527 - val_loss: 2.0818\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - accuracy: 0.1470 - loss: 2.0392 - val_accuracy: 0.1527 - val_loss: 2.1001\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - accuracy: 0.1357 - loss: 2.0411 - val_accuracy: 0.1527 - val_loss: 2.1254\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.1296 - loss: 2.0477 - val_accuracy: 0.1527 - val_loss: 2.1234\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - accuracy: 0.1375 - loss: 2.0349 - val_accuracy: 0.0794 - val_loss: 2.1659\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - accuracy: 0.1326 - loss: 2.0369 - val_accuracy: 0.0794 - val_loss: 2.1795\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 139ms/step - accuracy: 0.1349 - loss: 2.0354 - val_accuracy: 0.1527 - val_loss: 2.1888\n",
      "\n",
      "✔ Training completed successfully\n",
      "Script execution completed\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Enhanced execution pipeline with better debugging and output\"\"\"\n",
    "    # 1. Initialization with clear output\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Initializing pipeline: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID} | Variant: {MODEL_TAG}\")\n",
    "    print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # 2. Enhanced GPU configuration with verification\n",
    "    print(\"Configuring GPU...\")\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(f\"✔ GPU configured ({len(gpu_devices)} devices)\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"⚠ GPU configuration warning: {str(e)}\")\n",
    "    else:\n",
    "        print(\"⚠ No GPU devices found, using CPU\")\n",
    "\n",
    "    # 3. Dataset loading with validation\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Verify directories exist\n",
    "    for dir_path, dir_name in [(speech_dir, \"Speech\"), (song_dir, \"Song\")]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"❌ Error: {dir_name} directory not found at {dir_path}\")\n",
    "            return\n",
    "    \n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"❌ Dataset initialization failed\")\n",
    "        return\n",
    "    \n",
    "    # 4. Dataset statistics with better formatting\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"- Total samples: {len(audio_dataset):,}\")\n",
    "    print(\"- Emotion distribution:\")\n",
    "    print(audio_dataset['emotion'].value_counts().to_string())\n",
    "    \n",
    "    # 5. Data splitting with verification\n",
    "    try:\n",
    "        train_data, val_data = train_test_split(\n",
    "            audio_dataset, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=audio_dataset['emotion']\n",
    "        )\n",
    "        print(f\"\\nData Split Successful:\")\n",
    "        print(f\"- Training samples: {len(train_data):,}\")\n",
    "        print(f\"- Validation samples: {len(val_data):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data splitting failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 6. Feature processing with progress tracking\n",
    "    print(\"\\nProcessing features...\")\n",
    "    try:\n",
    "        print(\"Processing training features:\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        print(\"Processing validation features:\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"❌ Feature extraction failed - empty results\")\n",
    "            if len(X_train) == 0:\n",
    "                print(\"- Training features are empty\")\n",
    "            if len(X_val) == 0:\n",
    "                print(\"- Validation features are empty\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nFeature Extraction Complete:\")\n",
    "        print(f\"- Training features shape: {X_train.shape}\")\n",
    "        print(f\"- Validation features shape: {X_val.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Feature processing failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 7. Label processing\n",
    "    print(\"\\nEncoding labels...\")\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train_raw)\n",
    "    y_val = encoder.transform(y_val_raw)\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    y_val_categorical = to_categorical(y_val)\n",
    "    \n",
    "    print(\"Classes detected:\", list(encoder.classes_))\n",
    "\n",
    "       # 8. Model construction - REPLACE JUST THIS SECTION\n",
    "    print(\"\\nBuilding optimized model...\")\n",
    "    try:\n",
    "        emotion_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "            \n",
    "            # Block 1\n",
    "            tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            \n",
    "            # Block 2  \n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            \n",
    "            # Block 3\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            # Classifier\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(len(encoder.classes_), activation='softmax')\n",
    "        ])\n",
    "        print(\"Model architecture:\")\n",
    "        emotion_model.summary()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model construction failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "       # 9. Model compilation - REPLACE JUST THIS SECTION\n",
    "    print(\"\\nCompiling model...\")\n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Increased LR\n",
    "        emotion_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"✔ Model successfully compiled\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model compilation failed: {str(e)}\")\n",
    "        return\n",
    "    # 10. Training phase - REPLACE JUST THIS SECTION\n",
    "    print(\"\\nStarting training...\")\n",
    "    try:\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                f'{RUN_NAME}_best.keras',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=100,\n",
    "            batch_size=64,  # Increased batch size\n",
    "            callbacks=callbacks,\n",
    "            class_weight=calculate_class_importance(y_train),\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"\\n✔ Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # [Rest of your evaluation code remains the same...]\n",
    "    # Make sure to add similar print statements throughout\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script execution started\")\n",
    "    execute_pipeline()\n",
    "    print(\"Script execution completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c11e5bc5-f8f6-4f5c-8da2-37a81b9f1e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train_categorical\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m emotion_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Input shape:\", X_train.shape)\n",
    "print(\"Output shape:\", y_train_categorical.shape)\n",
    "emotion_model.summary()  # Should show ~500K-1M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c8a4cd7-59b9-48a1-a7d6-1346751bcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Verification:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# After feature extraction, add these checks:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mData Verification:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmin(X_train)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(X_train)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_val range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmin(X_val)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(X_val)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample label distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(y_train_categorical, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# After feature extraction, add these checks:\n",
    "print(\"\\nData Verification:\")\n",
    "print(f\"X_train range: [{np.min(X_train):.2f}, {np.max(X_train):.2f}]\")\n",
    "print(f\"X_val range: [{np.min(X_val):.2f}, {np.max(X_val):.2f}]\")\n",
    "print(\"Sample label distribution:\", np.sum(y_train_categorical, axis=0))\n",
    "\n",
    "# Add quick sanity check\n",
    "if np.max(np.abs(X_train)) > 100 or np.isnan(X_train).any():\n",
    "    print(\"❌ Abnormal feature values detected!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778c19d3-960f-4f8e-87ac-9d29d1cd7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Speech files found: 1440\n",
      "🎵 Song files found: 1012\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "speech_files = glob.glob(speech_dir + r\"\\**\\*.wav\", recursive=True)\n",
    "song_files = glob.glob(song_dir + r\"\\**\\*.wav\", recursive=True)\n",
    "\n",
    "print(\"🎤 Speech files found:\", len(speech_files))\n",
    "print(\"🎵 Song files found:\", len(song_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a672d1-781b-4bdc-88f5-dd9b146fdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                    Dense, Dropout, Input, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69157f8-cbc4-46bb-bb44-6fade7b7b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUDIO_DURATION = 94  # ~3 seconds with hop_length=512\n",
    "MEL_BANDS = 32  \n",
    "FEATURE_DIMENSIONS = 2\n",
    "MODEL_ID = \"v6\"  \n",
    "MODEL_TAG = \"enhanced\"  \n",
    "RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06205523-566e-454b-bd16-e6473d40ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_array_dimensions(data_array, desired_shape):\n",
    "    \"\"\"Resize array to match target dimensions by padding or trimming\"\"\"\n",
    "    if data_array.size == 0:\n",
    "        return np.zeros(desired_shape)\n",
    "    \n",
    "    current_dims = data_array.shape\n",
    "    \n",
    "    # Process time axis\n",
    "    if len(current_dims) >= 2:\n",
    "        if current_dims[1] > desired_shape[1]:\n",
    "            data_array = data_array[:, :desired_shape[1]]\n",
    "        elif current_dims[1] < desired_shape[1]:\n",
    "            padding = [(0, 0), (0, desired_shape[1] - current_dims[1])]\n",
    "            data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    # Process frequency axis\n",
    "    if current_dims[0] > desired_shape[0]:\n",
    "        data_array = data_array[:desired_shape[0], :]\n",
    "    elif current_dims[0] < desired_shape[0]:\n",
    "        padding = [(0, desired_shape[0] - current_dims[0]), (0, 0)]\n",
    "        data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def standardize_features(feature_matrix):\n",
    "    \"\"\"Normalize feature values with robust handling\"\"\"\n",
    "    if feature_matrix.size == 0:\n",
    "        return feature_matrix\n",
    "    \n",
    "    mean_val = np.mean(feature_matrix)\n",
    "    std_val = np.std(feature_matrix)\n",
    "    \n",
    "    if std_val == 0 or np.isnan(std_val):\n",
    "        return feature_matrix - mean_val\n",
    "    \n",
    "    return (feature_matrix - mean_val) / std_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793fc87-9049-4be1-a1a8-f174d258dd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47470c4a-ca11-4d00-ad54-a500fa922dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Feature extraction pipeline with consistent output dimensions\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = 32\n",
    "        \n",
    "        try:\n",
    "            # First channel: Mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=mel_bands, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        try:\n",
    "            # Second channel: Combined audio features + Delta features\n",
    "            mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, \n",
    "                                               n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma_features = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            # Calculate delta features (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc_features)\n",
    "            chroma_delta = librosa.feature.delta(chroma_features)\n",
    "            contrast_delta = librosa.feature.delta(spectral_contrast)\n",
    "            \n",
    "            # Stack original features with delta features\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc_features, \n",
    "                mfcc_delta,\n",
    "                chroma_features,\n",
    "                chroma_delta,\n",
    "                spectral_contrast,\n",
    "                contrast_delta\n",
    "            ])\n",
    "            \n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        feature_stack = np.stack([channel_1, channel_2], axis=-1)\n",
    "        \n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ca5754-b5aa-47a4-9b88-c8fdc2a8e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Data augmentation for underrepresented classes\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = np.where(label_set == class_id)[0]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            augmentation = np.random.choice(['add_noise', 'time_warp', 'frequency_mask', 'amplitude_scale'])\n",
    "            \n",
    "            if augmentation == 'add_noise':\n",
    "                modified_sample = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif augmentation == 'time_warp':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif augmentation == 'frequency_mask':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            else:\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = class_id\n",
    "            current_position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b477f39c-2c45-4e4d-937e-7ac3290c5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Neural network architecture for emotion classification\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f441b1c-8654-41ed-a895-7d6dc6baddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8823b975-5ab1-402b-8333-8f2b387cff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache'):\n",
    "    \"\"\"Feature processing pipeline with caching\"\"\"\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(base_features, base_labels)\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "461def5f-fe59-4a6c-a2c6-a18a7e4b876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_audio_dataset_from_two_dirs(speech_dir, song_dir):\n",
    "    \"\"\"Create dataset from audio files in two folders (speech & song) with emotion labels\"\"\"\n",
    "    emotion_codes = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def process_dir(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_codes:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_codes[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Process both folders\n",
    "    process_dir(speech_dir)\n",
    "    process_dir(song_dir)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No valid emotion-labeled files found. Using placeholder labels.\")\n",
    "        all_audio_files = glob.glob(os.path.join(speech_dir, \"*.wav\")) + glob.glob(os.path.join(song_dir, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_audio_files))])\n",
    "        paths.extend(all_audio_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ff0c67-a313-400e-9691-3d0c0a89e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(model_file, audio_file, encoder_file):\n",
    "    \"\"\"Predict emotion from audio using trained model\"\"\"\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    with open(encoder_file, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    features = extract_audio_characteristics(audio_file)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_index]\n",
    "    \n",
    "    emotion_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return emotion_label, confidence\n",
    "\n",
    "def record_training_performance(training_history, model_identifier, train_accuracy, val_accuracy,\n",
    "                              overall_acc, f1_macro, f1_weighted,\n",
    "                              f2_macro, f2_weighted, avg_precision, avg_recall,\n",
    "                              per_class_acc, classification_report):\n",
    "    \"\"\"Save comprehensive training metrics\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_identifier}_performance_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Performance Report for {model_identifier}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model Version: {MODEL_ID}\\n\")\n",
    "        f.write(f\"Model Variant: {MODEL_TAG}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY METRICS:\\n\")\n",
    "        f.write(f\"Training Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F1: {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F2: {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F2: {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "        for class_name, accuracy in per_class_acc.items():\n",
    "            f.write(f\"{class_name}: {accuracy:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(classification_report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Performance report saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e3dd975-15e7-4d23-af23-785b5cbe3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1931 - loss: 2.1101\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16904, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.1936 - loss: 2.1083 - val_accuracy: 0.1690 - val_loss: 2.1151 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3192 - loss: 1.7497\n",
      "Epoch 2: val_accuracy improved from 0.16904 to 0.20163, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.3199 - loss: 1.7483 - val_accuracy: 0.2016 - val_loss: 2.1778 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4019 - loss: 1.5729\n",
      "Epoch 3: val_accuracy did not improve from 0.20163\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.4017 - loss: 1.5729 - val_accuracy: 0.1548 - val_loss: 2.5297 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4354 - loss: 1.4956\n",
      "Epoch 4: val_accuracy did not improve from 0.20163\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.4356 - loss: 1.4946 - val_accuracy: 0.1548 - val_loss: 3.1984 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4804 - loss: 1.3913\n",
      "Epoch 5: val_accuracy did not improve from 0.20163\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.4807 - loss: 1.3903 - val_accuracy: 0.1568 - val_loss: 3.4104 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5229 - loss: 1.2830\n",
      "Epoch 6: val_accuracy improved from 0.20163 to 0.29532, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.5231 - loss: 1.2824 - val_accuracy: 0.2953 - val_loss: 1.9091 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5477 - loss: 1.1845\n",
      "Epoch 7: val_accuracy did not improve from 0.29532\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.5480 - loss: 1.1846 - val_accuracy: 0.2301 - val_loss: 1.9844 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5640 - loss: 1.1373\n",
      "Epoch 8: val_accuracy improved from 0.29532 to 0.44807, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.5645 - loss: 1.1370 - val_accuracy: 0.4481 - val_loss: 1.3691 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6144 - loss: 1.0786\n",
      "Epoch 9: val_accuracy improved from 0.44807 to 0.49084, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.6141 - loss: 1.0788 - val_accuracy: 0.4908 - val_loss: 1.4084 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6202 - loss: 1.0169\n",
      "Epoch 10: val_accuracy improved from 0.49084 to 0.54990, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.6200 - loss: 1.0176 - val_accuracy: 0.5499 - val_loss: 1.2484 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6324 - loss: 0.9804\n",
      "Epoch 11: val_accuracy did not improve from 0.54990\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.6326 - loss: 0.9800 - val_accuracy: 0.5112 - val_loss: 1.2276 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6872 - loss: 0.8926\n",
      "Epoch 12: val_accuracy did not improve from 0.54990\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.6871 - loss: 0.8925 - val_accuracy: 0.5153 - val_loss: 1.3403 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6830 - loss: 0.8981\n",
      "Epoch 13: val_accuracy did not improve from 0.54990\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.6831 - loss: 0.8975 - val_accuracy: 0.4786 - val_loss: 1.5014 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7163 - loss: 0.7842\n",
      "Epoch 14: val_accuracy did not improve from 0.54990\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7161 - loss: 0.7843 - val_accuracy: 0.5234 - val_loss: 1.2806 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7285 - loss: 0.7593\n",
      "Epoch 15: val_accuracy did not improve from 0.54990\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.7282 - loss: 0.7600 - val_accuracy: 0.5356 - val_loss: 1.2285 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7568 - loss: 0.7044\n",
      "Epoch 16: val_accuracy improved from 0.54990 to 0.62322, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7567 - loss: 0.7044 - val_accuracy: 0.6232 - val_loss: 0.9696 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7659 - loss: 0.6499\n",
      "Epoch 17: val_accuracy improved from 0.62322 to 0.68635, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7655 - loss: 0.6508 - val_accuracy: 0.6864 - val_loss: 0.9047 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7827 - loss: 0.6168\n",
      "Epoch 18: val_accuracy did not improve from 0.68635\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7825 - loss: 0.6173 - val_accuracy: 0.4908 - val_loss: 1.6248 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7809 - loss: 0.6152\n",
      "Epoch 19: val_accuracy did not improve from 0.68635\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7806 - loss: 0.6158 - val_accuracy: 0.3564 - val_loss: 2.4495 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7770 - loss: 0.6368\n",
      "Epoch 20: val_accuracy did not improve from 0.68635\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.7771 - loss: 0.6363 - val_accuracy: 0.6640 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7992 - loss: 0.5584\n",
      "Epoch 21: val_accuracy did not improve from 0.68635\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7991 - loss: 0.5588 - val_accuracy: 0.5764 - val_loss: 1.1754 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8004 - loss: 0.5651\n",
      "Epoch 22: val_accuracy did not improve from 0.68635\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8005 - loss: 0.5648 - val_accuracy: 0.6843 - val_loss: 0.8253 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8224 - loss: 0.5083\n",
      "Epoch 23: val_accuracy improved from 0.68635 to 0.71894, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8220 - loss: 0.5092 - val_accuracy: 0.7189 - val_loss: 0.7999 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8540 - loss: 0.4393\n",
      "Epoch 24: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8534 - loss: 0.4404 - val_accuracy: 0.5825 - val_loss: 1.1960 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8296 - loss: 0.4858\n",
      "Epoch 25: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8296 - loss: 0.4856 - val_accuracy: 0.6802 - val_loss: 0.8393 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8203 - loss: 0.5029\n",
      "Epoch 26: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8203 - loss: 0.5029 - val_accuracy: 0.5642 - val_loss: 1.3056 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8204 - loss: 0.5043\n",
      "Epoch 27: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8205 - loss: 0.5037 - val_accuracy: 0.5418 - val_loss: 1.4751 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8459 - loss: 0.4410\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8459 - loss: 0.4410 - val_accuracy: 0.6843 - val_loss: 0.8096 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8766 - loss: 0.3751\n",
      "Epoch 29: val_accuracy improved from 0.71894 to 0.76578, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8764 - loss: 0.3752 - val_accuracy: 0.7658 - val_loss: 0.6117 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8685 - loss: 0.3602\n",
      "Epoch 30: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8686 - loss: 0.3600 - val_accuracy: 0.6680 - val_loss: 0.8830 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8701 - loss: 0.3417\n",
      "Epoch 31: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8703 - loss: 0.3414 - val_accuracy: 0.7251 - val_loss: 0.7088 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8789 - loss: 0.3353\n",
      "Epoch 32: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.8790 - loss: 0.3348 - val_accuracy: 0.7271 - val_loss: 0.7853 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8992 - loss: 0.2807\n",
      "Epoch 33: val_accuracy improved from 0.76578 to 0.77393, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8990 - loss: 0.2813 - val_accuracy: 0.7739 - val_loss: 0.6010 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9028 - loss: 0.2767\n",
      "Epoch 34: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9029 - loss: 0.2769 - val_accuracy: 0.7128 - val_loss: 0.7516 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9148 - loss: 0.2556\n",
      "Epoch 35: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9148 - loss: 0.2557 - val_accuracy: 0.7026 - val_loss: 0.8008 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9090 - loss: 0.2626\n",
      "Epoch 36: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9090 - loss: 0.2628 - val_accuracy: 0.7026 - val_loss: 0.7446 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9048 - loss: 0.2674\n",
      "Epoch 37: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9049 - loss: 0.2675 - val_accuracy: 0.6843 - val_loss: 0.8973 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9081 - loss: 0.2391\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.9082 - loss: 0.2394 - val_accuracy: 0.6456 - val_loss: 1.0970 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9238 - loss: 0.2352\n",
      "Epoch 39: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9239 - loss: 0.2350 - val_accuracy: 0.7312 - val_loss: 0.6103 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9314 - loss: 0.2167\n",
      "Epoch 40: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.9313 - loss: 0.2167 - val_accuracy: 0.7699 - val_loss: 0.5764 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9259 - loss: 0.2299\n",
      "Epoch 41: val_accuracy improved from 0.77393 to 0.77597, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.9258 - loss: 0.2301 - val_accuracy: 0.7760 - val_loss: 0.6339 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9340 - loss: 0.1952\n",
      "Epoch 42: val_accuracy improved from 0.77597 to 0.80448, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9339 - loss: 0.1956 - val_accuracy: 0.8045 - val_loss: 0.5563 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9285 - loss: 0.2102\n",
      "Epoch 43: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.9286 - loss: 0.2099 - val_accuracy: 0.7862 - val_loss: 0.5640 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9341 - loss: 0.2107\n",
      "Epoch 44: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9342 - loss: 0.2104 - val_accuracy: 0.7739 - val_loss: 0.5537 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9429 - loss: 0.1733\n",
      "Epoch 45: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.9428 - loss: 0.1734 - val_accuracy: 0.7821 - val_loss: 0.6072 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9458 - loss: 0.1806\n",
      "Epoch 46: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9456 - loss: 0.1807 - val_accuracy: 0.7841 - val_loss: 0.6120 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9282 - loss: 0.1968\n",
      "Epoch 47: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9281 - loss: 0.1966 - val_accuracy: 0.7862 - val_loss: 0.5824 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9403 - loss: 0.1675\n",
      "Epoch 48: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9403 - loss: 0.1677 - val_accuracy: 0.7637 - val_loss: 0.6531 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9359 - loss: 0.1818\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9360 - loss: 0.1819 - val_accuracy: 0.7352 - val_loss: 0.7217 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9451 - loss: 0.1650\n",
      "Epoch 50: val_accuracy did not improve from 0.80448\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.9452 - loss: 0.1647 - val_accuracy: 0.7943 - val_loss: 0.5169 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9958\n",
      "Validation Accuracy: 0.8045\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.8045\n",
      "Weighted F1: 0.8029\n",
      "Macro F1: 0.8029\n",
      "Weighted F2: 0.8028\n",
      "Macro F2: 0.8088\n",
      "Precision: 0.8101\n",
      "Recall: 0.8045\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.9333\n",
      "calm: 0.8400\n",
      "disgust: 0.9487\n",
      "fearful: 0.7333\n",
      "happy: 0.7067\n",
      "neutral: 0.8947\n",
      "sad: 0.7200\n",
      "surprised: 0.7436\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.83      0.93      0.88        75\n",
      "        calm       0.86      0.84      0.85        75\n",
      "     disgust       0.76      0.95      0.84        39\n",
      "     fearful       0.77      0.73      0.75        75\n",
      "       happy       0.87      0.71      0.78        75\n",
      "     neutral       0.68      0.89      0.77        38\n",
      "         sad       0.78      0.72      0.75        75\n",
      "   surprised       0.85      0.74      0.79        39\n",
      "\n",
      "    accuracy                           0.80       491\n",
      "   macro avg       0.80      0.82      0.80       491\n",
      "weighted avg       0.81      0.80      0.80       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250623_182829.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c30994-bcf9-4249-816b-90e009e137ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Feature extraction with optimized delta features\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        \n",
    "        try:\n",
    "            # Channel 1: Mel-spectrogram (keep unchanged)\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=MEL_BANDS, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        try:\n",
    "            # Channel 2: Strategic delta feature inclusion\n",
    "            mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, \n",
    "                                      n_fft=window_size, hop_length=frame_shift)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)  # Only MFCC deltas\n",
    "            \n",
    "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            # Selectively combine features (prioritizing MFCC and its deltas)\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc,          # Original MFCC (13)\n",
    "                mfcc_delta,    # MFCC deltas (13)\n",
    "                chroma,        # Chroma (12)\n",
    "                contrast[:6]   # Top 6 contrast features (instead of 7)\n",
    "            ])\n",
    "            \n",
    "            # Ensure we maintain the exact dimensions\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        feature_stack = np.stack([channel_1, channel_2], axis=-1)\n",
    "        \n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1280e8a-46a8-4d39-834e-fd9eb5aff233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SpatialDropout2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd675dc2-8e3f-44cf-991f-b44f4fc21bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Optimized network with better regularization\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # NEW: Spatial dropout for better feature map regularization\n",
    "    x = SpatialDropout2D(0.2)(x)  # Added <<<<<<<<<<<<\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # NEW: Kernel regularizer to prevent overfitting\n",
    "    output_layer = Dense(num_classes, \n",
    "                        activation='softmax',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Added <<<<<<<<\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da31066f-d9e0-4c09-b5f2-f287c40d0b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1678 - loss: 2.3664\n",
      "Epoch 1: val_accuracy improved from -inf to 0.17515, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.1686 - loss: 2.3623 - val_accuracy: 0.1752 - val_loss: 2.2184 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2362 - loss: 2.0227\n",
      "Epoch 2: val_accuracy did not improve from 0.17515\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.2367 - loss: 2.0218 - val_accuracy: 0.1527 - val_loss: 3.0305 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3566 - loss: 1.8021\n",
      "Epoch 3: val_accuracy did not improve from 0.17515\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.3564 - loss: 1.8014 - val_accuracy: 0.1527 - val_loss: 3.4330 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3905 - loss: 1.6641\n",
      "Epoch 4: val_accuracy did not improve from 0.17515\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3907 - loss: 1.6634 - val_accuracy: 0.1752 - val_loss: 2.5014 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.4187 - loss: 1.5849\n",
      "Epoch 5: val_accuracy improved from 0.17515 to 0.22607, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.4188 - loss: 1.5844 - val_accuracy: 0.2261 - val_loss: 2.2056 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4574 - loss: 1.4646\n",
      "Epoch 6: val_accuracy did not improve from 0.22607\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.4572 - loss: 1.4652 - val_accuracy: 0.1548 - val_loss: 3.2478 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4855 - loss: 1.3965\n",
      "Epoch 7: val_accuracy improved from 0.22607 to 0.27902, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.4859 - loss: 1.3958 - val_accuracy: 0.2790 - val_loss: 2.0120 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5003 - loss: 1.3331\n",
      "Epoch 8: val_accuracy improved from 0.27902 to 0.30754, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.5008 - loss: 1.3322 - val_accuracy: 0.3075 - val_loss: 1.8385 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5379 - loss: 1.2884\n",
      "Epoch 9: val_accuracy improved from 0.30754 to 0.46232, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.5381 - loss: 1.2878 - val_accuracy: 0.4623 - val_loss: 1.4150 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5974 - loss: 1.1169\n",
      "Epoch 10: val_accuracy improved from 0.46232 to 0.47251, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.5967 - loss: 1.1181 - val_accuracy: 0.4725 - val_loss: 1.3844 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5667 - loss: 1.1935\n",
      "Epoch 11: val_accuracy did not improve from 0.47251\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.5675 - loss: 1.1922 - val_accuracy: 0.4053 - val_loss: 1.5217 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5974 - loss: 1.0762\n",
      "Epoch 12: val_accuracy improved from 0.47251 to 0.55193, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.5977 - loss: 1.0758 - val_accuracy: 0.5519 - val_loss: 1.2644 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6358 - loss: 1.0126\n",
      "Epoch 13: val_accuracy did not improve from 0.55193\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.6356 - loss: 1.0129 - val_accuracy: 0.5051 - val_loss: 1.4353 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6444 - loss: 1.0122\n",
      "Epoch 14: val_accuracy did not improve from 0.55193\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.6445 - loss: 1.0124 - val_accuracy: 0.5397 - val_loss: 1.2514 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6562 - loss: 0.9671\n",
      "Epoch 15: val_accuracy improved from 0.55193 to 0.59063, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.6565 - loss: 0.9663 - val_accuracy: 0.5906 - val_loss: 1.1865 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6741 - loss: 0.9175\n",
      "Epoch 16: val_accuracy improved from 0.59063 to 0.59878, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.6742 - loss: 0.9177 - val_accuracy: 0.5988 - val_loss: 1.1062 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7204 - loss: 0.8317\n",
      "Epoch 17: val_accuracy improved from 0.59878 to 0.64766, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.7202 - loss: 0.8323 - val_accuracy: 0.6477 - val_loss: 1.0464 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7204 - loss: 0.8258\n",
      "Epoch 18: val_accuracy improved from 0.64766 to 0.66395, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7202 - loss: 0.8259 - val_accuracy: 0.6640 - val_loss: 1.0028 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7187 - loss: 0.8112\n",
      "Epoch 19: val_accuracy did not improve from 0.66395\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.7189 - loss: 0.8109 - val_accuracy: 0.5703 - val_loss: 1.1474 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7368 - loss: 0.7944\n",
      "Epoch 20: val_accuracy did not improve from 0.66395\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.7370 - loss: 0.7941 - val_accuracy: 0.3483 - val_loss: 1.9902 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7405 - loss: 0.7431\n",
      "Epoch 21: val_accuracy improved from 0.66395 to 0.67821, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.7407 - loss: 0.7432 - val_accuracy: 0.6782 - val_loss: 0.9310 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7531 - loss: 0.7529\n",
      "Epoch 22: val_accuracy did not improve from 0.67821\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7530 - loss: 0.7536 - val_accuracy: 0.5214 - val_loss: 1.2395 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7768 - loss: 0.6971\n",
      "Epoch 23: val_accuracy did not improve from 0.67821\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.7768 - loss: 0.6972 - val_accuracy: 0.6375 - val_loss: 1.0096 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7721 - loss: 0.6601\n",
      "Epoch 24: val_accuracy did not improve from 0.67821\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7722 - loss: 0.6599 - val_accuracy: 0.6762 - val_loss: 0.8813 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8038 - loss: 0.6324\n",
      "Epoch 25: val_accuracy did not improve from 0.67821\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8036 - loss: 0.6327 - val_accuracy: 0.6171 - val_loss: 1.1317 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8026 - loss: 0.6094\n",
      "Epoch 26: val_accuracy did not improve from 0.67821\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8024 - loss: 0.6098 - val_accuracy: 0.6721 - val_loss: 0.9716 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8001 - loss: 0.6131\n",
      "Epoch 27: val_accuracy improved from 0.67821 to 0.72098, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8001 - loss: 0.6132 - val_accuracy: 0.7210 - val_loss: 0.8492 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8162 - loss: 0.5991\n",
      "Epoch 28: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8162 - loss: 0.5987 - val_accuracy: 0.6660 - val_loss: 1.0296 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8319 - loss: 0.5320\n",
      "Epoch 29: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8317 - loss: 0.5325 - val_accuracy: 0.5377 - val_loss: 1.2793 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8227 - loss: 0.5424\n",
      "Epoch 30: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8226 - loss: 0.5428 - val_accuracy: 0.6843 - val_loss: 0.9266 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8178 - loss: 0.5356\n",
      "Epoch 31: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.8177 - loss: 0.5361 - val_accuracy: 0.6232 - val_loss: 0.9917 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8385 - loss: 0.5201\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8383 - loss: 0.5205 - val_accuracy: 0.6619 - val_loss: 0.9692 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8451 - loss: 0.4982\n",
      "Epoch 33: val_accuracy improved from 0.72098 to 0.76375, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8453 - loss: 0.4978 - val_accuracy: 0.7637 - val_loss: 0.7856 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8659 - loss: 0.4395\n",
      "Epoch 34: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8659 - loss: 0.4396 - val_accuracy: 0.7393 - val_loss: 0.7618 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8758 - loss: 0.4227\n",
      "Epoch 35: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8757 - loss: 0.4228 - val_accuracy: 0.7495 - val_loss: 0.7397 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8932 - loss: 0.3804 \n",
      "Epoch 36: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.8930 - loss: 0.3806 - val_accuracy: 0.7251 - val_loss: 0.7883 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8694 - loss: 0.4012\n",
      "Epoch 37: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8693 - loss: 0.4014 - val_accuracy: 0.6599 - val_loss: 0.9717 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8819 - loss: 0.4039\n",
      "Epoch 38: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8820 - loss: 0.4039 - val_accuracy: 0.6762 - val_loss: 1.0157 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8972 - loss: 0.3730\n",
      "Epoch 39: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8970 - loss: 0.3732 - val_accuracy: 0.7189 - val_loss: 0.7835 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9060 - loss: 0.3620\n",
      "Epoch 40: val_accuracy improved from 0.76375 to 0.76578, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9059 - loss: 0.3621 - val_accuracy: 0.7658 - val_loss: 0.6934 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8973 - loss: 0.3680\n",
      "Epoch 41: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8973 - loss: 0.3679 - val_accuracy: 0.7312 - val_loss: 0.7658 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9048 - loss: 0.3387\n",
      "Epoch 42: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9045 - loss: 0.3390 - val_accuracy: 0.7658 - val_loss: 0.6840 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9094 - loss: 0.3350\n",
      "Epoch 43: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9092 - loss: 0.3356 - val_accuracy: 0.7128 - val_loss: 0.8318 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8984 - loss: 0.3569\n",
      "Epoch 44: val_accuracy did not improve from 0.76578\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.8983 - loss: 0.3570 - val_accuracy: 0.7291 - val_loss: 0.7462 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8881 - loss: 0.3605\n",
      "Epoch 45: val_accuracy improved from 0.76578 to 0.77597, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.8884 - loss: 0.3599 - val_accuracy: 0.7760 - val_loss: 0.6933 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8998 - loss: 0.3328\n",
      "Epoch 46: val_accuracy improved from 0.77597 to 0.77800, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8998 - loss: 0.3329 - val_accuracy: 0.7780 - val_loss: 0.6614 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9057 - loss: 0.3448\n",
      "Epoch 47: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.9058 - loss: 0.3445 - val_accuracy: 0.7291 - val_loss: 0.7776 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9123 - loss: 0.3090\n",
      "Epoch 48: val_accuracy improved from 0.77800 to 0.78819, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9121 - loss: 0.3094 - val_accuracy: 0.7882 - val_loss: 0.7168 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9199 - loss: 0.3043\n",
      "Epoch 49: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9198 - loss: 0.3045 - val_accuracy: 0.7332 - val_loss: 0.8205 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8885 - loss: 0.3466\n",
      "Epoch 50: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.8888 - loss: 0.3459 - val_accuracy: 0.7515 - val_loss: 0.8038 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9809\n",
      "Validation Accuracy: 0.7882\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.7882\n",
      "Weighted F1: 0.7922\n",
      "Macro F1: 0.7892\n",
      "Weighted F2: 0.7873\n",
      "Macro F2: 0.7813\n",
      "Precision: 0.8165\n",
      "Recall: 0.7882\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.8933\n",
      "calm: 0.7467\n",
      "disgust: 0.7179\n",
      "fearful: 0.7867\n",
      "happy: 0.7333\n",
      "neutral: 0.8947\n",
      "sad: 0.8533\n",
      "surprised: 0.6154\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.96      0.89      0.92        75\n",
      "        calm       0.95      0.75      0.84        75\n",
      "     disgust       0.85      0.72      0.78        39\n",
      "     fearful       0.69      0.79      0.73        75\n",
      "       happy       0.86      0.73      0.79        75\n",
      "     neutral       0.74      0.89      0.81        38\n",
      "         sad       0.60      0.85      0.70        75\n",
      "   surprised       0.92      0.62      0.74        39\n",
      "\n",
      "    accuracy                           0.79       491\n",
      "   macro avg       0.82      0.78      0.79       491\n",
      "weighted avg       0.82      0.79      0.79       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250623_184920.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b899aa-f0ac-49b7-814f-c6dfd1a32aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Network with targeted regularization\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction (keep exactly the same)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # NEW: Add GaussianNoise layer (better than SpatialDropout for your case)\n",
    "    x = tf.keras.layers.GaussianNoise(0.01)(x)  # Only added line\n",
    "    \n",
    "    # Keep everything else identical\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e7f1ef-efb4-4dd2-8d44-485f515c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f6687ed-b63e-4a6f-b167-ddc80c24f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1523 - loss: 2.1753\n",
      "Epoch 1: val_accuracy improved from -inf to 0.15275, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.1527 - loss: 2.1739 - val_accuracy: 0.1527 - val_loss: 2.0798 - learning_rate: 3.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2762 - loss: 1.8813\n",
      "Epoch 2: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.2765 - loss: 1.8803 - val_accuracy: 0.1507 - val_loss: 2.0817 - learning_rate: 3.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3212 - loss: 1.7765\n",
      "Epoch 3: val_accuracy improved from 0.15275 to 0.25051, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.3217 - loss: 1.7752 - val_accuracy: 0.2505 - val_loss: 2.0101 - learning_rate: 3.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3728 - loss: 1.6310\n",
      "Epoch 4: val_accuracy did not improve from 0.25051\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.3732 - loss: 1.6304 - val_accuracy: 0.1833 - val_loss: 2.0160 - learning_rate: 3.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4068 - loss: 1.5494\n",
      "Epoch 5: val_accuracy did not improve from 0.25051\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.4071 - loss: 1.5490 - val_accuracy: 0.2077 - val_loss: 2.1176 - learning_rate: 3.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4367 - loss: 1.4911\n",
      "Epoch 6: val_accuracy improved from 0.25051 to 0.26884, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.4369 - loss: 1.4907 - val_accuracy: 0.2688 - val_loss: 1.8840 - learning_rate: 3.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4744 - loss: 1.3995\n",
      "Epoch 7: val_accuracy improved from 0.26884 to 0.30957, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.4747 - loss: 1.3991 - val_accuracy: 0.3096 - val_loss: 1.7335 - learning_rate: 3.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4941 - loss: 1.3570\n",
      "Epoch 8: val_accuracy improved from 0.30957 to 0.48473, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.4942 - loss: 1.3564 - val_accuracy: 0.4847 - val_loss: 1.4215 - learning_rate: 3.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5514 - loss: 1.2533\n",
      "Epoch 9: val_accuracy improved from 0.48473 to 0.49491, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.5510 - loss: 1.2539 - val_accuracy: 0.4949 - val_loss: 1.3795 - learning_rate: 3.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5425 - loss: 1.2359\n",
      "Epoch 10: val_accuracy improved from 0.49491 to 0.54175, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.5425 - loss: 1.2359 - val_accuracy: 0.5418 - val_loss: 1.3062 - learning_rate: 3.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5589 - loss: 1.2034\n",
      "Epoch 11: val_accuracy did not improve from 0.54175\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.5589 - loss: 1.2031 - val_accuracy: 0.4990 - val_loss: 1.3853 - learning_rate: 3.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5908 - loss: 1.1076\n",
      "Epoch 12: val_accuracy did not improve from 0.54175\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.5910 - loss: 1.1078 - val_accuracy: 0.4440 - val_loss: 1.3060 - learning_rate: 3.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6050 - loss: 1.1043\n",
      "Epoch 13: val_accuracy improved from 0.54175 to 0.54379, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.6051 - loss: 1.1037 - val_accuracy: 0.5438 - val_loss: 1.1765 - learning_rate: 3.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6096 - loss: 1.0828 \n",
      "Epoch 14: val_accuracy improved from 0.54379 to 0.59063, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.6097 - loss: 1.0817 - val_accuracy: 0.5906 - val_loss: 1.1191 - learning_rate: 3.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6299 - loss: 1.0200\n",
      "Epoch 15: val_accuracy improved from 0.59063 to 0.60081, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.6299 - loss: 1.0200 - val_accuracy: 0.6008 - val_loss: 1.1400 - learning_rate: 3.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6346 - loss: 0.9994\n",
      "Epoch 16: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.6347 - loss: 0.9994 - val_accuracy: 0.4481 - val_loss: 1.3967 - learning_rate: 3.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6492 - loss: 0.9193\n",
      "Epoch 17: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.6494 - loss: 0.9196 - val_accuracy: 0.5173 - val_loss: 1.2365 - learning_rate: 3.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6650 - loss: 0.9284\n",
      "Epoch 18: val_accuracy improved from 0.60081 to 0.66802, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6653 - loss: 0.9286 - val_accuracy: 0.6680 - val_loss: 0.9636 - learning_rate: 3.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6826 - loss: 0.8803 \n",
      "Epoch 19: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.6827 - loss: 0.8800 - val_accuracy: 0.5906 - val_loss: 1.1399 - learning_rate: 3.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7045 - loss: 0.8447\n",
      "Epoch 20: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7042 - loss: 0.8449 - val_accuracy: 0.6334 - val_loss: 1.0005 - learning_rate: 3.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7349 - loss: 0.7787\n",
      "Epoch 21: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7342 - loss: 0.7797 - val_accuracy: 0.6110 - val_loss: 1.0698 - learning_rate: 3.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7146 - loss: 0.8222\n",
      "Epoch 22: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7143 - loss: 0.8224 - val_accuracy: 0.6008 - val_loss: 1.0309 - learning_rate: 3.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7431 - loss: 0.7411\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.7427 - loss: 0.7415 - val_accuracy: 0.6008 - val_loss: 1.1511 - learning_rate: 3.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7452 - loss: 0.7397\n",
      "Epoch 24: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7453 - loss: 0.7394 - val_accuracy: 0.6517 - val_loss: 1.0258 - learning_rate: 1.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7506 - loss: 0.7091\n",
      "Epoch 25: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7507 - loss: 0.7091 - val_accuracy: 0.6334 - val_loss: 0.9630 - learning_rate: 1.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7657 - loss: 0.6466\n",
      "Epoch 26: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7658 - loss: 0.6471 - val_accuracy: 0.6212 - val_loss: 1.0335 - learning_rate: 1.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7746 - loss: 0.6677\n",
      "Epoch 27: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7745 - loss: 0.6676 - val_accuracy: 0.6151 - val_loss: 1.0371 - learning_rate: 1.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7676 - loss: 0.6413\n",
      "Epoch 28: val_accuracy did not improve from 0.66802\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7679 - loss: 0.6412 - val_accuracy: 0.6558 - val_loss: 0.9049 - learning_rate: 1.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7878 - loss: 0.6198 \n",
      "Epoch 29: val_accuracy improved from 0.66802 to 0.69450, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.7877 - loss: 0.6202 - val_accuracy: 0.6945 - val_loss: 0.8808 - learning_rate: 1.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8028 - loss: 0.5841\n",
      "Epoch 30: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8023 - loss: 0.5851 - val_accuracy: 0.6823 - val_loss: 0.8478 - learning_rate: 1.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7926 - loss: 0.6055\n",
      "Epoch 31: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.7926 - loss: 0.6053 - val_accuracy: 0.6680 - val_loss: 0.8716 - learning_rate: 1.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7954 - loss: 0.5875 \n",
      "Epoch 32: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.7954 - loss: 0.5876 - val_accuracy: 0.6701 - val_loss: 0.9440 - learning_rate: 1.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7937 - loss: 0.5981\n",
      "Epoch 33: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7936 - loss: 0.5984 - val_accuracy: 0.6884 - val_loss: 0.8804 - learning_rate: 1.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7845 - loss: 0.5958\n",
      "Epoch 34: val_accuracy improved from 0.69450 to 0.73727, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7848 - loss: 0.5952 - val_accuracy: 0.7373 - val_loss: 0.7893 - learning_rate: 1.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7844 - loss: 0.5647\n",
      "Epoch 35: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.7848 - loss: 0.5641 - val_accuracy: 0.6660 - val_loss: 0.8611 - learning_rate: 1.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7906 - loss: 0.6061\n",
      "Epoch 36: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7909 - loss: 0.6055 - val_accuracy: 0.6090 - val_loss: 1.1058 - learning_rate: 1.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8041 - loss: 0.5537\n",
      "Epoch 37: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.8041 - loss: 0.5537 - val_accuracy: 0.6721 - val_loss: 0.8504 - learning_rate: 1.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8214 - loss: 0.5295\n",
      "Epoch 38: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8214 - loss: 0.5296 - val_accuracy: 0.7088 - val_loss: 0.8102 - learning_rate: 1.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8144 - loss: 0.5480\n",
      "Epoch 39: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8147 - loss: 0.5473 - val_accuracy: 0.7332 - val_loss: 0.7743 - learning_rate: 1.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8168 - loss: 0.5276\n",
      "Epoch 40: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8171 - loss: 0.5273 - val_accuracy: 0.6802 - val_loss: 0.8229 - learning_rate: 1.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8363 - loss: 0.4928\n",
      "Epoch 41: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8363 - loss: 0.4929 - val_accuracy: 0.7189 - val_loss: 0.7351 - learning_rate: 1.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8226 - loss: 0.5070\n",
      "Epoch 42: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8225 - loss: 0.5073 - val_accuracy: 0.6395 - val_loss: 0.9426 - learning_rate: 1.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8307 - loss: 0.4878\n",
      "Epoch 43: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.8305 - loss: 0.4882 - val_accuracy: 0.7149 - val_loss: 0.7487 - learning_rate: 1.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8352 - loss: 0.4811\n",
      "Epoch 44: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.8353 - loss: 0.4811 - val_accuracy: 0.6619 - val_loss: 0.8943 - learning_rate: 1.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8445 - loss: 0.4723\n",
      "Epoch 45: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8444 - loss: 0.4723 - val_accuracy: 0.7352 - val_loss: 0.7608 - learning_rate: 1.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8444 - loss: 0.4534\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.73727\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8442 - loss: 0.4538 - val_accuracy: 0.7230 - val_loss: 0.7723 - learning_rate: 1.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8497 - loss: 0.4519\n",
      "Epoch 47: val_accuracy improved from 0.73727 to 0.75356, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.8498 - loss: 0.4516 - val_accuracy: 0.7536 - val_loss: 0.7011 - learning_rate: 7.5000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8529 - loss: 0.4262\n",
      "Epoch 48: val_accuracy did not improve from 0.75356\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.8527 - loss: 0.4266 - val_accuracy: 0.7210 - val_loss: 0.7541 - learning_rate: 7.5000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8469 - loss: 0.4393 \n",
      "Epoch 49: val_accuracy did not improve from 0.75356\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 102ms/step - accuracy: 0.8470 - loss: 0.4395 - val_accuracy: 0.7230 - val_loss: 0.7154 - learning_rate: 7.5000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8538 - loss: 0.4305\n",
      "Epoch 50: val_accuracy did not improve from 0.75356\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8538 - loss: 0.4304 - val_accuracy: 0.7088 - val_loss: 0.7366 - learning_rate: 7.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9564\n",
      "Validation Accuracy: 0.7536\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.7536\n",
      "Weighted F1: 0.7520\n",
      "Macro F1: 0.7524\n",
      "Weighted F2: 0.7519\n",
      "Macro F2: 0.7569\n",
      "Precision: 0.7580\n",
      "Recall: 0.7536\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.8533\n",
      "calm: 0.7600\n",
      "disgust: 0.9487\n",
      "fearful: 0.8133\n",
      "happy: 0.6667\n",
      "neutral: 0.7368\n",
      "sad: 0.6000\n",
      "surprised: 0.7179\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.90      0.85      0.88        75\n",
      "        calm       0.78      0.76      0.77        75\n",
      "     disgust       0.67      0.95      0.79        39\n",
      "     fearful       0.70      0.81      0.75        75\n",
      "       happy       0.77      0.67      0.71        75\n",
      "     neutral       0.78      0.74      0.76        38\n",
      "         sad       0.69      0.60      0.64        75\n",
      "   surprised       0.72      0.72      0.72        39\n",
      "\n",
      "    accuracy                           0.75       491\n",
      "   macro avg       0.75      0.76      0.75       491\n",
      "weighted avg       0.76      0.75      0.75       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250623_190012.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0003),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcfa40a-4dc6-4d24-8dc8-9812837ab040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Original working architecture\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Only modification: Added L2 regularization here\n",
    "    x = Dense(128, activation='relu', \n",
    "              kernel_regularizer=regularizers.l2(0.001))(x)  # Very mild regularization\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e55a6e-7ca1-470a-8b99-26b017467d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=12,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee485b9-86f2-49a3-a8fd-b19a20b68e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m74/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1716 - loss: 2.3344\n",
      "Epoch 1: val_accuracy improved from -inf to 0.08554, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.1728 - loss: 2.3283 - val_accuracy: 0.0855 - val_loss: 2.2027 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2724 - loss: 2.0255\n",
      "Epoch 2: val_accuracy improved from 0.08554 to 0.15275, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.2727 - loss: 2.0247 - val_accuracy: 0.1527 - val_loss: 2.2673 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3553 - loss: 1.8299\n",
      "Epoch 3: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.3552 - loss: 1.8293 - val_accuracy: 0.1527 - val_loss: 2.4805 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3620 - loss: 1.7476\n",
      "Epoch 4: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.3623 - loss: 1.7471 - val_accuracy: 0.1527 - val_loss: 2.8307 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4285 - loss: 1.6042\n",
      "Epoch 5: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4286 - loss: 1.6037 - val_accuracy: 0.1527 - val_loss: 3.2636 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4868 - loss: 1.5057\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.15275 to 0.16090, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.4871 - loss: 1.5051 - val_accuracy: 0.1609 - val_loss: 2.7101 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4879 - loss: 1.4531\n",
      "Epoch 7: val_accuracy improved from 0.16090 to 0.19959, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.4887 - loss: 1.4519 - val_accuracy: 0.1996 - val_loss: 2.4801 - learning_rate: 2.5000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5264 - loss: 1.3777\n",
      "Epoch 8: val_accuracy improved from 0.19959 to 0.31161, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.5267 - loss: 1.3769 - val_accuracy: 0.3116 - val_loss: 1.8521 - learning_rate: 2.5000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5620 - loss: 1.3008\n",
      "Epoch 9: val_accuracy improved from 0.31161 to 0.39308, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.5619 - loss: 1.3006 - val_accuracy: 0.3931 - val_loss: 1.6686 - learning_rate: 2.5000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5875 - loss: 1.2526\n",
      "Epoch 10: val_accuracy improved from 0.39308 to 0.46640, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.5872 - loss: 1.2529 - val_accuracy: 0.4664 - val_loss: 1.5225 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6137 - loss: 1.1865\n",
      "Epoch 11: val_accuracy improved from 0.46640 to 0.55601, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.6134 - loss: 1.1868 - val_accuracy: 0.5560 - val_loss: 1.3322 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6068 - loss: 1.1654 \n",
      "Epoch 12: val_accuracy did not improve from 0.55601\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.6069 - loss: 1.1656 - val_accuracy: 0.5418 - val_loss: 1.3330 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6218 - loss: 1.1545\n",
      "Epoch 13: val_accuracy did not improve from 0.55601\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.6219 - loss: 1.1540 - val_accuracy: 0.5560 - val_loss: 1.3041 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6367 - loss: 1.0925\n",
      "Epoch 14: val_accuracy improved from 0.55601 to 0.56619, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.6367 - loss: 1.0927 - val_accuracy: 0.5662 - val_loss: 1.3009 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6487 - loss: 1.0688\n",
      "Epoch 15: val_accuracy improved from 0.56619 to 0.58045, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.6487 - loss: 1.0686 - val_accuracy: 0.5804 - val_loss: 1.2430 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6539 - loss: 1.0770\n",
      "Epoch 16: val_accuracy did not improve from 0.58045\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.6540 - loss: 1.0769 - val_accuracy: 0.5560 - val_loss: 1.2795 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6984 - loss: 0.9993\n",
      "Epoch 17: val_accuracy improved from 0.58045 to 0.59063, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.6977 - loss: 0.9997 - val_accuracy: 0.5906 - val_loss: 1.2090 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6485 - loss: 1.0312\n",
      "Epoch 18: val_accuracy did not improve from 0.59063\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.6486 - loss: 1.0312 - val_accuracy: 0.5804 - val_loss: 1.2580 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6846 - loss: 0.9513\n",
      "Epoch 19: val_accuracy improved from 0.59063 to 0.60081, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.6852 - loss: 0.9507 - val_accuracy: 0.6008 - val_loss: 1.1808 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6986 - loss: 0.9291\n",
      "Epoch 20: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.6987 - loss: 0.9292 - val_accuracy: 0.5132 - val_loss: 1.3758 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6995 - loss: 0.9337\n",
      "Epoch 21: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.6997 - loss: 0.9331 - val_accuracy: 0.5642 - val_loss: 1.2966 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6985 - loss: 0.8969\n",
      "Epoch 22: val_accuracy improved from 0.60081 to 0.61507, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6990 - loss: 0.8962 - val_accuracy: 0.6151 - val_loss: 1.1070 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7269 - loss: 0.8388\n",
      "Epoch 23: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.7268 - loss: 0.8393 - val_accuracy: 0.6130 - val_loss: 1.1271 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7255 - loss: 0.8728\n",
      "Epoch 24: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7255 - loss: 0.8727 - val_accuracy: 0.5845 - val_loss: 1.1993 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7486 - loss: 0.8024\n",
      "Epoch 25: val_accuracy improved from 0.61507 to 0.64766, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.7486 - loss: 0.8028 - val_accuracy: 0.6477 - val_loss: 1.0570 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7535 - loss: 0.7973\n",
      "Epoch 26: val_accuracy did not improve from 0.64766\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.7537 - loss: 0.7973 - val_accuracy: 0.6029 - val_loss: 1.1886 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7468 - loss: 0.8031\n",
      "Epoch 27: val_accuracy improved from 0.64766 to 0.65173, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7471 - loss: 0.8027 - val_accuracy: 0.6517 - val_loss: 1.0445 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7784 - loss: 0.7274\n",
      "Epoch 28: val_accuracy did not improve from 0.65173\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.7780 - loss: 0.7281 - val_accuracy: 0.6212 - val_loss: 1.0850 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7664 - loss: 0.7504\n",
      "Epoch 29: val_accuracy improved from 0.65173 to 0.68024, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.7665 - loss: 0.7504 - val_accuracy: 0.6802 - val_loss: 0.9680 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7801 - loss: 0.7221\n",
      "Epoch 30: val_accuracy did not improve from 0.68024\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7801 - loss: 0.7220 - val_accuracy: 0.6253 - val_loss: 1.0858 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7760 - loss: 0.7282\n",
      "Epoch 31: val_accuracy did not improve from 0.68024\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.7759 - loss: 0.7285 - val_accuracy: 0.5947 - val_loss: 1.1465 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7974 - loss: 0.6748\n",
      "Epoch 32: val_accuracy did not improve from 0.68024\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7971 - loss: 0.6757 - val_accuracy: 0.6253 - val_loss: 1.0841 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7936 - loss: 0.6882\n",
      "Epoch 33: val_accuracy did not improve from 0.68024\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7937 - loss: 0.6879 - val_accuracy: 0.6456 - val_loss: 1.0367 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7963 - loss: 0.6807\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.68024\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.7965 - loss: 0.6806 - val_accuracy: 0.6619 - val_loss: 1.0004 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8020 - loss: 0.6692\n",
      "Epoch 35: val_accuracy improved from 0.68024 to 0.69654, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8021 - loss: 0.6685 - val_accuracy: 0.6965 - val_loss: 0.8936 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8398 - loss: 0.5808\n",
      "Epoch 36: val_accuracy did not improve from 0.69654\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8397 - loss: 0.5810 - val_accuracy: 0.6945 - val_loss: 0.9401 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8396 - loss: 0.5698\n",
      "Epoch 37: val_accuracy did not improve from 0.69654\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8395 - loss: 0.5702 - val_accuracy: 0.6782 - val_loss: 0.9194 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8495 - loss: 0.5567\n",
      "Epoch 38: val_accuracy did not improve from 0.69654\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8491 - loss: 0.5575 - val_accuracy: 0.6578 - val_loss: 1.0143 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8475 - loss: 0.5528\n",
      "Epoch 39: val_accuracy did not improve from 0.69654\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8472 - loss: 0.5531 - val_accuracy: 0.6945 - val_loss: 0.9112 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8412 - loss: 0.5511\n",
      "Epoch 40: val_accuracy improved from 0.69654 to 0.70876, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.8410 - loss: 0.5517 - val_accuracy: 0.7088 - val_loss: 0.8663 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8344 - loss: 0.5712\n",
      "Epoch 41: val_accuracy improved from 0.70876 to 0.71690, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8346 - loss: 0.5712 - val_accuracy: 0.7169 - val_loss: 0.8912 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8405 - loss: 0.5654\n",
      "Epoch 42: val_accuracy improved from 0.71690 to 0.71894, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8407 - loss: 0.5651 - val_accuracy: 0.7189 - val_loss: 0.8821 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8333 - loss: 0.5653\n",
      "Epoch 43: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8334 - loss: 0.5651 - val_accuracy: 0.7128 - val_loss: 0.9185 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8375 - loss: 0.5573\n",
      "Epoch 44: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8376 - loss: 0.5571 - val_accuracy: 0.7189 - val_loss: 0.8720 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8345 - loss: 0.5797\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8346 - loss: 0.5791 - val_accuracy: 0.7108 - val_loss: 0.8710 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8441 - loss: 0.5344\n",
      "Epoch 46: val_accuracy did not improve from 0.71894\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8444 - loss: 0.5338 - val_accuracy: 0.7067 - val_loss: 0.8632 - learning_rate: 6.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8652 - loss: 0.5102\n",
      "Epoch 47: val_accuracy improved from 0.71894 to 0.74542, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8651 - loss: 0.5100 - val_accuracy: 0.7454 - val_loss: 0.8147 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8744 - loss: 0.4813\n",
      "Epoch 48: val_accuracy did not improve from 0.74542\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8742 - loss: 0.4818 - val_accuracy: 0.7169 - val_loss: 0.8451 - learning_rate: 6.2500e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8667 - loss: 0.4854\n",
      "Epoch 49: val_accuracy did not improve from 0.74542\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8667 - loss: 0.4855 - val_accuracy: 0.7312 - val_loss: 0.8191 - learning_rate: 6.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8748 - loss: 0.4718\n",
      "Epoch 50: val_accuracy did not improve from 0.74542\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8746 - loss: 0.4723 - val_accuracy: 0.6945 - val_loss: 0.8684 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9547\n",
      "Validation Accuracy: 0.7454\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.7454\n",
      "Weighted F1: 0.7434\n",
      "Macro F1: 0.7400\n",
      "Weighted F2: 0.7436\n",
      "Macro F2: 0.7428\n",
      "Precision: 0.7499\n",
      "Recall: 0.7454\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.8400\n",
      "calm: 0.8267\n",
      "disgust: 0.6667\n",
      "fearful: 0.7467\n",
      "happy: 0.7733\n",
      "neutral: 0.8684\n",
      "sad: 0.5333\n",
      "surprised: 0.7179\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.90      0.84      0.87        75\n",
      "        calm       0.77      0.83      0.79        75\n",
      "     disgust       0.72      0.67      0.69        39\n",
      "     fearful       0.67      0.75      0.71        75\n",
      "       happy       0.75      0.77      0.76        75\n",
      "     neutral       0.63      0.87      0.73        38\n",
      "         sad       0.71      0.53      0.61        75\n",
      "   surprised       0.78      0.72      0.75        39\n",
      "\n",
      "    accuracy                           0.75       491\n",
      "   macro avg       0.74      0.75      0.74       491\n",
      "weighted avg       0.75      0.75      0.74       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250623_194040.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6137810-1c99-48a6-a526-77062c296af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Original architecture that gave 80% validation accuracy\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Keep EXACTLY your original layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14f5d7f-0561-4bb1-809a-feae1bbd8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e8c2ba-6e35-4756-b815-e5c518647c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Optimized feature extraction maintaining original performance\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        # Audio loading (unchanged)\n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        \n",
    "        # Channel 1: Mel-spectrogram (keep identical)\n",
    "        try:\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=MEL_BANDS, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        # Channel 2: Revised feature combination\n",
    "        try:\n",
    "            # 1. Basic features\n",
    "            mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=10,  # Reduced from 13\n",
    "                                      n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            # 2. Only use MFCC deltas (no other deltas)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            \n",
    "            # 3. Conservative combination\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc,          # 10 coefficients\n",
    "                mfcc_delta,    # 10 delta coefficients\n",
    "                chroma         # 12 chroma features\n",
    "            ])\n",
    "            \n",
    "            # Ensure exact dimensions (pad if needed)\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        return np.stack([channel_1, channel_2], axis=-1)\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513b5a5f-8e18-4a16-8d30-8807947b4eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1881 - loss: 2.1107\n",
      "Epoch 1: val_accuracy improved from -inf to 0.15275, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.1886 - loss: 2.1091 - val_accuracy: 0.1527 - val_loss: 2.1843 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3439 - loss: 1.7348\n",
      "Epoch 2: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.3440 - loss: 1.7338 - val_accuracy: 0.1527 - val_loss: 3.5376 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3904 - loss: 1.5785\n",
      "Epoch 3: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.3908 - loss: 1.5777 - val_accuracy: 0.1527 - val_loss: 3.7697 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4327 - loss: 1.4874\n",
      "Epoch 4: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.4331 - loss: 1.4869 - val_accuracy: 0.1527 - val_loss: 2.9012 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4901 - loss: 1.3687\n",
      "Epoch 5: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.4901 - loss: 1.3686 - val_accuracy: 0.1527 - val_loss: 4.2171 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5437 - loss: 1.2783\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.5437 - loss: 1.2780 - val_accuracy: 0.1527 - val_loss: 3.9227 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5552 - loss: 1.2147 \n",
      "Epoch 7: val_accuracy improved from 0.15275 to 0.18941, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.5556 - loss: 1.2133 - val_accuracy: 0.1894 - val_loss: 2.7393 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6161 - loss: 1.0736\n",
      "Epoch 8: val_accuracy improved from 0.18941 to 0.50916, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.6158 - loss: 1.0739 - val_accuracy: 0.5092 - val_loss: 1.2793 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6129 - loss: 1.0523\n",
      "Epoch 9: val_accuracy did not improve from 0.50916\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.6132 - loss: 1.0514 - val_accuracy: 0.3503 - val_loss: 1.9578 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6445 - loss: 0.9794\n",
      "Epoch 10: val_accuracy improved from 0.50916 to 0.56008, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.6445 - loss: 0.9794 - val_accuracy: 0.5601 - val_loss: 1.2027 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6554 - loss: 0.9516\n",
      "Epoch 11: val_accuracy did not improve from 0.56008\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.6556 - loss: 0.9512 - val_accuracy: 0.5601 - val_loss: 1.1573 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6688 - loss: 0.9364\n",
      "Epoch 12: val_accuracy improved from 0.56008 to 0.60081, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6686 - loss: 0.9365 - val_accuracy: 0.6008 - val_loss: 1.1059 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6938 - loss: 0.8538\n",
      "Epoch 13: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.6938 - loss: 0.8537 - val_accuracy: 0.5845 - val_loss: 1.1356 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7165 - loss: 0.8192\n",
      "Epoch 14: val_accuracy did not improve from 0.60081\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7163 - loss: 0.8192 - val_accuracy: 0.6008 - val_loss: 1.0528 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7081 - loss: 0.8043\n",
      "Epoch 15: val_accuracy improved from 0.60081 to 0.62525, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.7083 - loss: 0.8041 - val_accuracy: 0.6253 - val_loss: 1.0045 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7161 - loss: 0.7614\n",
      "Epoch 16: val_accuracy did not improve from 0.62525\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.7162 - loss: 0.7611 - val_accuracy: 0.6049 - val_loss: 1.0313 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7673 - loss: 0.6870\n",
      "Epoch 17: val_accuracy did not improve from 0.62525\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7671 - loss: 0.6876 - val_accuracy: 0.6212 - val_loss: 1.0420 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7442 - loss: 0.7142 \n",
      "Epoch 18: val_accuracy improved from 0.62525 to 0.65784, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.7445 - loss: 0.7137 - val_accuracy: 0.6578 - val_loss: 0.9287 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7833 - loss: 0.6515\n",
      "Epoch 19: val_accuracy did not improve from 0.65784\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7830 - loss: 0.6516 - val_accuracy: 0.4929 - val_loss: 1.4914 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7654 - loss: 0.6597\n",
      "Epoch 20: val_accuracy did not improve from 0.65784\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.7654 - loss: 0.6594 - val_accuracy: 0.5906 - val_loss: 1.1305 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7744 - loss: 0.6144\n",
      "Epoch 21: val_accuracy did not improve from 0.65784\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7745 - loss: 0.6145 - val_accuracy: 0.5825 - val_loss: 1.1488 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7966 - loss: 0.5820\n",
      "Epoch 22: val_accuracy did not improve from 0.65784\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.7965 - loss: 0.5818 - val_accuracy: 0.5499 - val_loss: 1.1795 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7965 - loss: 0.5491\n",
      "Epoch 23: val_accuracy improved from 0.65784 to 0.67210, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.7961 - loss: 0.5498 - val_accuracy: 0.6721 - val_loss: 0.9066 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8050 - loss: 0.5840\n",
      "Epoch 24: val_accuracy did not improve from 0.67210\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.8051 - loss: 0.5834 - val_accuracy: 0.4827 - val_loss: 1.6724 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7863 - loss: 0.5659\n",
      "Epoch 25: val_accuracy did not improve from 0.67210\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7869 - loss: 0.5650 - val_accuracy: 0.6110 - val_loss: 1.0768 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8299 - loss: 0.4819\n",
      "Epoch 26: val_accuracy did not improve from 0.67210\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8295 - loss: 0.4826 - val_accuracy: 0.5703 - val_loss: 1.2563 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7976 - loss: 0.5258\n",
      "Epoch 27: val_accuracy did not improve from 0.67210\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.7980 - loss: 0.5253 - val_accuracy: 0.5112 - val_loss: 1.8359 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8119 - loss: 0.5278\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.67210\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8123 - loss: 0.5271 - val_accuracy: 0.6130 - val_loss: 1.0543 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8474 - loss: 0.4498\n",
      "Epoch 29: val_accuracy improved from 0.67210 to 0.76782, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.8474 - loss: 0.4493 - val_accuracy: 0.7678 - val_loss: 0.6829 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8564 - loss: 0.4104\n",
      "Epoch 30: val_accuracy improved from 0.76782 to 0.77800, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8565 - loss: 0.4105 - val_accuracy: 0.7780 - val_loss: 0.6526 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8675 - loss: 0.4024\n",
      "Epoch 31: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 98ms/step - accuracy: 0.8674 - loss: 0.4025 - val_accuracy: 0.7006 - val_loss: 0.8390 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8837 - loss: 0.3576\n",
      "Epoch 32: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8835 - loss: 0.3580 - val_accuracy: 0.7393 - val_loss: 0.7308 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8772 - loss: 0.3518\n",
      "Epoch 33: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.8773 - loss: 0.3519 - val_accuracy: 0.7658 - val_loss: 0.6750 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8727 - loss: 0.3616\n",
      "Epoch 34: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8728 - loss: 0.3612 - val_accuracy: 0.7332 - val_loss: 0.7104 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8748 - loss: 0.3570\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8748 - loss: 0.3571 - val_accuracy: 0.7210 - val_loss: 0.7507 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8920 - loss: 0.3220\n",
      "Epoch 36: val_accuracy did not improve from 0.77800\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.8920 - loss: 0.3218 - val_accuracy: 0.7576 - val_loss: 0.6718 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8981 - loss: 0.2930\n",
      "Epoch 37: val_accuracy improved from 0.77800 to 0.78004, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8980 - loss: 0.2931 - val_accuracy: 0.7800 - val_loss: 0.6345 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9105 - loss: 0.3019\n",
      "Epoch 38: val_accuracy did not improve from 0.78004\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9105 - loss: 0.3018 - val_accuracy: 0.7699 - val_loss: 0.6191 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9033 - loss: 0.2900\n",
      "Epoch 39: val_accuracy improved from 0.78004 to 0.78208, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.9034 - loss: 0.2899 - val_accuracy: 0.7821 - val_loss: 0.6235 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9090 - loss: 0.2817\n",
      "Epoch 40: val_accuracy did not improve from 0.78208\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9088 - loss: 0.2820 - val_accuracy: 0.7434 - val_loss: 0.6819 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9128 - loss: 0.2745\n",
      "Epoch 41: val_accuracy did not improve from 0.78208\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9124 - loss: 0.2752 - val_accuracy: 0.7800 - val_loss: 0.6413 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9090 - loss: 0.2636\n",
      "Epoch 42: val_accuracy improved from 0.78208 to 0.78411, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.9090 - loss: 0.2636 - val_accuracy: 0.7841 - val_loss: 0.6056 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9153 - loss: 0.2742\n",
      "Epoch 43: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9150 - loss: 0.2746 - val_accuracy: 0.7841 - val_loss: 0.6305 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8945 - loss: 0.2916\n",
      "Epoch 44: val_accuracy improved from 0.78411 to 0.80041, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8946 - loss: 0.2914 - val_accuracy: 0.8004 - val_loss: 0.6115 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9066 - loss: 0.2632\n",
      "Epoch 45: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9065 - loss: 0.2634 - val_accuracy: 0.7597 - val_loss: 0.6502 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9136 - loss: 0.2545\n",
      "Epoch 46: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9135 - loss: 0.2549 - val_accuracy: 0.7434 - val_loss: 0.6835 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9079 - loss: 0.2629\n",
      "Epoch 47: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.9077 - loss: 0.2632 - val_accuracy: 0.7800 - val_loss: 0.6022 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9168 - loss: 0.2519\n",
      "Epoch 48: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9165 - loss: 0.2524 - val_accuracy: 0.7800 - val_loss: 0.5873 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9062 - loss: 0.2759\n",
      "Epoch 49: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9062 - loss: 0.2757 - val_accuracy: 0.7800 - val_loss: 0.6038 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9039 - loss: 0.2568\n",
      "Epoch 50: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9040 - loss: 0.2567 - val_accuracy: 0.7637 - val_loss: 0.6283 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9850\n",
      "Validation Accuracy: 0.8004\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.8004\n",
      "Weighted F1: 0.7995\n",
      "Macro F1: 0.7943\n",
      "Weighted F2: 0.7991\n",
      "Macro F2: 0.7922\n",
      "Precision: 0.8089\n",
      "Recall: 0.8004\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.8933\n",
      "calm: 0.8800\n",
      "disgust: 0.8462\n",
      "fearful: 0.8400\n",
      "happy: 0.7733\n",
      "neutral: 0.8158\n",
      "sad: 0.6800\n",
      "surprised: 0.6154\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.87      0.89      0.88        75\n",
      "        calm       0.79      0.88      0.83        75\n",
      "     disgust       0.77      0.85      0.80        39\n",
      "     fearful       0.80      0.84      0.82        75\n",
      "       happy       0.87      0.77      0.82        75\n",
      "     neutral       0.69      0.82      0.75        38\n",
      "         sad       0.71      0.68      0.69        75\n",
      "   surprised       1.00      0.62      0.76        39\n",
      "\n",
      "    accuracy                           0.80       491\n",
      "   macro avg       0.81      0.79      0.79       491\n",
      "weighted avg       0.81      0.80      0.80       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250623_195350.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba4dbf86-2f33-4f86-9341-4beb2e1a2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                    Dense, Dropout, Input, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43014445-8def-4fce-835d-ea6970bd977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Original architecture that gave 80% validation accuracy\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Keep EXACTLY your original layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "     # Specialized Feature Branches\n",
    "    x_sad = Conv2D(64, (5,5), activation='relu', name='sad_conv')(x)\n",
    "    x_sad = GlobalMaxPooling2D()(x_sad)\n",
    "    \n",
    "    x_surprised = Conv2D(64, (3,7), activation='relu', name='surprised_conv')(x)\n",
    "    x_surprised = GlobalMaxPooling2D()(x_surprised)\n",
    "    \n",
    "    # Combine features\n",
    "    merged = Concatenate()([x, x_sad, x_surprised])\n",
    "    \n",
    "    # Main output\n",
    "    main_output = Dense(num_classes, activation='softmax', name='main')(merged)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=main_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6100aadb-8bc6-45b1-94c5-f6c0da884172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Enhanced feature extraction with emotion-specific optimizations\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        # Audio loading with resampling protection\n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "            if sr != sample_rate:\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {str(e)}\")\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        # Handle empty/short audio\n",
    "        if len(audio_data) < 0.5 * sample_rate:  # Less than 0.5s of audio\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        # Pad/trim to exact duration\n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        audio_data = librosa.util.fix_length(audio_data, target_samples)\n",
    "        \n",
    "        # Feature extraction parameters\n",
    "        window_size = 2048 if sample_rate > 16000 else 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = MEL_BANDS\n",
    "        \n",
    "        # Channel 1: Enhanced Mel-spectrogram\n",
    "        try:\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sample_rate, \n",
    "                n_mels=mel_bands, \n",
    "                n_fft=window_size, \n",
    "                hop_length=frame_shift,\n",
    "                fmin=50,  # Focus on vocal range\n",
    "                fmax=8000 # Reduce high-frequency noise\n",
    "            )\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (mel_bands, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((mel_bands, AUDIO_DURATION))\n",
    "        \n",
    "        # Channel 2: Emotion-optimized features\n",
    "        try:\n",
    "            # 1. Emotion-specific MFCC configuration\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=audio_data, sr=sample_rate, \n",
    "                n_mfcc=13,  # Keep all coefficients\n",
    "                n_fft=window_size,\n",
    "                hop_length=frame_shift,\n",
    "                lifter=2  # Emphasize spectral details\n",
    "            )\n",
    "            \n",
    "            # 2. Chroma with tuning for emotional speech\n",
    "            chroma = librosa.feature.chroma_stft(\n",
    "                y=audio_data, sr=sample_rate,\n",
    "                n_chroma=12,\n",
    "                hop_length=frame_shift,\n",
    "                tuning=0  # Important for emotional pitch accuracy\n",
    "            )\n",
    "            \n",
    "            # 3. Delta features only for MFCCs\n",
    "            mfcc_delta = librosa.feature.delta(mfcc, width=5)  # Wider window for smoother deltas\n",
    "            \n",
    "            # 4. Spectral contrast with emotion focus\n",
    "            contrast = librosa.feature.spectral_contrast(\n",
    "                y=audio_data, sr=sample_rate,\n",
    "                n_bands=6,\n",
    "                hop_length=frame_shift,\n",
    "                fmin=100  # Focus on vocal frequencies\n",
    "            )\n",
    "            \n",
    "            # Stack features with priority ordering\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc[:10],       # First 10 MFCCs (most important)\n",
    "                mfcc_delta[:10], # Their deltas\n",
    "                chroma,          # All 12 chroma bins\n",
    "                contrast[:4]     # Top 4 contrast bands\n",
    "            ])\n",
    "            \n",
    "            # Ensure exact dimensions\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (mel_bands, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception as e:\n",
    "            print(f\"Feature error in {audio_path}: {str(e)}\")\n",
    "            channel_2 = np.zeros((mel_bands, AUDIO_DURATION))\n",
    "        \n",
    "        return np.stack([channel_1, channel_2], axis=-1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error with {audio_path}: {str(e)}\")\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bb2f0c1-be5a-4855-a91b-1e04127191bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def fix_audio_length(y, sr, target_duration):\n",
    "    \"\"\"Fix audio length to target duration\"\"\"\n",
    "    target_length = int(target_duration * sr)\n",
    "    if len(y) > target_length:\n",
    "        y = y[:target_length]\n",
    "    elif len(y) < target_length:\n",
    "        y = np.pad(y, (0, max(0, target_length - len(y))), mode='constant')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c236cc62-acb8-4624-a899-d27e7b06caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set, encoder=None):\n",
    "    \"\"\"Balanced augmentation for all classes with extra focus on sad/surprised\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "    \n",
    "    # Ensure we only work with labels known to encoder\n",
    "    if encoder is not None:\n",
    "        valid_mask = np.isin(label_set, encoder.classes_)\n",
    "        label_set = label_set[valid_mask]\n",
    "        feature_set = feature_set[valid_mask]\n",
    "        if len(feature_set) == 0:\n",
    "            return feature_set, label_set\n",
    "    \n",
    "    # Ensure labels are numerical\n",
    "    if isinstance(label_set[0], str) and encoder is not None:\n",
    "        try:\n",
    "            label_set = encoder.transform(label_set)\n",
    "        except ValueError as e:\n",
    "            print(f\"Label conversion warning in augmentation: {e}\")\n",
    "            # Filter out invalid labels\n",
    "            valid_indices = [i for i, lbl in enumerate(label_set) if lbl in encoder.classes_]\n",
    "            feature_set = feature_set[valid_indices]\n",
    "            label_set = label_set[valid_indices]\n",
    "    \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    # Handle both cases: with and without encoder\n",
    "    if encoder is not None:\n",
    "        try:\n",
    "            class_names = encoder.inverse_transform(unique_labels)\n",
    "            print(\"Class distribution before augmentation:\")\n",
    "            print(dict(zip(class_names, label_counts)))\n",
    "            # Create a mapping from label to class name\n",
    "            label_to_name = {label: name for label, name in zip(unique_labels, class_names)}\n",
    "        except:\n",
    "            print(\"Warning: Could not use encoder - falling back to numeric labels\")\n",
    "            label_to_name = {label: str(label) for label in unique_labels}\n",
    "    else:\n",
    "        label_to_name = {label: str(label) for label in unique_labels}\n",
    "    \n",
    "    # Standard augmentation for all classes\n",
    "    standard_target = int(median_count * 1.0)  # Bring all classes to median\n",
    "    \n",
    "    # Extra augmentation for sad/surprised (only if encoder is available and fitted)\n",
    "    extra_targets = {}\n",
    "    if encoder is not None:\n",
    "        try:\n",
    "            class_names_list = encoder.inverse_transform(unique_labels)\n",
    "            if 'sad' in class_names_list:\n",
    "                extra_targets['sad'] = int(median_count * 1.3)  # 30% more\n",
    "            if 'surprised' in class_names_list:\n",
    "                extra_targets['surprised'] = int(median_count * 1.5)  # 50% more\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Calculate needed samples\n",
    "    augmentation_plan = []\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        if encoder is not None and label not in encoder.classes_:\n",
    "            continue  # Skip invalid labels\n",
    "            \n",
    "        base_needed = max(0, standard_target - count)\n",
    "        class_name = label_to_name[label]\n",
    "        extra_needed = max(0, extra_targets.get(class_name, 0) - count) if class_name in extra_targets else 0\n",
    "        augmentation_plan.append((label, count, base_needed, extra_needed, class_name))\n",
    "    \n",
    "    total_new_samples = sum(plan[2] + plan[3] for plan in augmentation_plan)\n",
    "    \n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "\n",
    "    # Augmentation configurations (unchanged)\n",
    "    augmentation_strategies = {\n",
    "        'default': {\n",
    "            'types': ['add_noise', 'time_warp'],\n",
    "            'params': {\n",
    "                'add_noise': {'std': 0.01},\n",
    "                'time_warp': {'mask_size': (5, 10)}\n",
    "            }\n",
    "        },\n",
    "        'sad': {\n",
    "            'types': ['add_noise', 'time_warp', 'frequency_mask'],\n",
    "            'params': {\n",
    "                'add_noise': {'std': 0.02},\n",
    "                'time_warp': {'mask_size': (5, 15)},\n",
    "                'frequency_mask': {'mask_size': (3, 7)}\n",
    "            }\n",
    "        },\n",
    "        'surprised': {\n",
    "            'types': ['time_warp', 'frequency_mask', 'amplitude_scale'],\n",
    "            'params': {\n",
    "                'time_warp': {'mask_size': (10, 20)},\n",
    "                'frequency_mask': {'mask_size': (5, 10)},\n",
    "                'amplitude_scale': {'scale': (0.7, 1.3)}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for label, count, base_needed, extra_needed, name in augmentation_plan:\n",
    "        class_samples = np.where(label_set == label)[0]\n",
    "        total_needed = base_needed + extra_needed\n",
    "        \n",
    "        if total_needed == 0:\n",
    "            continue\n",
    "            \n",
    "        # Select strategy based on class\n",
    "        strategy = augmentation_strategies.get(name, augmentation_strategies['default'])\n",
    "        \n",
    "        for _ in range(total_needed):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            # Apply augmentation (unchanged)\n",
    "            aug_type = np.random.choice(strategy['types'])\n",
    "            params = strategy['params'][aug_type]\n",
    "            \n",
    "            modified_sample = np.copy(sample)\n",
    "            if aug_type == 'add_noise':\n",
    "                modified_sample += np.random.normal(0, params['std'], sample.shape)\n",
    "            elif aug_type == 'time_warp':\n",
    "                mask_size = np.random.randint(*params['mask_size'])\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif aug_type == 'frequency_mask':\n",
    "                mask_size = np.random.randint(*params['mask_size'])\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            elif aug_type == 'amplitude_scale':\n",
    "                scale = np.random.uniform(*params['scale'])\n",
    "                modified_sample *= scale\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = label\n",
    "            current_position += 1\n",
    "    \n",
    "    # Combine with original data\n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    # Final validation check\n",
    "    if encoder is not None:\n",
    "        invalid = set(combined_labels) - set(encoder.classes_)\n",
    "        if invalid:\n",
    "            raise ValueError(f\"Augmentation created invalid labels: {invalid}. \"\n",
    "                          f\"Encoder only knows: {encoder.classes_}\")\n",
    "    \n",
    "    # Verify results \n",
    "    final_counts = np.bincount(combined_labels)\n",
    "    unique_labels = np.unique(combined_labels)\n",
    "    \n",
    "    if encoder is not None:\n",
    "        try:\n",
    "            final_class_names = encoder.inverse_transform(unique_labels)\n",
    "            print(\"\\nClass distribution after augmentation:\")\n",
    "            print(dict(zip(final_class_names, final_counts[unique_labels])))\n",
    "        except:\n",
    "            print(\"\\nClass distribution after augmentation (numeric labels):\")\n",
    "            print(dict(zip(unique_labels, final_counts[unique_labels])))\n",
    "    else:\n",
    "        print(\"\\nClass distribution after augmentation (numeric labels):\")\n",
    "        print(dict(zip(unique_labels, final_counts[unique_labels])))\n",
    "    \n",
    "    print(f\"Total samples: {len(combined_features)}\")\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09be6e6e-5e95-4c8c-90f3-4178c559445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache', encoder=None, allowed_labels=None):\n",
    "    \"\"\"Feature processing pipeline with caching and label control\"\"\"\n",
    "    # Apply allowed_labels filter FIRST\n",
    "    if allowed_labels is not None:\n",
    "        dataframe = dataframe[dataframe['emotion'].isin(allowed_labels)]\n",
    "        print(f\"Filtered to {len(dataframe)} samples with allowed labels\")\n",
    "\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    # Update cache key to include allowed_labels\n",
    "    allowed_key = ''.join(sorted(allowed_labels)) if allowed_labels else 'all'\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}{allowed_key}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    # Convert labels to numerical if encoder is provided\n",
    "    if encoder is not None:\n",
    "        # Validate encoder has all needed classes\n",
    "        if allowed_labels is not None:\n",
    "            missing = set(allowed_labels) - set(encoder.classes_)\n",
    "            if missing:\n",
    "                raise ValueError(f\"Encoder missing classes: {missing}. \"\n",
    "                               f\"Encoder knows: {encoder.classes_}\")\n",
    "        \n",
    "        try:\n",
    "            base_labels = encoder.transform(base_labels)\n",
    "        except ValueError as e:\n",
    "            print(f\"Label conversion warning: {e}\")\n",
    "            # Filter out any labels not in encoder.classes_\n",
    "            valid_indices = [i for i, lbl in enumerate(base_labels) if lbl in encoder.classes_]\n",
    "            base_features = base_features[valid_indices]\n",
    "            base_labels = base_labels[valid_indices]\n",
    "            print(f\"Filtered to {len(base_features)} valid samples\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(\n",
    "            base_features, \n",
    "            base_labels,\n",
    "            encoder=encoder\n",
    "        )\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4f2eac6-3490-4f5d-b2c7-62643e3e5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Enhanced feature extraction with emotion-specific optimizations\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        y = fix_audio_length(y, sr, target_duration=clip_length)\n",
    "        \n",
    "        # Feature extraction parameters\n",
    "        window_size = 2048 if sample_rate > 16000 else 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = MEL_BANDS\n",
    "        \n",
    "        # Channel 1: Enhanced Mel-spectrogram\n",
    "        try:\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=y, sr=sample_rate, \n",
    "                n_mels=mel_bands, \n",
    "                n_fft=window_size, \n",
    "                hop_length=frame_shift,\n",
    "                fmin=50,  # Focus on vocal range\n",
    "                fmax=8000 # Reduce high-frequency noise\n",
    "            )\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (mel_bands, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((mel_bands, AUDIO_DURATION))\n",
    "        \n",
    "        # Channel 2: Emotion-optimized features\n",
    "        try:\n",
    "            # 1. Emotion-specific MFCC configuration\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=y, sr=sample_rate, \n",
    "                n_mfcc=13,  # Keep all coefficients\n",
    "                n_fft=window_size,\n",
    "                hop_length=frame_shift,\n",
    "                lifter=2  # Emphasize spectral details\n",
    "            )\n",
    "            \n",
    "            # 2. Chroma with tuning for emotional speech\n",
    "            chroma = librosa.feature.chroma_stft(\n",
    "                y=y, sr=sample_rate,\n",
    "                n_chroma=12,\n",
    "                hop_length=frame_shift,\n",
    "                tuning=0  # Important for emotional pitch accuracy\n",
    "            )\n",
    "            \n",
    "            # 3. Delta features only for MFCCs\n",
    "            mfcc_delta = librosa.feature.delta(mfcc, width=5)  # Wider window for smoother deltas\n",
    "            \n",
    "            # 4. Spectral contrast with emotion focus\n",
    "            contrast = librosa.feature.spectral_contrast(\n",
    "                y=y, sr=sample_rate,\n",
    "                n_bands=6,\n",
    "                hop_length=frame_shift,\n",
    "                fmin=100  # Focus on vocal frequencies\n",
    "            )\n",
    "            \n",
    "            # Stack features with priority ordering\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc[:10],       # First 10 MFCCs (most important)\n",
    "                mfcc_delta[:10], # Their deltas\n",
    "                chroma,          # All 12 chroma bins\n",
    "                contrast[:4]     # Top 4 contrast bands\n",
    "            ])\n",
    "            \n",
    "            # Ensure exact dimensions\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (mel_bands, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception as e:\n",
    "            print(f\"Feature error in {audio_path}: {str(e)}\")\n",
    "            channel_2 = np.zeros((mel_bands, AUDIO_DURATION))\n",
    "        \n",
    "        return np.stack([channel_1, channel_2], axis=-1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error with {audio_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75226bad-3f0f-47a7-aa94-836dc966da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Filtered to 1961 samples with allowed labels\n",
      "Processing 1961 audio files...\n",
      "Saved features to cache: feature_cache\\d31407beba8be99f2cd5617f8299d078.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Augmented dataset size: 0\n",
      "\n",
      "=== Processing validation features ===\n",
      "Processing 491 audio files...\n",
      "Saved features to cache: feature_cache\\249e6313409abc3745b96a393f399df4.npz\n",
      "Initial feature count: 491\n",
      "Feature extraction failed. Terminating.\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    # Configuration Constants\n",
    "    # MODEL_ID = \"v6\"\n",
    "    # MODEL_TAG = \"enhanced\" \n",
    "    # RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    # Get all unique emotion labels from complete dataset\n",
    "    all_emotions = audio_dataset['emotion'].unique()\n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {all_emotions}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Initialize and fit label encoder on ALL possible labels\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(all_emotions)  # Fit on ALL possible emotions\n",
    "    \n",
    "    # Split data (using original string labels)\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Process training features with strict label checking\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(\n",
    "            train_data, \n",
    "            enable_augmentation=True,\n",
    "            encoder=encoder,\n",
    "            allowed_labels=set(encoder.classes_)  # Pass allowed labels\n",
    "        )\n",
    "        \n",
    "        # Verify no new labels were introduced\n",
    "        unique_labels = np.unique(y_train_raw)\n",
    "        for label in unique_labels:\n",
    "            if label not in encoder.classes_:\n",
    "                raise ValueError(f\"Augmentation introduced invalid label: {label}\")\n",
    "        \n",
    "        y_train = encoder.transform(y_train_raw)\n",
    "        \n",
    "        # Process validation features\n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(\n",
    "            val_data, \n",
    "            enable_augmentation=False\n",
    "        )\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        # Convert to categorical\n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Dynamic class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            weighted_metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=get_enhanced_callbacks(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # [Rest of evaluation code...]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "138418ba-3156-4fd9-b0c9-5ce128648d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique labels before augmentation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(base_labels))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoder\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_labels' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels before augmentation:\", np.unique(base_labels))\n",
    "print(\"Encoder classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "072751fc-b9fe-4634-b044-9998a99aa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                    Dense, Dropout, Input, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52ffa48d-25e9-4971-bec9-e7c2c0bc024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUDIO_DURATION = 94  # ~3 seconds with hop_length=512\n",
    "MEL_BANDS = 32  \n",
    "FEATURE_DIMENSIONS = 2\n",
    "MODEL_ID = \"v6\"  \n",
    "MODEL_TAG = \"enhanced\"  \n",
    "RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "018f39cb-49c9-44ce-9ed3-dcc6bb3704c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_array_dimensions(data_array, desired_shape):\n",
    "    \"\"\"Resize array to match target dimensions by padding or trimming\"\"\"\n",
    "    if data_array.size == 0:\n",
    "        return np.zeros(desired_shape)\n",
    "    \n",
    "    current_dims = data_array.shape\n",
    "    \n",
    "    # Process time axis\n",
    "    if len(current_dims) >= 2:\n",
    "        if current_dims[1] > desired_shape[1]:\n",
    "            data_array = data_array[:, :desired_shape[1]]\n",
    "        elif current_dims[1] < desired_shape[1]:\n",
    "            padding = [(0, 0), (0, desired_shape[1] - current_dims[1])]\n",
    "            data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    # Process frequency axis\n",
    "    if current_dims[0] > desired_shape[0]:\n",
    "        data_array = data_array[:desired_shape[0], :]\n",
    "    elif current_dims[0] < desired_shape[0]:\n",
    "        padding = [(0, desired_shape[0] - current_dims[0]), (0, 0)]\n",
    "        data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def standardize_features(feature_matrix):\n",
    "    \"\"\"Normalize feature values with robust handling\"\"\"\n",
    "    if feature_matrix.size == 0:\n",
    "        return feature_matrix\n",
    "    \n",
    "    mean_val = np.mean(feature_matrix)\n",
    "    std_val = np.std(feature_matrix)\n",
    "    \n",
    "    if std_val == 0 or np.isnan(std_val):\n",
    "        return feature_matrix - mean_val\n",
    "    \n",
    "    return (feature_matrix - mean_val) / std_val\n",
    "\n",
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Feature extraction pipeline with consistent output dimensions\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = 32\n",
    "        \n",
    "        try:\n",
    "            # First channel: Mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=mel_bands, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        try:\n",
    "            # Second channel: Combined audio features\n",
    "            mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, \n",
    "                                               n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma_features = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            channel_2 = np.vstack([mfcc_features, chroma_features, spectral_contrast])\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        feature_stack = np.stack([channel_1, channel_2], axis=-1)\n",
    "        \n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b330bc96-bea1-4846-b9f7-796729d844e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Data augmentation for underrepresented classes\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = np.where(label_set == class_id)[0]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            augmentation = np.random.choice(['add_noise', 'time_warp', 'frequency_mask', 'amplitude_scale'])\n",
    "            \n",
    "            if augmentation == 'add_noise':\n",
    "                modified_sample = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif augmentation == 'time_warp':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif augmentation == 'frequency_mask':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            else:\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = class_id\n",
    "            current_position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "725fbd59-5e41-42b0-b600-2232c0d029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Neural network architecture for emotion classification\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8509e45b-05c3-408f-a153-5b4b1fb13da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "667122bd-d20e-42ea-a4e9-42c096ddca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache'):\n",
    "    \"\"\"Feature processing pipeline with caching\"\"\"\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(base_features, base_labels)\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "380fdc07-bf0b-4ff0-9d07-ce44522c675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_audio_dataset_from_two_dirs(speech_dir, song_dir):\n",
    "    \"\"\"Create dataset from audio files in two folders (speech & song) with emotion labels\"\"\"\n",
    "    emotion_codes = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def process_dir(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_codes:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_codes[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Process both folders\n",
    "    process_dir(speech_dir)\n",
    "    process_dir(song_dir)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No valid emotion-labeled files found. Using placeholder labels.\")\n",
    "        all_audio_files = glob.glob(os.path.join(speech_dir, \"*.wav\")) + glob.glob(os.path.join(song_dir, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_audio_files))])\n",
    "        paths.extend(all_audio_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20fc1b51-faec-458a-93ff-3300a8469394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(model_file, audio_file, encoder_file):\n",
    "    \"\"\"Predict emotion from audio using trained model\"\"\"\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    with open(encoder_file, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    features = extract_audio_characteristics(audio_file)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_index]\n",
    "    \n",
    "    emotion_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return emotion_label, confidence\n",
    "\n",
    "def record_training_performance(training_history, model_identifier, train_accuracy, val_accuracy,\n",
    "                              overall_acc, f1_macro, f1_weighted,\n",
    "                              f2_macro, f2_weighted, avg_precision, avg_recall,\n",
    "                              per_class_acc, classification_report):\n",
    "    \"\"\"Save comprehensive training metrics\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_identifier}_performance_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Performance Report for {model_identifier}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model Version: {MODEL_ID}\\n\")\n",
    "        f.write(f\"Model Variant: {MODEL_TAG}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY METRICS:\\n\")\n",
    "        f.write(f\"Training Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F1: {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F2: {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F2: {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "        for class_name, accuracy in per_class_acc.items():\n",
    "            f.write(f\"{class_name}: {accuracy:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(classification_report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Performance report saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bd1d6b7-47e2-42c4-b8f6-1eeb0f567b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1847 - loss: 2.1092\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16701, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.1853 - loss: 2.1077 - val_accuracy: 0.1670 - val_loss: 2.1271 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3204 - loss: 1.7541\n",
      "Epoch 2: val_accuracy did not improve from 0.16701\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.3207 - loss: 1.7534 - val_accuracy: 0.1527 - val_loss: 2.3002 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3797 - loss: 1.6050\n",
      "Epoch 3: val_accuracy did not improve from 0.16701\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.3798 - loss: 1.6041 - val_accuracy: 0.1527 - val_loss: 3.2655 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4253 - loss: 1.4957\n",
      "Epoch 4: val_accuracy did not improve from 0.16701\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.4254 - loss: 1.4953 - val_accuracy: 0.1568 - val_loss: 2.8474 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4547 - loss: 1.4241\n",
      "Epoch 5: val_accuracy did not improve from 0.16701\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.4549 - loss: 1.4236 - val_accuracy: 0.1527 - val_loss: 3.6956 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4726 - loss: 1.3415\n",
      "Epoch 6: val_accuracy improved from 0.16701 to 0.32587, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.4732 - loss: 1.3408 - val_accuracy: 0.3259 - val_loss: 1.8665 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5350 - loss: 1.2401\n",
      "Epoch 7: val_accuracy did not improve from 0.32587\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.5351 - loss: 1.2394 - val_accuracy: 0.2464 - val_loss: 2.5207 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5452 - loss: 1.1990\n",
      "Epoch 8: val_accuracy improved from 0.32587 to 0.52546, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.5455 - loss: 1.1986 - val_accuracy: 0.5255 - val_loss: 1.2980 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5733 - loss: 1.1254\n",
      "Epoch 9: val_accuracy did not improve from 0.52546\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.5735 - loss: 1.1252 - val_accuracy: 0.5112 - val_loss: 1.2620 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6197 - loss: 1.0741\n",
      "Epoch 10: val_accuracy did not improve from 0.52546\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.6194 - loss: 1.0742 - val_accuracy: 0.1955 - val_loss: 3.0362 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.6527 - loss: 0.9687\n",
      "Epoch 11: val_accuracy did not improve from 0.52546\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.6524 - loss: 0.9692 - val_accuracy: 0.4827 - val_loss: 1.3393 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6506 - loss: 0.9662\n",
      "Epoch 12: val_accuracy did not improve from 0.52546\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6508 - loss: 0.9660 - val_accuracy: 0.4664 - val_loss: 1.3839 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6491 - loss: 0.9281\n",
      "Epoch 13: val_accuracy improved from 0.52546 to 0.53971, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6493 - loss: 0.9277 - val_accuracy: 0.5397 - val_loss: 1.1555 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7047 - loss: 0.8214\n",
      "Epoch 14: val_accuracy did not improve from 0.53971\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.7043 - loss: 0.8219 - val_accuracy: 0.4012 - val_loss: 1.7409 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7047 - loss: 0.8189\n",
      "Epoch 15: val_accuracy improved from 0.53971 to 0.63136, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7047 - loss: 0.8188 - val_accuracy: 0.6314 - val_loss: 1.0230 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7203 - loss: 0.7848\n",
      "Epoch 16: val_accuracy did not improve from 0.63136\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.7202 - loss: 0.7852 - val_accuracy: 0.5947 - val_loss: 1.0231 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7220 - loss: 0.7811\n",
      "Epoch 17: val_accuracy did not improve from 0.63136\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7220 - loss: 0.7811 - val_accuracy: 0.6273 - val_loss: 1.0634 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7497 - loss: 0.7167\n",
      "Epoch 18: val_accuracy did not improve from 0.63136\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.7494 - loss: 0.7174 - val_accuracy: 0.5866 - val_loss: 1.1065 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7531 - loss: 0.7243\n",
      "Epoch 19: val_accuracy did not improve from 0.63136\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.7532 - loss: 0.7233 - val_accuracy: 0.4766 - val_loss: 1.9559 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7598 - loss: 0.6586\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.63136\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.7596 - loss: 0.6590 - val_accuracy: 0.5397 - val_loss: 1.1884 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7998 - loss: 0.5696\n",
      "Epoch 21: val_accuracy improved from 0.63136 to 0.63340, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.7998 - loss: 0.5699 - val_accuracy: 0.6334 - val_loss: 1.0633 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8136 - loss: 0.5434\n",
      "Epoch 22: val_accuracy improved from 0.63340 to 0.68839, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8137 - loss: 0.5429 - val_accuracy: 0.6884 - val_loss: 0.8301 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8178 - loss: 0.5123\n",
      "Epoch 23: val_accuracy improved from 0.68839 to 0.73320, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.8178 - loss: 0.5124 - val_accuracy: 0.7332 - val_loss: 0.7238 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8308 - loss: 0.4668\n",
      "Epoch 24: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 68ms/step - accuracy: 0.8306 - loss: 0.4672 - val_accuracy: 0.6273 - val_loss: 1.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8322 - loss: 0.5051\n",
      "Epoch 25: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8322 - loss: 0.5047 - val_accuracy: 0.6884 - val_loss: 0.8215 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8254 - loss: 0.5031\n",
      "Epoch 26: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.8253 - loss: 0.5028 - val_accuracy: 0.6558 - val_loss: 0.8929 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8565 - loss: 0.4399\n",
      "Epoch 27: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8565 - loss: 0.4397 - val_accuracy: 0.7026 - val_loss: 0.8035 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8549 - loss: 0.4113\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.8550 - loss: 0.4114 - val_accuracy: 0.6965 - val_loss: 0.7931 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8553 - loss: 0.3885\n",
      "Epoch 29: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.8553 - loss: 0.3887 - val_accuracy: 0.6965 - val_loss: 0.7747 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8687 - loss: 0.3695\n",
      "Epoch 30: val_accuracy did not improve from 0.73320\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.8688 - loss: 0.3695 - val_accuracy: 0.6884 - val_loss: 0.8138 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8841 - loss: 0.3559\n",
      "Epoch 31: val_accuracy improved from 0.73320 to 0.73931, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8839 - loss: 0.3559 - val_accuracy: 0.7393 - val_loss: 0.6690 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8830 - loss: 0.3485\n",
      "Epoch 32: val_accuracy did not improve from 0.73931\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.8828 - loss: 0.3490 - val_accuracy: 0.6925 - val_loss: 0.8008 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8722 - loss: 0.3331\n",
      "Epoch 33: val_accuracy improved from 0.73931 to 0.76986, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8720 - loss: 0.3340 - val_accuracy: 0.7699 - val_loss: 0.6430 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8763 - loss: 0.3461\n",
      "Epoch 34: val_accuracy improved from 0.76986 to 0.77393, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8763 - loss: 0.3460 - val_accuracy: 0.7739 - val_loss: 0.6474 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8867 - loss: 0.3345\n",
      "Epoch 35: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.8866 - loss: 0.3344 - val_accuracy: 0.7495 - val_loss: 0.6472 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8928 - loss: 0.3244\n",
      "Epoch 36: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.8928 - loss: 0.3241 - val_accuracy: 0.7434 - val_loss: 0.6530 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9090 - loss: 0.2829\n",
      "Epoch 37: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9086 - loss: 0.2836 - val_accuracy: 0.7556 - val_loss: 0.6369 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8935 - loss: 0.3067\n",
      "Epoch 38: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 75ms/step - accuracy: 0.8935 - loss: 0.3069 - val_accuracy: 0.7576 - val_loss: 0.6557 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8926 - loss: 0.3110\n",
      "Epoch 39: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.8927 - loss: 0.3109 - val_accuracy: 0.7576 - val_loss: 0.6729 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9017 - loss: 0.2972\n",
      "Epoch 40: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.9016 - loss: 0.2971 - val_accuracy: 0.6701 - val_loss: 0.8838 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9011 - loss: 0.2907\n",
      "Epoch 41: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - accuracy: 0.9012 - loss: 0.2905 - val_accuracy: 0.7413 - val_loss: 0.7339 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9101 - loss: 0.2808\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.77393\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9100 - loss: 0.2811 - val_accuracy: 0.7067 - val_loss: 0.7611 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9164 - loss: 0.2458\n",
      "Epoch 43: val_accuracy improved from 0.77393 to 0.79226, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.9163 - loss: 0.2462 - val_accuracy: 0.7923 - val_loss: 0.5870 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9101 - loss: 0.2592\n",
      "Epoch 44: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.9101 - loss: 0.2593 - val_accuracy: 0.7739 - val_loss: 0.6201 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9183 - loss: 0.2402\n",
      "Epoch 45: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9180 - loss: 0.2406 - val_accuracy: 0.7597 - val_loss: 0.6402 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8853 - loss: 0.3011\n",
      "Epoch 46: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.8856 - loss: 0.3007 - val_accuracy: 0.7251 - val_loss: 0.7394 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9135 - loss: 0.2586\n",
      "Epoch 47: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9134 - loss: 0.2585 - val_accuracy: 0.7576 - val_loss: 0.6550 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9242 - loss: 0.2250\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.9240 - loss: 0.2255 - val_accuracy: 0.7189 - val_loss: 0.7352 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9185 - loss: 0.2289\n",
      "Epoch 49: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.9186 - loss: 0.2290 - val_accuracy: 0.7760 - val_loss: 0.6109 - learning_rate: 6.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9170 - loss: 0.2478 \n",
      "Epoch 50: val_accuracy did not improve from 0.79226\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 100ms/step - accuracy: 0.9172 - loss: 0.2475 - val_accuracy: 0.7841 - val_loss: 0.6146 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9821\n",
      "Validation Accuracy: 0.7923\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.7923\n",
      "Weighted F1: 0.7907\n",
      "Macro F1: 0.7870\n",
      "Weighted F2: 0.7906\n",
      "Macro F2: 0.7883\n",
      "Precision: 0.7978\n",
      "Recall: 0.7923\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.8667\n",
      "calm: 0.8267\n",
      "disgust: 0.7949\n",
      "fearful: 0.8533\n",
      "happy: 0.8000\n",
      "neutral: 0.8947\n",
      "sad: 0.6267\n",
      "surprised: 0.6667\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.84      0.87      0.86        75\n",
      "        calm       0.86      0.83      0.84        75\n",
      "     disgust       0.79      0.79      0.79        39\n",
      "     fearful       0.73      0.85      0.79        75\n",
      "       happy       0.80      0.80      0.80        75\n",
      "     neutral       0.68      0.89      0.77        38\n",
      "         sad       0.80      0.63      0.70        75\n",
      "   surprised       0.84      0.67      0.74        39\n",
      "\n",
      "    accuracy                           0.79       491\n",
      "   macro avg       0.79      0.79      0.79       491\n",
      "weighted avg       0.80      0.79      0.79       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250624_123532.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4e159-df90-440c-99c4-c8e9249928ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  i think i got the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443d4239-875f-4673-91ef-5cc1b7b472cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, \n",
    "                                    Dense, Dropout, Input, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd1fa8a-1913-45ff-a40b-41cb88470e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUDIO_DURATION = 94  # ~3 seconds with hop_length=512\n",
    "MEL_BANDS = 32  \n",
    "FEATURE_DIMENSIONS = 2\n",
    "MODEL_ID = \"v6\"  \n",
    "MODEL_TAG = \"enhanced\"  \n",
    "RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b660d-6f6a-4f9d-9ca3-0bcbab1034bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_array_dimensions(data_array, desired_shape):\n",
    "    \"\"\"Resize array to match target dimensions by padding or trimming\"\"\"\n",
    "    if data_array.size == 0:\n",
    "        return np.zeros(desired_shape)\n",
    "    \n",
    "    current_dims = data_array.shape\n",
    "    \n",
    "    # Process time axis\n",
    "    if len(current_dims) >= 2:\n",
    "        if current_dims[1] > desired_shape[1]:\n",
    "            data_array = data_array[:, :desired_shape[1]]\n",
    "        elif current_dims[1] < desired_shape[1]:\n",
    "            padding = [(0, 0), (0, desired_shape[1] - current_dims[1])]\n",
    "            data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    # Process frequency axis\n",
    "    if current_dims[0] > desired_shape[0]:\n",
    "        data_array = data_array[:desired_shape[0], :]\n",
    "    elif current_dims[0] < desired_shape[0]:\n",
    "        padding = [(0, desired_shape[0] - current_dims[0]), (0, 0)]\n",
    "        data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def standardize_features(feature_matrix):\n",
    "    \"\"\"Normalize feature values with robust handling\"\"\"\n",
    "    if feature_matrix.size == 0:\n",
    "        return feature_matrix\n",
    "    \n",
    "    mean_val = np.mean(feature_matrix)\n",
    "    std_val = np.std(feature_matrix)\n",
    "    \n",
    "    if std_val == 0 or np.isnan(std_val):\n",
    "        return feature_matrix - mean_val\n",
    "    \n",
    "    return (feature_matrix - mean_val) / std_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600f8338-7209-4103-bad2-eca7cc38b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache'):\n",
    "    \"\"\"Feature processing pipeline with caching\"\"\"\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(base_features, base_labels)\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc245af-ef21-4761-91e8-b3d8ae82faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Data augmentation for underrepresented classes\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = np.where(label_set == class_id)[0]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            augmentation = np.random.choice(['add_noise', 'time_warp', 'frequency_mask', 'amplitude_scale'])\n",
    "            \n",
    "            if augmentation == 'add_noise':\n",
    "                modified_sample = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif augmentation == 'time_warp':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif augmentation == 'frequency_mask':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            else:\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = class_id\n",
    "            current_position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c9e024-cbc3-4444-85d2-99c3f9bed9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(model_file, audio_file, encoder_file):\n",
    "    \"\"\"Predict emotion from audio using trained model\"\"\"\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    with open(encoder_file, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    features = extract_audio_characteristics(audio_file)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_index]\n",
    "    \n",
    "    emotion_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return emotion_label, confidence\n",
    "\n",
    "def record_training_performance(training_history, model_identifier, train_accuracy, val_accuracy,\n",
    "                              overall_acc, f1_macro, f1_weighted,\n",
    "                              f2_macro, f2_weighted, avg_precision, avg_recall,\n",
    "                              per_class_acc, classification_report):\n",
    "    \"\"\"Save comprehensive training metrics\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_identifier}_performance_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Performance Report for {model_identifier}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model Version: {MODEL_ID}\\n\")\n",
    "        f.write(f\"Model Variant: {MODEL_TAG}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY METRICS:\\n\")\n",
    "        f.write(f\"Training Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F1: {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F2: {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F2: {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "        for class_name, accuracy in per_class_acc.items():\n",
    "            f.write(f\"{class_name}: {accuracy:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(classification_report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Performance report saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bc422f-3344-4235-b7d8-265f4f3a214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_audio_dataset_from_two_dirs(speech_dir, song_dir):\n",
    "    \"\"\"Create dataset from audio files in two folders (speech & song) with emotion labels\"\"\"\n",
    "    emotion_codes = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def process_dir(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_codes:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_codes[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Process both folders\n",
    "    process_dir(speech_dir)\n",
    "    process_dir(song_dir)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No valid emotion-labeled files found. Using placeholder labels.\")\n",
    "        all_audio_files = glob.glob(os.path.join(speech_dir, \"*.wav\")) + glob.glob(os.path.join(song_dir, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_audio_files))])\n",
    "        paths.extend(all_audio_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e59c268-9c55-42af-bd2e-36184883bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Optimized feature extraction maintaining original performance\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        # Audio loading (unchanged)\n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        \n",
    "        # Channel 1: Mel-spectrogram (keep identical)\n",
    "        try:\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=MEL_BANDS, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        # Channel 2: Revised feature combination\n",
    "        try:\n",
    "            # 1. Basic features\n",
    "            mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=10,  # Reduced from 13\n",
    "                                      n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            # 2. Only use MFCC deltas (no other deltas)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            \n",
    "            # 3. Conservative combination\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc,          # 10 coefficients\n",
    "                mfcc_delta,    # 10 delta coefficients\n",
    "                chroma         # 12 chroma features\n",
    "            ])\n",
    "            \n",
    "            # Ensure exact dimensions (pad if needed)\n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        return np.stack([channel_1, channel_2], axis=-1)\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173ed5cd-5985-4fe7-a04b-fc9dcd80dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828c0459-706e-4d9c-9b76-8a211db24f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Original architecture that gave 80% validation accuracy\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Keep EXACTLY your original layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5545a4-a594-482a-a3e0-87366b708082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.1856 - loss: 2.1343 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.19756, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.1867 - loss: 2.1305 - val_accuracy: 0.1976 - val_loss: 2.1271 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.2881 - loss: 1.7857\n",
      "Epoch 2: val_accuracy did not improve from 0.19756\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.2885 - loss: 1.7850 - val_accuracy: 0.1527 - val_loss: 2.8981 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.3731 - loss: 1.5937 \n",
      "Epoch 3: val_accuracy did not improve from 0.19756\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.3734 - loss: 1.5933 - val_accuracy: 0.1527 - val_loss: 3.4580 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.4379 - loss: 1.4939\n",
      "Epoch 4: val_accuracy did not improve from 0.19756\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.4380 - loss: 1.4935 - val_accuracy: 0.1527 - val_loss: 3.9251 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.4585 - loss: 1.3849 \n",
      "Epoch 5: val_accuracy did not improve from 0.19756\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.4587 - loss: 1.3845 - val_accuracy: 0.1527 - val_loss: 4.2040 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.4850 - loss: 1.3174\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.19756 to 0.21996, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.4855 - loss: 1.3169 - val_accuracy: 0.2200 - val_loss: 2.4407 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5445 - loss: 1.1840 \n",
      "Epoch 7: val_accuracy improved from 0.21996 to 0.23422, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.5450 - loss: 1.1836 - val_accuracy: 0.2342 - val_loss: 2.2705 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.5925 - loss: 1.1047\n",
      "Epoch 8: val_accuracy improved from 0.23422 to 0.36660, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.5925 - loss: 1.1044 - val_accuracy: 0.3666 - val_loss: 1.6680 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6224 - loss: 1.0142 \n",
      "Epoch 9: val_accuracy improved from 0.36660 to 0.45010, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.6224 - loss: 1.0143 - val_accuracy: 0.4501 - val_loss: 1.3708 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.6319 - loss: 0.9979\n",
      "Epoch 10: val_accuracy improved from 0.45010 to 0.61507, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.6323 - loss: 0.9973 - val_accuracy: 0.6151 - val_loss: 1.0992 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6344 - loss: 0.9602 \n",
      "Epoch 11: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.6349 - loss: 0.9595 - val_accuracy: 0.5906 - val_loss: 1.1513 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6719 - loss: 0.9006\n",
      "Epoch 12: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - accuracy: 0.6724 - loss: 0.8999 - val_accuracy: 0.5051 - val_loss: 1.4557 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6952 - loss: 0.8289\n",
      "Epoch 13: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.6952 - loss: 0.8287 - val_accuracy: 0.6029 - val_loss: 1.0706 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7019 - loss: 0.8388\n",
      "Epoch 14: val_accuracy did not improve from 0.61507\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.7016 - loss: 0.8389 - val_accuracy: 0.6029 - val_loss: 1.0352 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7192 - loss: 0.7479\n",
      "Epoch 15: val_accuracy improved from 0.61507 to 0.64155, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.7191 - loss: 0.7479 - val_accuracy: 0.6415 - val_loss: 1.0075 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7576 - loss: 0.7007\n",
      "Epoch 16: val_accuracy did not improve from 0.64155\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.7574 - loss: 0.7010 - val_accuracy: 0.6375 - val_loss: 1.0161 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7490 - loss: 0.7099\n",
      "Epoch 17: val_accuracy did not improve from 0.64155\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.7491 - loss: 0.7100 - val_accuracy: 0.4745 - val_loss: 1.5598 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7525 - loss: 0.7088\n",
      "Epoch 18: val_accuracy did not improve from 0.64155\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.7528 - loss: 0.7075 - val_accuracy: 0.6395 - val_loss: 0.9704 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7821 - loss: 0.6229 \n",
      "Epoch 19: val_accuracy improved from 0.64155 to 0.69043, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.7819 - loss: 0.6239 - val_accuracy: 0.6904 - val_loss: 0.8855 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.7898 - loss: 0.6149\n",
      "Epoch 20: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.7897 - loss: 0.6151 - val_accuracy: 0.5947 - val_loss: 1.0896 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7802 - loss: 0.6154\n",
      "Epoch 21: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.7803 - loss: 0.6151 - val_accuracy: 0.5662 - val_loss: 1.2112 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7925 - loss: 0.5748\n",
      "Epoch 22: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.7925 - loss: 0.5750 - val_accuracy: 0.6660 - val_loss: 0.9154 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8022 - loss: 0.5609\n",
      "Epoch 23: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 100ms/step - accuracy: 0.8021 - loss: 0.5610 - val_accuracy: 0.6802 - val_loss: 0.8739 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7994 - loss: 0.5664\n",
      "Epoch 24: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 110ms/step - accuracy: 0.7997 - loss: 0.5656 - val_accuracy: 0.6538 - val_loss: 0.9099 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8165 - loss: 0.5206 \n",
      "Epoch 25: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.8164 - loss: 0.5206 - val_accuracy: 0.5092 - val_loss: 1.4675 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8320 - loss: 0.4716\n",
      "Epoch 26: val_accuracy did not improve from 0.69043\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.8317 - loss: 0.4722 - val_accuracy: 0.6619 - val_loss: 0.9993 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8321 - loss: 0.4731\n",
      "Epoch 27: val_accuracy improved from 0.69043 to 0.69654, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.8321 - loss: 0.4731 - val_accuracy: 0.6965 - val_loss: 0.7989 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8446 - loss: 0.4523\n",
      "Epoch 28: val_accuracy improved from 0.69654 to 0.72505, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 102ms/step - accuracy: 0.8443 - loss: 0.4531 - val_accuracy: 0.7251 - val_loss: 0.7610 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8450 - loss: 0.4443\n",
      "Epoch 29: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 102ms/step - accuracy: 0.8446 - loss: 0.4453 - val_accuracy: 0.6640 - val_loss: 0.8590 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8389 - loss: 0.4618 \n",
      "Epoch 30: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8390 - loss: 0.4614 - val_accuracy: 0.6049 - val_loss: 1.2346 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8515 - loss: 0.4282\n",
      "Epoch 31: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 102ms/step - accuracy: 0.8514 - loss: 0.4284 - val_accuracy: 0.6415 - val_loss: 0.9488 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8594 - loss: 0.3923 \n",
      "Epoch 32: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.8595 - loss: 0.3925 - val_accuracy: 0.6762 - val_loss: 0.8697 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8598 - loss: 0.3957\n",
      "Epoch 33: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 101ms/step - accuracy: 0.8599 - loss: 0.3953 - val_accuracy: 0.7149 - val_loss: 0.7076 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8641 - loss: 0.3783 \n",
      "Epoch 34: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.8642 - loss: 0.3787 - val_accuracy: 0.5723 - val_loss: 1.2904 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8767 - loss: 0.3454\n",
      "Epoch 35: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.8767 - loss: 0.3454 - val_accuracy: 0.6212 - val_loss: 1.1170 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8514 - loss: 0.4046 \n",
      "Epoch 36: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.8519 - loss: 0.4041 - val_accuracy: 0.6802 - val_loss: 0.8348 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8839 - loss: 0.3333\n",
      "Epoch 37: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.8839 - loss: 0.3335 - val_accuracy: 0.6395 - val_loss: 1.0056 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8653 - loss: 0.3562\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.72505\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.8653 - loss: 0.3567 - val_accuracy: 0.7251 - val_loss: 0.7168 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8772 - loss: 0.3412\n",
      "Epoch 39: val_accuracy improved from 0.72505 to 0.73931, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - accuracy: 0.8775 - loss: 0.3404 - val_accuracy: 0.7393 - val_loss: 0.6834 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8950 - loss: 0.2934\n",
      "Epoch 40: val_accuracy did not improve from 0.73931\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.8952 - loss: 0.2931 - val_accuracy: 0.7393 - val_loss: 0.6869 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9176 - loss: 0.2687\n",
      "Epoch 41: val_accuracy did not improve from 0.73931\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.9177 - loss: 0.2684 - val_accuracy: 0.7352 - val_loss: 0.7013 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9038 - loss: 0.2708 \n",
      "Epoch 42: val_accuracy did not improve from 0.73931\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 101ms/step - accuracy: 0.9038 - loss: 0.2709 - val_accuracy: 0.7352 - val_loss: 0.7563 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9251 - loss: 0.2233\n",
      "Epoch 43: val_accuracy did not improve from 0.73931\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.9249 - loss: 0.2240 - val_accuracy: 0.7332 - val_loss: 0.6463 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9158 - loss: 0.2508\n",
      "Epoch 44: val_accuracy improved from 0.73931 to 0.74338, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.9158 - loss: 0.2507 - val_accuracy: 0.7434 - val_loss: 0.6665 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9082 - loss: 0.2654\n",
      "Epoch 45: val_accuracy did not improve from 0.74338\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.9082 - loss: 0.2656 - val_accuracy: 0.6578 - val_loss: 0.9207 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9227 - loss: 0.2323 \n",
      "Epoch 46: val_accuracy did not improve from 0.74338\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.9227 - loss: 0.2321 - val_accuracy: 0.6965 - val_loss: 0.8305 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9205 - loss: 0.2171\n",
      "Epoch 47: val_accuracy did not improve from 0.74338\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - accuracy: 0.9205 - loss: 0.2173 - val_accuracy: 0.7312 - val_loss: 0.7659 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9246 - loss: 0.2241 \n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.74338 to 0.75356, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.9245 - loss: 0.2244 - val_accuracy: 0.7536 - val_loss: 0.6793 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9219 - loss: 0.2198\n",
      "Epoch 49: val_accuracy improved from 0.75356 to 0.76578, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.9219 - loss: 0.2199 - val_accuracy: 0.7658 - val_loss: 0.6411 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9444 - loss: 0.1677 \n",
      "Epoch 50: val_accuracy improved from 0.76578 to 0.77189, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.9443 - loss: 0.1680 - val_accuracy: 0.7719 - val_loss: 0.6248 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9938\n",
      "Validation Accuracy: 0.7719\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.7719\n",
      "Weighted F1: 0.7698\n",
      "Macro F1: 0.7656\n",
      "Weighted F2: 0.7681\n",
      "Macro F2: 0.7677\n",
      "Precision: 0.7918\n",
      "Recall: 0.7719\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.9733\n",
      "calm: 0.7600\n",
      "disgust: 0.7179\n",
      "fearful: 0.8667\n",
      "happy: 0.6133\n",
      "neutral: 0.9211\n",
      "sad: 0.6267\n",
      "surprised: 0.7179\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.79      0.97      0.87        75\n",
      "        calm       0.90      0.76      0.83        75\n",
      "     disgust       0.76      0.72      0.74        39\n",
      "     fearful       0.70      0.87      0.77        75\n",
      "       happy       0.88      0.61      0.72        75\n",
      "     neutral       0.58      0.92      0.71        38\n",
      "         sad       0.76      0.63      0.69        75\n",
      "   surprised       0.88      0.72      0.79        39\n",
      "\n",
      "    accuracy                           0.77       491\n",
      "   macro avg       0.78      0.77      0.77       491\n",
      "weighted avg       0.79      0.77      0.77       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250625_002605.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "\n",
    "        # # Visualization\n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        # plt.title(f'Confusion Matrix - {RUN_NAME}')\n",
    "        # plt.colorbar()\n",
    "        # tick_positions = np.arange(len(encoder.classes_))\n",
    "        # plt.xticks(tick_positions, encoder.classes_, rotation=45)\n",
    "        # plt.yticks(tick_positions, encoder.classes_)\n",
    "        # plt.ylabel('Actual Label')\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f'{RUN_NAME}_confusion_matrix.png')\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d6fdd9-6130-4cf8-84b0-3794e1cb035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_characteristics(audio_path, sample_rate=16000, clip_length=3.0):\n",
    "    \"\"\"Feature extraction pipeline with consistent output dimensions\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        try:\n",
    "            audio_data, sr = librosa.load(audio_path, sr=sample_rate, duration=clip_length)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        if len(audio_data) == 0:\n",
    "            return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))\n",
    "        \n",
    "        target_samples = int(clip_length * sample_rate)\n",
    "        if len(audio_data) > target_samples:\n",
    "            audio_data = audio_data[:target_samples]\n",
    "        else:\n",
    "            audio_data = np.pad(audio_data, (0, max(0, target_samples - len(audio_data))), mode='constant')\n",
    "        \n",
    "        # Spectrogram parameters\n",
    "        window_size = 1024\n",
    "        frame_shift = 512\n",
    "        mel_bands = 32\n",
    "        \n",
    "        try:\n",
    "            # First channel: Mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_data, sr=sr, n_mels=mel_bands, n_fft=window_size, hop_length=frame_shift)\n",
    "            channel_1 = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            channel_1 = adjust_array_dimensions(channel_1, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_1 = standardize_features(channel_1)\n",
    "        except Exception:\n",
    "            channel_1 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        try:\n",
    "            # Second channel: Combined audio features + Delta features\n",
    "            mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, \n",
    "                                               n_fft=window_size, hop_length=frame_shift)\n",
    "            chroma_features = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, hop_length=frame_shift)\n",
    "            \n",
    "            # Calculate delta features (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc_features)\n",
    "            chroma_delta = librosa.feature.delta(chroma_features)\n",
    "            contrast_delta = librosa.feature.delta(spectral_contrast)\n",
    "            \n",
    "            # Stack original features with delta features\n",
    "            channel_2 = np.vstack([\n",
    "                mfcc_features, \n",
    "                mfcc_delta,\n",
    "                chroma_features,\n",
    "                chroma_delta,\n",
    "                spectral_contrast,\n",
    "                contrast_delta\n",
    "            ])\n",
    "            \n",
    "            channel_2 = adjust_array_dimensions(channel_2, (MEL_BANDS, AUDIO_DURATION))\n",
    "            channel_2 = standardize_features(channel_2)\n",
    "        except Exception:\n",
    "            channel_2 = np.zeros((MEL_BANDS, AUDIO_DURATION))\n",
    "        \n",
    "        feature_stack = np.stack([channel_1, channel_2], axis=-1)\n",
    "        \n",
    "        return feature_stack\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935ad4ab-2485-4eb3-8eec-1474a3138ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_array_dimensions(data_array, desired_shape):\n",
    "    \"\"\"Resize array to match target dimensions by padding or trimming\"\"\"\n",
    "    if data_array.size == 0:\n",
    "        return np.zeros(desired_shape)\n",
    "    \n",
    "    current_dims = data_array.shape\n",
    "    \n",
    "    # Process time axis\n",
    "    if len(current_dims) >= 2:\n",
    "        if current_dims[1] > desired_shape[1]:\n",
    "            data_array = data_array[:, :desired_shape[1]]\n",
    "        elif current_dims[1] < desired_shape[1]:\n",
    "            padding = [(0, 0), (0, desired_shape[1] - current_dims[1])]\n",
    "            data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    # Process frequency axis\n",
    "    if current_dims[0] > desired_shape[0]:\n",
    "        data_array = data_array[:desired_shape[0], :]\n",
    "    elif current_dims[0] < desired_shape[0]:\n",
    "        padding = [(0, desired_shape[0] - current_dims[0]), (0, 0)]\n",
    "        data_array = np.pad(data_array, padding, mode='constant')\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def standardize_features(feature_matrix):\n",
    "    \"\"\"Normalize feature values with robust handling\"\"\"\n",
    "    if feature_matrix.size == 0:\n",
    "        return feature_matrix\n",
    "    \n",
    "    mean_val = np.mean(feature_matrix)\n",
    "    std_val = np.std(feature_matrix)\n",
    "    \n",
    "    if std_val == 0 or np.isnan(std_val):\n",
    "        return feature_matrix - mean_val\n",
    "    \n",
    "    return (feature_matrix - mean_val) / std_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06fdb6e8-3015-44fb-9ea9-522aaa2e0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "AUDIO_DURATION = 94  # ~3 seconds with hop_length=512\n",
    "MEL_BANDS = 32  \n",
    "FEATURE_DIMENSIONS = 2\n",
    "MODEL_ID = \"v6\"  \n",
    "MODEL_TAG = \"enhanced\"  \n",
    "RUN_NAME = f\"emotion_detector_{MODEL_ID}_{MODEL_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34e1cab-3ec7-4b3e-ad32-4ae911a3b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_distribution(feature_set, label_set):\n",
    "    \"\"\"Data augmentation for underrepresented classes\"\"\"\n",
    "    if len(feature_set) == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    unique_labels, label_counts = np.unique(label_set, return_counts=True)\n",
    "    median_count = np.median(label_counts)\n",
    "    \n",
    "    print(\"Balancing class distribution:\")\n",
    "    print(f\"Initial counts: {dict(zip(unique_labels, label_counts))}\")\n",
    "    print(f\"Median count: {median_count}\")\n",
    "    \n",
    "    total_new_samples = sum(max(0, int(median_count - count)) for count in label_counts)\n",
    "    if total_new_samples == 0:\n",
    "        return feature_set, label_set\n",
    "        \n",
    "    augmented_features = np.zeros((total_new_samples, *feature_set.shape[1:]), dtype=feature_set.dtype)\n",
    "    augmented_labels = np.zeros(total_new_samples, dtype=label_set.dtype)\n",
    "    \n",
    "    current_position = 0\n",
    "    \n",
    "    for class_id, count in zip(unique_labels, label_counts):\n",
    "        needed_samples = max(0, int(median_count - count))\n",
    "        if needed_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        class_samples = np.where(label_set == class_id)[0]\n",
    "        \n",
    "        for _ in range(needed_samples):\n",
    "            random_idx = np.random.choice(class_samples)\n",
    "            sample = feature_set[random_idx]\n",
    "            \n",
    "            augmentation = np.random.choice(['add_noise', 'time_warp', 'frequency_mask', 'amplitude_scale'])\n",
    "            \n",
    "            if augmentation == 'add_noise':\n",
    "                modified_sample = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif augmentation == 'time_warp':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(5, 15)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[1] - mask_size))\n",
    "                modified_sample[:, start_pos:start_pos+mask_size, :] = 0\n",
    "            elif augmentation == 'frequency_mask':\n",
    "                modified_sample = np.copy(sample)\n",
    "                mask_size = np.random.randint(3, 8)\n",
    "                start_pos = np.random.randint(0, max(1, sample.shape[0] - mask_size))\n",
    "                modified_sample[start_pos:start_pos+mask_size, :, :] = 0\n",
    "            else:\n",
    "                scale_factor = np.random.uniform(0.8, 1.2)\n",
    "                modified_sample = sample * scale_factor\n",
    "            \n",
    "            augmented_features[current_position] = modified_sample\n",
    "            augmented_labels[current_position] = class_id\n",
    "            current_position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([feature_set, augmented_features], axis=0)\n",
    "    combined_labels = np.concatenate([label_set, augmented_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3d4a02-244b-4b37-aaa7-3c82ebfe1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_emotion_network(input_dims, num_classes):\n",
    "    \"\"\"Neural network architecture for emotion classification\"\"\"\n",
    "    input_layer = Input(shape=input_dims)\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5d3b8c-bf8b-4f12-9728-91d3170e741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_importance(labels):\n",
    "    \"\"\"Compute dynamic class weights with emphasis on rare classes\"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(class_counts)\n",
    "    weight_dict = {}\n",
    "    \n",
    "    print(\"\\nClass distribution analysis:\")\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        base_weight = median_count / class_counts[i]\n",
    "        if class_counts[i] < median_count * 0.5:\n",
    "            base_weight = base_weight ** 1.5\n",
    "        weight_dict[i] = base_weight\n",
    "        print(f\"Class {cls}: samples={class_counts[i]}, weight={base_weight:.2f}\")\n",
    "        \n",
    "    return weight_dict\n",
    "\n",
    "def setup_training_monitors(model_identifier):\n",
    "    \"\"\"Configure training callbacks with custom naming\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_identifier}_top.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4f6514-4f23-410f-8e8d-2df5789b74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_features(dataframe, enable_augmentation=False, cache_folder='feature_cache'):\n",
    "    \"\"\"Feature processing pipeline with caching\"\"\"\n",
    "    file_list = sorted(dataframe['filepath'].tolist())\n",
    "    cache_key = f\"{MODEL_ID}{MEL_BANDS}{AUDIO_DURATION}{FEATURE_DIMENSIONS}{len(file_list)}\"\n",
    "    cache_hash = hashlib.md5(cache_key.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_folder, cache_hash + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached features from: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            base_features = data['features']\n",
    "            base_labels = data['labels']\n",
    "    else:\n",
    "        base_features = []\n",
    "        base_labels = []\n",
    "        print(f\"Processing {len(dataframe)} audio files...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataframe), batch_size):\n",
    "            batch_data = dataframe.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                features = extract_audio_characteristics(row['filepath'])\n",
    "                if features.shape == (MEL_BANDS, AUDIO_DURATION, FEATURE_DIMENSIONS):\n",
    "                    batch_features.append(features)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                base_features.extend(batch_features)\n",
    "                base_labels.extend(batch_labels)\n",
    "        \n",
    "        base_features = np.array(base_features)\n",
    "        base_labels = np.array(base_labels)\n",
    "        \n",
    "        np.savez(cache_path, features=base_features, labels=base_labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(base_features)}\")\n",
    "    \n",
    "    if enable_augmentation:\n",
    "        print(\"Applying data balancing...\")\n",
    "        final_features, final_labels = balance_data_distribution(base_features, base_labels)\n",
    "        print(f\"Augmented dataset size: {final_features.shape[0]}\")\n",
    "    else:\n",
    "        final_features, final_labels = base_features, base_labels\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fef9f98-6829-4869-ab3b-3cdf6f47cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_audio_dataset_from_two_dirs(speech_dir, song_dir):\n",
    "    \"\"\"Create dataset from audio files in two folders (speech & song) with emotion labels\"\"\"\n",
    "    emotion_codes = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def process_dir(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_codes:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_codes[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Process both folders\n",
    "    process_dir(speech_dir)\n",
    "    process_dir(song_dir)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No valid emotion-labeled files found. Using placeholder labels.\")\n",
    "        all_audio_files = glob.glob(os.path.join(speech_dir, \"*.wav\")) + glob.glob(os.path.join(song_dir, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_audio_files))])\n",
    "        paths.extend(all_audio_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7968302-2137-4f55-8b78-b1a3d3474b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(model_file, audio_file, encoder_file):\n",
    "    \"\"\"Predict emotion from audio using trained model\"\"\"\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    with open(encoder_file, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    features = extract_audio_characteristics(audio_file)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_index]\n",
    "    \n",
    "    emotion_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return emotion_label, confidence\n",
    "\n",
    "def record_training_performance(training_history, model_identifier, train_accuracy, val_accuracy,\n",
    "                              overall_acc, f1_macro, f1_weighted,\n",
    "                              f2_macro, f2_weighted, avg_precision, avg_recall,\n",
    "                              per_class_acc, classification_report):\n",
    "    \"\"\"Save comprehensive training metrics\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_identifier}_performance_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Performance Report for {model_identifier}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model Version: {MODEL_ID}\\n\")\n",
    "        f.write(f\"Model Variant: {MODEL_TAG}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY METRICS:\\n\")\n",
    "        f.write(f\"Training Accuracy: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F1: {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F2: {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"Macro F2: {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "        for class_name, accuracy in per_class_acc.items():\n",
    "            f.write(f\"{class_name}: {accuracy:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(classification_report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Performance report saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc7e15f9-5c8b-4e38-920e-0b26e89513d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating pipeline for: emotion_detector_v6_enhanced\n",
      "Model Version: v6\n",
      "Model Variant: enhanced\n",
      "==================================================\n",
      "Dataset contains 2452 samples\n",
      "Detected emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Class distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training samples: 1961\n",
      "Validation samples: 491\n",
      "\n",
      "=== Processing training features ===\n",
      "Loading cached features from: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data balancing...\n",
      "Balancing class distribution:\n",
      "Initial counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Median count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation features ===\n",
      "Loading cached features from: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input dimensions: (32, 94, 2)\n",
      "\n",
      "Class distribution analysis:\n",
      "Class 0: samples=301, weight=1.00\n",
      "Class 1: samples=301, weight=1.00\n",
      "Class 2: samples=301, weight=1.00\n",
      "Class 3: samples=301, weight=1.00\n",
      "Class 4: samples=301, weight=1.00\n",
      "Class 5: samples=301, weight=1.00\n",
      "Class 6: samples=301, weight=1.00\n",
      "Class 7: samples=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training session: emotion_detector_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1621 - loss: 2.1468\n",
      "Epoch 1: val_accuracy improved from -inf to 0.15275, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.1628 - loss: 2.1450 - val_accuracy: 0.1527 - val_loss: 2.0494 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3077 - loss: 1.8018\n",
      "Epoch 2: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - accuracy: 0.3081 - loss: 1.8007 - val_accuracy: 0.1100 - val_loss: 2.1363 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3787 - loss: 1.6195\n",
      "Epoch 3: val_accuracy improved from 0.15275 to 0.24033, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.3789 - loss: 1.6192 - val_accuracy: 0.2403 - val_loss: 2.1672 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4356 - loss: 1.5068\n",
      "Epoch 4: val_accuracy did not improve from 0.24033\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - accuracy: 0.4355 - loss: 1.5067 - val_accuracy: 0.1711 - val_loss: 2.2277 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4685 - loss: 1.4399\n",
      "Epoch 5: val_accuracy improved from 0.24033 to 0.24644, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.4688 - loss: 1.4386 - val_accuracy: 0.2464 - val_loss: 1.9060 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4946 - loss: 1.3119\n",
      "Epoch 6: val_accuracy improved from 0.24644 to 0.33401, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.4947 - loss: 1.3117 - val_accuracy: 0.3340 - val_loss: 1.8329 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5411 - loss: 1.2222\n",
      "Epoch 7: val_accuracy improved from 0.33401 to 0.38697, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.5415 - loss: 1.2218 - val_accuracy: 0.3870 - val_loss: 1.5566 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5711 - loss: 1.1347\n",
      "Epoch 8: val_accuracy improved from 0.38697 to 0.46640, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.5710 - loss: 1.1351 - val_accuracy: 0.4664 - val_loss: 1.4524 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5936 - loss: 1.0939\n",
      "Epoch 9: val_accuracy improved from 0.46640 to 0.47251, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.5935 - loss: 1.0942 - val_accuracy: 0.4725 - val_loss: 1.3759 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6359 - loss: 1.0145\n",
      "Epoch 10: val_accuracy did not improve from 0.47251\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.6361 - loss: 1.0138 - val_accuracy: 0.3951 - val_loss: 1.4091 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6258 - loss: 0.9883\n",
      "Epoch 11: val_accuracy improved from 0.47251 to 0.59878, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.6263 - loss: 0.9877 - val_accuracy: 0.5988 - val_loss: 1.1524 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6585 - loss: 0.9283\n",
      "Epoch 12: val_accuracy did not improve from 0.59878\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.6587 - loss: 0.9279 - val_accuracy: 0.5092 - val_loss: 1.1931 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6991 - loss: 0.8663\n",
      "Epoch 13: val_accuracy improved from 0.59878 to 0.62118, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.6990 - loss: 0.8665 - val_accuracy: 0.6212 - val_loss: 1.0545 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7042 - loss: 0.8104\n",
      "Epoch 14: val_accuracy did not improve from 0.62118\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.7042 - loss: 0.8108 - val_accuracy: 0.5295 - val_loss: 1.2607 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7171 - loss: 0.7814\n",
      "Epoch 15: val_accuracy did not improve from 0.62118\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.7171 - loss: 0.7816 - val_accuracy: 0.6029 - val_loss: 1.1121 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7269 - loss: 0.7571\n",
      "Epoch 16: val_accuracy improved from 0.62118 to 0.63544, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.7268 - loss: 0.7576 - val_accuracy: 0.6354 - val_loss: 0.9862 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7404 - loss: 0.6891\n",
      "Epoch 17: val_accuracy improved from 0.63544 to 0.70672, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.7404 - loss: 0.6893 - val_accuracy: 0.7067 - val_loss: 0.8206 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7695 - loss: 0.6651\n",
      "Epoch 18: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.7692 - loss: 0.6655 - val_accuracy: 0.5397 - val_loss: 1.2851 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7817 - loss: 0.6273\n",
      "Epoch 19: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.7814 - loss: 0.6279 - val_accuracy: 0.6477 - val_loss: 0.9556 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7812 - loss: 0.6095\n",
      "Epoch 20: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.7811 - loss: 0.6100 - val_accuracy: 0.6538 - val_loss: 0.9395 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7781 - loss: 0.6156\n",
      "Epoch 21: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.7782 - loss: 0.6154 - val_accuracy: 0.6110 - val_loss: 1.0398 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7862 - loss: 0.5954\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.7861 - loss: 0.5954 - val_accuracy: 0.5397 - val_loss: 1.1665 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8159 - loss: 0.5273\n",
      "Epoch 23: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.8160 - loss: 0.5272 - val_accuracy: 0.5743 - val_loss: 1.1298 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8431 - loss: 0.4627\n",
      "Epoch 24: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.8430 - loss: 0.4628 - val_accuracy: 0.6151 - val_loss: 1.0102 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8425 - loss: 0.4563\n",
      "Epoch 25: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.8425 - loss: 0.4563 - val_accuracy: 0.6680 - val_loss: 0.9178 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8610 - loss: 0.4268\n",
      "Epoch 26: val_accuracy did not improve from 0.70672\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8608 - loss: 0.4274 - val_accuracy: 0.6314 - val_loss: 0.9336 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8415 - loss: 0.4433\n",
      "Epoch 27: val_accuracy improved from 0.70672 to 0.75764, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.8415 - loss: 0.4429 - val_accuracy: 0.7576 - val_loss: 0.6651 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8600 - loss: 0.4149\n",
      "Epoch 28: val_accuracy did not improve from 0.75764\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.8601 - loss: 0.4145 - val_accuracy: 0.6802 - val_loss: 0.8351 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8561 - loss: 0.3980\n",
      "Epoch 29: val_accuracy improved from 0.75764 to 0.78819, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8562 - loss: 0.3979 - val_accuracy: 0.7882 - val_loss: 0.5830 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8578 - loss: 0.3978\n",
      "Epoch 30: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.8579 - loss: 0.3976 - val_accuracy: 0.6456 - val_loss: 1.0156 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8537 - loss: 0.4151\n",
      "Epoch 31: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.8537 - loss: 0.4152 - val_accuracy: 0.7495 - val_loss: 0.6695 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8670 - loss: 0.3856\n",
      "Epoch 32: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.8671 - loss: 0.3852 - val_accuracy: 0.6253 - val_loss: 1.1374 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8822 - loss: 0.3434\n",
      "Epoch 33: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8820 - loss: 0.3437 - val_accuracy: 0.7576 - val_loss: 0.6277 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8829 - loss: 0.3320\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.8830 - loss: 0.3318 - val_accuracy: 0.7352 - val_loss: 0.7222 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8764 - loss: 0.3272\n",
      "Epoch 35: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - accuracy: 0.8767 - loss: 0.3264 - val_accuracy: 0.7434 - val_loss: 0.6726 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9018 - loss: 0.2800\n",
      "Epoch 36: val_accuracy did not improve from 0.78819\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.9017 - loss: 0.2803 - val_accuracy: 0.7637 - val_loss: 0.6053 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9079 - loss: 0.2999\n",
      "Epoch 37: val_accuracy improved from 0.78819 to 0.79633, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.9077 - loss: 0.2999 - val_accuracy: 0.7963 - val_loss: 0.5395 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8991 - loss: 0.2825\n",
      "Epoch 38: val_accuracy did not improve from 0.79633\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8991 - loss: 0.2826 - val_accuracy: 0.7515 - val_loss: 0.6503 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8994 - loss: 0.2902\n",
      "Epoch 39: val_accuracy did not improve from 0.79633\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.8996 - loss: 0.2898 - val_accuracy: 0.7393 - val_loss: 0.6574 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9099 - loss: 0.2649\n",
      "Epoch 40: val_accuracy did not improve from 0.79633\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.9098 - loss: 0.2652 - val_accuracy: 0.7841 - val_loss: 0.5922 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9115 - loss: 0.2547\n",
      "Epoch 41: val_accuracy improved from 0.79633 to 0.80041, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.9113 - loss: 0.2548 - val_accuracy: 0.8004 - val_loss: 0.5551 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9150 - loss: 0.2501\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.9149 - loss: 0.2504 - val_accuracy: 0.7210 - val_loss: 0.7035 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9053 - loss: 0.2788\n",
      "Epoch 43: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.9055 - loss: 0.2783 - val_accuracy: 0.8004 - val_loss: 0.5255 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9179 - loss: 0.2343\n",
      "Epoch 44: val_accuracy did not improve from 0.80041\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.9178 - loss: 0.2345 - val_accuracy: 0.7800 - val_loss: 0.6195 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9167 - loss: 0.2351\n",
      "Epoch 45: val_accuracy improved from 0.80041 to 0.81263, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.9167 - loss: 0.2350 - val_accuracy: 0.8126 - val_loss: 0.5104 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9150 - loss: 0.2344\n",
      "Epoch 46: val_accuracy did not improve from 0.81263\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.9150 - loss: 0.2343 - val_accuracy: 0.8086 - val_loss: 0.5044 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9166 - loss: 0.2303\n",
      "Epoch 47: val_accuracy did not improve from 0.81263\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.9167 - loss: 0.2300 - val_accuracy: 0.8126 - val_loss: 0.5093 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9215 - loss: 0.2225\n",
      "Epoch 48: val_accuracy improved from 0.81263 to 0.82077, saving model to emotion_detector_v6_enhanced_top.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.9217 - loss: 0.2222 - val_accuracy: 0.8208 - val_loss: 0.5070 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9255 - loss: 0.2221\n",
      "Epoch 49: val_accuracy did not improve from 0.82077\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.9253 - loss: 0.2225 - val_accuracy: 0.7923 - val_loss: 0.5297 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9257 - loss: 0.1983\n",
      "Epoch 50: val_accuracy did not improve from 0.82077\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.9258 - loss: 0.1985 - val_accuracy: 0.7902 - val_loss: 0.5228 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE FOR emotion_detector_v6_enhanced:\n",
      "Training Accuracy: 0.9917\n",
      "Validation Accuracy: 0.8208\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step \n",
      "PERFORMANCE SUMMARY:\n",
      "Accuracy: 0.8208\n",
      "Weighted F1: 0.8193\n",
      "Macro F1: 0.8137\n",
      "Weighted F2: 0.8196\n",
      "Macro F2: 0.8142\n",
      "Precision: 0.8235\n",
      "Recall: 0.8208\n",
      "\n",
      "CLASS-SPECIFIC ACCURACY:\n",
      "angry: 0.9333\n",
      "calm: 0.9333\n",
      "disgust: 0.8718\n",
      "fearful: 0.7867\n",
      "happy: 0.7600\n",
      "neutral: 0.8421\n",
      "sad: 0.7333\n",
      "surprised: 0.6667\n",
      "\n",
      "CLASSIFICATION DETAILS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.91      0.93      0.92        75\n",
      "        calm       0.83      0.93      0.88        75\n",
      "     disgust       0.74      0.87      0.80        39\n",
      "     fearful       0.80      0.79      0.79        75\n",
      "       happy       0.85      0.76      0.80        75\n",
      "     neutral       0.76      0.84      0.80        38\n",
      "         sad       0.76      0.73      0.75        75\n",
      "   surprised       0.90      0.67      0.76        39\n",
      "\n",
      "    accuracy                           0.82       491\n",
      "   macro avg       0.82      0.82      0.81       491\n",
      "weighted avg       0.82      0.82      0.82       491\n",
      "\n",
      "Performance report saved to: emotion_detector_v6_enhanced_performance_20250625_010417.txt\n",
      "\n",
      "Output files generated:\n",
      "  - emotion_detector_v6_enhanced_top.keras (best validation model)\n",
      "  - emotion_detector_v6_enhanced_final.keras (final trained model)\n",
      "  - emotion_detector_v6_enhanced_label_encoder.pkl (label mapping)\n",
      "  - emotion_detector_v6_enhanced_confusion_matrix.png (visualization)\n",
      "  - emotion_detector_v6_enhanced_performance_*.txt (detailed metrics)\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(f\"Initiating pipeline for: {RUN_NAME}\")\n",
    "    print(f\"Model Version: {MODEL_ID}\")\n",
    "    print(f\"Model Variant: {MODEL_TAG}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU configuration\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpu_devices:\n",
    "        try:\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    speech_dir = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_dir = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Initialize dataset\n",
    "    audio_dataset = initialize_audio_dataset_from_two_dirs(speech_dir, song_dir)\n",
    "    if audio_dataset is None:\n",
    "        print(\"Dataset initialization failed. Verify data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset contains {len(audio_dataset)} samples\")\n",
    "    print(f\"Detected emotions: {audio_dataset['emotion'].unique()}\")\n",
    "    print(f\"Class distribution:\\n{audio_dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(audio_dataset, test_size=0.2, \n",
    "                                          random_state=42, stratify=audio_dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature processing\n",
    "        print(\"\\n=== Processing training features ===\")\n",
    "        X_train, y_train_raw = process_audio_features(train_data, enable_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation features ===\")\n",
    "        X_val, y_val_raw = process_audio_features(val_data, enable_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed. Terminating.\")\n",
    "            return\n",
    "        \n",
    "        # Label encoding\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train_raw)\n",
    "        y_val = encoder.transform(y_val_raw)\n",
    "        \n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input dimensions: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Model construction\n",
    "        emotion_model = construct_emotion_network(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weighting\n",
    "        class_weights = calculate_class_importance(y_train)\n",
    "        \n",
    "        # Model compilation\n",
    "        emotion_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {emotion_model.count_params():,}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(f\"\\n=== Training session: {RUN_NAME} ===\")\n",
    "        training_history = emotion_model.fit(\n",
    "            X_train, y_train_categorical,\n",
    "            validation_data=(X_val, y_val_categorical),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=setup_training_monitors(RUN_NAME),\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best performing model\n",
    "        emotion_model = tf.keras.models.load_model(f'{RUN_NAME}_top.keras')\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        train_loss, train_accuracy = emotion_model.evaluate(X_train, y_train_categorical, verbose=0)\n",
    "        val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val_categorical, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MODEL PERFORMANCE FOR {RUN_NAME}:\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = emotion_model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Comprehensive metrics\n",
    "        from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class-specific accuracy\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "        class_accuracy = {}\n",
    "        for i, class_name in enumerate(encoder.classes_):\n",
    "            class_accuracy[class_name] = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
    "\n",
    "        # Detailed report\n",
    "        clf_report = classification_report(y_val, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display metrics\n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"Weighted F2: {weighted_f2:.4f}\")\n",
    "        print(f\"Macro F2: {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS-SPECIFIC ACCURACY:\")\n",
    "        for class_name, acc in class_accuracy.items():\n",
    "            print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASSIFICATION DETAILS:\")\n",
    "        print(clf_report)\n",
    "        \n",
    "        # Save artifacts\n",
    "        emotion_model.save(f'{RUN_NAME}_final.keras')\n",
    "        with open(f'{RUN_NAME}_label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Record performance\n",
    "        record_training_performance(training_history, RUN_NAME, train_accuracy, val_accuracy,\n",
    "                                  overall_accuracy, macro_f1, weighted_f1,\n",
    "                                  macro_f2, weighted_f2, precision, recall,\n",
    "                                  class_accuracy, clf_report)\n",
    "        \n",
    "        print(f\"\\nOutput files generated:\")\n",
    "        print(f\"  - {RUN_NAME}_top.keras (best validation model)\")\n",
    "        print(f\"  - {RUN_NAME}_final.keras (final trained model)\")\n",
    "        print(f\"  - {RUN_NAME}_label_encoder.pkl (label mapping)\")\n",
    "        print(f\"  - {RUN_NAME}_confusion_matrix.png (visualization)\")\n",
    "        print(f\"  - {RUN_NAME}_performance_*.txt (detailed metrics)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99987147-b262-45c7-a88a-39204fc1e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, fbeta_score, precision_score, recall_score, accuracy_score\n",
    "import pickle\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdecb21-09a1-44b0-94e6-b499a062b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_FREQUENCIES = 32  \n",
    "TIME_STEPS = 94  \n",
    "FEATURE_CHANNELS = 2\n",
    "MODEL_VERSION = \"v6\"  \n",
    "MODEL_TYPE = \"enhanced\"  \n",
    "EXPERIMENT_NAME = f\"emotion_classifier_{MODEL_VERSION}_{MODEL_TYPE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adce47ff-8acf-47e0-b0a0-5cbe0a66415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features(audio_file, target_sr=16000, duration=3.0):\n",
    "    \"\"\"Extracts and processes audio features with consistent output size\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_file):\n",
    "            return np.zeros((MEL_FREQUENCIES, TIME_STEPS, FEATURE_CHANNELS))\n",
    "        \n",
    "        try:\n",
    "            sound_data, sample_rate = librosa.load(audio_file, sr=target_sr, duration=duration)\n",
    "        except Exception:\n",
    "            return np.zeros((MEL_FREQUENCIES, TIME_STEPS, FEATURE_CHANNELS))\n",
    "        \n",
    "        if len(sound_data) == 0:\n",
    "            return np.zeros((MEL_FREQUENCIES, TIME_STEPS, FEATURE_CHANNELS))\n",
    "        \n",
    "        required_samples = int(duration * target_sr)\n",
    "        if len(sound_data) > required_samples:\n",
    "            sound_data = sound_data[:required_samples]\n",
    "        else:\n",
    "            sound_data = np.pad(sound_data, (0, max(0, required_samples - len(sound_data))), mode='constant')\n",
    "        \n",
    "        # Audio processing parameters\n",
    "        fft_size = 1024\n",
    "        hop_size = 512\n",
    "        mel_filters = 32\n",
    "        \n",
    "        # First feature channel: Mel spectrogram\n",
    "        try:\n",
    "            mel_spectrum = librosa.feature.melspectrogram(\n",
    "                y=sound_data, sr=sample_rate, n_mels=mel_filters, \n",
    "                n_fft=fft_size, hop_length=hop_size)\n",
    "            mel_channel = librosa.power_to_db(mel_spectrum, ref=np.max)\n",
    "            mel_channel = resize_array(mel_channel, (MEL_FREQUENCIES, TIME_STEPS))\n",
    "            mel_channel = normalize_features(mel_channel)\n",
    "        except Exception:\n",
    "            mel_channel = np.zeros((MEL_FREQUENCIES, TIME_STEPS))\n",
    "        \n",
    "        # Second feature channel: Combined features\n",
    "        try:\n",
    "            mfcc = librosa.feature.mfcc(y=sound_data, sr=sample_rate, n_mfcc=13, \n",
    "                                      n_fft=fft_size, hop_length=hop_size)\n",
    "            chroma = librosa.feature.chroma_stft(y=sound_data, sr=sample_rate, hop_length=hop_size)\n",
    "            spectral_features = librosa.feature.spectral_contrast(y=sound_data, sr=sample_rate, hop_length=hop_size)\n",
    "            \n",
    "            # Feature derivatives\n",
    "            mfcc_derivative = librosa.feature.delta(mfcc)\n",
    "            chroma_derivative = librosa.feature.delta(chroma)\n",
    "            spectral_derivative = librosa.feature.delta(spectral_features)\n",
    "            \n",
    "            # Combine features\n",
    "            feature_channel = np.vstack([\n",
    "                mfcc, \n",
    "                mfcc_derivative,\n",
    "                chroma,\n",
    "                chroma_derivative,\n",
    "                spectral_features,\n",
    "                spectral_derivative\n",
    "            ])\n",
    "            \n",
    "            feature_channel = resize_array(feature_channel, (MEL_FREQUENCIES, TIME_STEPS))\n",
    "            feature_channel = normalize_features(feature_channel)\n",
    "        except Exception:\n",
    "            feature_channel = np.zeros((MEL_FREQUENCIES, TIME_STEPS))\n",
    "        \n",
    "        combined_features = np.stack([mel_channel, feature_channel], axis=-1)\n",
    "        \n",
    "        return combined_features\n",
    "    \n",
    "    except Exception:\n",
    "        return np.zeros((MEL_FREQUENCIES, TIME_STEPS, FEATURE_CHANNELS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6faa65d5-288c-4b22-9f28-c140ec4ca24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_array(input_array, target_size):\n",
    "    \"\"\"Adjusts array dimensions to match target size\"\"\"\n",
    "    if input_array.size == 0:\n",
    "        return np.zeros(target_size)\n",
    "    \n",
    "    current_shape = input_array.shape\n",
    "    \n",
    "    # Adjust time dimension\n",
    "    if len(current_shape) >= 2:\n",
    "        if current_shape[1] > target_size[1]:\n",
    "            input_array = input_array[:, :target_size[1]]\n",
    "        elif current_shape[1] < target_size[1]:\n",
    "            pad_width = [(0, 0), (0, target_size[1] - current_shape[1])]\n",
    "            input_array = np.pad(input_array, pad_width, mode='constant')\n",
    "    \n",
    "    # Adjust frequency dimension\n",
    "    if current_shape[0] > target_size[0]:\n",
    "        input_array = input_array[:target_size[0], :]\n",
    "    elif current_shape[0] < target_size[0]:\n",
    "        pad_width = [(0, target_size[0] - current_shape[0]), (0, 0)]\n",
    "        input_array = np.pad(input_array, pad_width, mode='constant')\n",
    "    \n",
    "    return input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d64e752-9a29-4e2c-b201-526e959d268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(feature_data):\n",
    "    \"\"\"Standardizes feature values\"\"\"\n",
    "    if feature_data.size == 0:\n",
    "        return feature_data\n",
    "    \n",
    "    mean_value = np.mean(feature_data)\n",
    "    std_value = np.std(feature_data)\n",
    "    \n",
    "    if std_value == 0 or np.isnan(std_value):\n",
    "        return feature_data - mean_value\n",
    "    \n",
    "    return (feature_data - mean_value) / std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfbfb1f-77c6-4820-9b4c-3f081e0e7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(features, labels):\n",
    "    \"\"\"Balances dataset through augmentation\"\"\"\n",
    "    if len(features) == 0:\n",
    "        return features, labels\n",
    "        \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    median_count = np.median(counts)\n",
    "    \n",
    "    print(\"Adjusting class distribution:\")\n",
    "    print(f\"Current counts: {dict(zip(unique_labels, counts))}\")\n",
    "    print(f\"Target count: {median_count}\")\n",
    "    \n",
    "    total_new = sum(max(0, int(median_count - count)) for count in counts)\n",
    "    if total_new == 0:\n",
    "        return features, labels\n",
    "        \n",
    "    new_features = np.zeros((total_new, *features.shape[1:]), dtype=features.dtype)\n",
    "    new_labels = np.zeros(total_new, dtype=labels.dtype)\n",
    "    \n",
    "    position = 0\n",
    "    \n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        needed = max(0, int(median_count - count))\n",
    "        if needed == 0:\n",
    "            continue\n",
    "            \n",
    "        samples = np.where(labels == label)[0]\n",
    "        \n",
    "        for _ in range(needed):\n",
    "            idx = np.random.choice(samples)\n",
    "            sample = features[idx]\n",
    "            \n",
    "            method = np.random.choice(['noise', 'time_shift', 'freq_mask', 'amplitude'])\n",
    "            \n",
    "            if method == 'noise':\n",
    "                modified = sample + np.random.normal(0, 0.02, sample.shape)\n",
    "            elif method == 'time_shift':\n",
    "                modified = np.copy(sample)\n",
    "                mask = np.random.randint(5, 15)\n",
    "                start = np.random.randint(0, max(1, sample.shape[1] - mask))\n",
    "                modified[:, start:start+mask, :] = 0\n",
    "            elif method == 'freq_mask':\n",
    "                modified = np.copy(sample)\n",
    "                mask = np.random.randint(3, 8)\n",
    "                start = np.random.randint(0, max(1, sample.shape[0] - mask))\n",
    "                modified[start:start+mask, :, :] = 0\n",
    "            else:\n",
    "                scale = np.random.uniform(0.8, 1.2)\n",
    "                modified = sample * scale\n",
    "            \n",
    "            new_features[position] = modified\n",
    "            new_labels[position] = label\n",
    "            position += 1\n",
    "    \n",
    "    combined_features = np.concatenate([features, new_features], axis=0)\n",
    "    combined_labels = np.concatenate([labels, new_labels], axis=0)\n",
    "    \n",
    "    return combined_features, combined_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8b5098-8815-4c4b-bc63-94fba2370c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_shape, num_classes):\n",
    "    \"\"\"Constructs neural network for emotion classification\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Feature extraction\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b83993-287b-43b1-a268-9ec311142af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    \"\"\"Calculates class weights for imbalanced data\"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    median = np.median(counts)\n",
    "    weights = {}\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        weight = median / counts[i]\n",
    "        if counts[i] < median * 0.5:\n",
    "            weight = weight ** 1.5\n",
    "        weights[i] = weight\n",
    "        print(f\"Class {cls}: count={counts[i]}, weight={weight:.2f}\")\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61a8ab37-e7b8-4ca4-b71a-5d3e125e9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_callbacks(model_name):\n",
    "    \"\"\"Sets up training callbacks\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_name}_best.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e350a59-6b6f-4df3-a2fb-f6640a84a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, use_augmentation=False, cache_dir='feature_cache'):\n",
    "    \"\"\"Processes audio features with optional caching\"\"\"\n",
    "    files = sorted(data['filepath'].tolist())\n",
    "    cache_id = f\"{MODEL_VERSION}{MEL_FREQUENCIES}{TIME_STEPS}{FEATURE_CHANNELS}{len(files)}\"\n",
    "    cache_file = hashlib.md5(cache_id.encode('utf-8')).hexdigest()\n",
    "    cache_path = os.path.join(cache_dir, cache_file + \".npz\")\n",
    "    \n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading features from cache: {cache_path}\")\n",
    "        with np.load(cache_path) as data:\n",
    "            features = data['features']\n",
    "            labels = data['labels']\n",
    "    else:\n",
    "        features = []\n",
    "        labels = []\n",
    "        print(f\"Processing {len(data)} audio files...\")\n",
    "        \n",
    "        batch = 100\n",
    "        for i in range(0, len(data), batch):\n",
    "            batch_data = data.iloc[i:i+batch]\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_data.iterrows():\n",
    "                feats = get_audio_features(row['filepath'])\n",
    "                if feats.shape == (MEL_FREQUENCIES, TIME_STEPS, FEATURE_CHANNELS):\n",
    "                    batch_features.append(feats)\n",
    "                    batch_labels.append(row['emotion'])\n",
    "            \n",
    "            if batch_features:\n",
    "                features.extend(batch_features)\n",
    "                labels.extend(batch_labels)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        np.savez(cache_path, features=features, labels=labels)\n",
    "        print(f\"Saved features to cache: {cache_path}\")\n",
    "    \n",
    "    print(f\"Initial feature count: {len(features)}\")\n",
    "    \n",
    "    if use_augmentation:\n",
    "        print(\"Applying data augmentation...\")\n",
    "        features, labels = augment_dataset(features, labels)\n",
    "        print(f\"Augmented dataset size: {features.shape[0]}\")\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc79ff8-540d-4577-8044-454d14724b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data(speech_path, song_path):\n",
    "    \"\"\"Loads audio dataset from directories\"\"\"\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    paths = []\n",
    "    emotions = []\n",
    "\n",
    "    def scan_directory(directory):\n",
    "        audio_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2]\n",
    "                    if code in emotion_map:\n",
    "                        paths.append(file)\n",
    "                        emotions.append(emotion_map[code])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    scan_directory(speech_path)\n",
    "    scan_directory(song_path)\n",
    "\n",
    "    if not paths:\n",
    "        print(\"No labeled files found. Using default labels.\")\n",
    "        all_files = glob.glob(os.path.join(speech_path, \"*.wav\")) + glob.glob(os.path.join(song_path, \"*.wav\"))\n",
    "        emotions.extend([f\"emotion_{i%4}\" for i in range(len(all_files))])\n",
    "        paths.extend(all_files)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'filepath': paths,\n",
    "        'emotion': emotions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7809fd-0840-458b-a47f-46a58dd84b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model_path, audio_path, encoder_path):\n",
    "    \"\"\"Predicts emotion from audio file\"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    with open(encoder_path, 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "    \n",
    "    features = get_audio_features(audio_path)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "    predictions = model.predict(features, verbose=0)\n",
    "    pred_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][pred_idx]\n",
    "    \n",
    "    emotion = encoder.inverse_transform([pred_idx])[0]\n",
    "    return emotion, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941b4f39-ca8b-4b1a-babe-5af79a5adc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(history, model_id, train_acc, val_acc,\n",
    "               accuracy, f1_macro, f1_weighted,\n",
    "               f2_macro, f2_weighted, precision, recall,\n",
    "               class_acc, report):\n",
    "    \"\"\"Saves training results to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{model_id}_results_{timestamp}.txt\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"Results for {model_id}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Time: {timestamp}\\n\")\n",
    "        f.write(f\"Version: {MODEL_VERSION}\\n\")\n",
    "        f.write(f\"Type: {MODEL_TYPE}\\n\\n\")\n",
    "        \n",
    "        f.write(\"METRICS:\\n\")\n",
    "        f.write(f\"Train Accuracy: {train_acc:.4f}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"F1 (weighted): {f1_weighted:.4f}\\n\")\n",
    "        f.write(f\"F1 (macro): {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"F2 (weighted): {f2_weighted:.4f}\\n\")\n",
    "        f.write(f\"F2 (macro): {f2_macro:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"CLASS ACCURACY:\\n\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            f.write(f\"{cls}: {acc:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "    return results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28779ffd-5e8b-4ca4-bc29-4a9511e5a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis: emotion_classifier_v6_enhanced\n",
      "Version: v6\n",
      "Type: enhanced\n",
      "==================================================\n",
      "Loaded 2452 samples\n",
      "Emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 1961\n",
      "Validation set: 491\n",
      "\n",
      "=== Processing training data ===\n",
      "Loading features from cache: feature_cache\\aa82c0a65cc0633a599c6eae847d31b9.npz\n",
      "Initial feature count: 1961\n",
      "Applying data augmentation...\n",
      "Adjusting class distribution:\n",
      "Current counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301, 'surprised': 153}\n",
      "Target count: 301.0\n",
      "Augmented dataset size: 2408\n",
      "\n",
      "=== Processing validation data ===\n",
      "Loading features from cache: feature_cache\\5608923ad4f113abf46a3a9b7cb97fe5.npz\n",
      "Initial feature count: 491\n",
      "\n",
      "Data statistics:\n",
      "Training samples: 2408\n",
      "Validation samples: 491\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "Input shape: (32, 94, 2)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=301, weight=1.00\n",
      "Class 1: count=301, weight=1.00\n",
      "Class 2: count=301, weight=1.00\n",
      "Class 3: count=301, weight=1.00\n",
      "Class 4: count=301, weight=1.00\n",
      "Class 5: count=301, weight=1.00\n",
      "Class 6: count=301, weight=1.00\n",
      "Class 7: count=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,400\n",
      "\n",
      "=== Training: emotion_classifier_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.1986 - loss: 2.0742 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.15275, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.1998 - loss: 2.0706 - val_accuracy: 0.1527 - val_loss: 2.1519 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3431 - loss: 1.7168\n",
      "Epoch 2: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.3436 - loss: 1.7158 - val_accuracy: 0.1527 - val_loss: 3.3189 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.4067 - loss: 1.5451\n",
      "Epoch 3: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.4065 - loss: 1.5456 - val_accuracy: 0.1527 - val_loss: 4.1862 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.4461 - loss: 1.4824\n",
      "Epoch 4: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 123ms/step - accuracy: 0.4466 - loss: 1.4814 - val_accuracy: 0.1527 - val_loss: 5.6820 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5036 - loss: 1.3299\n",
      "Epoch 5: val_accuracy did not improve from 0.15275\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - accuracy: 0.5033 - loss: 1.3300 - val_accuracy: 0.1527 - val_loss: 4.2371 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.5194 - loss: 1.2656 \n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.15275 to 0.17312, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.5198 - loss: 1.2651 - val_accuracy: 0.1731 - val_loss: 2.8086 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5556 - loss: 1.1639\n",
      "Epoch 7: val_accuracy improved from 0.17312 to 0.27495, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.5564 - loss: 1.1629 - val_accuracy: 0.2749 - val_loss: 2.0833 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6135 - loss: 1.0492 \n",
      "Epoch 8: val_accuracy improved from 0.27495 to 0.51527, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.6133 - loss: 1.0497 - val_accuracy: 0.5153 - val_loss: 1.3284 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6503 - loss: 0.9938\n",
      "Epoch 9: val_accuracy did not improve from 0.51527\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.6503 - loss: 0.9939 - val_accuracy: 0.4908 - val_loss: 1.3334 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.6466 - loss: 0.9458\n",
      "Epoch 10: val_accuracy improved from 0.51527 to 0.58045, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.6467 - loss: 0.9461 - val_accuracy: 0.5804 - val_loss: 1.1305 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.6832 - loss: 0.8982\n",
      "Epoch 11: val_accuracy did not improve from 0.58045\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 123ms/step - accuracy: 0.6828 - loss: 0.8988 - val_accuracy: 0.5173 - val_loss: 1.2992 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6874 - loss: 0.8667\n",
      "Epoch 12: val_accuracy improved from 0.58045 to 0.60896, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.6874 - loss: 0.8664 - val_accuracy: 0.6090 - val_loss: 1.1135 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.6980 - loss: 0.8356 \n",
      "Epoch 13: val_accuracy did not improve from 0.60896\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.6980 - loss: 0.8353 - val_accuracy: 0.5764 - val_loss: 1.2333 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7097 - loss: 0.8054\n",
      "Epoch 14: val_accuracy did not improve from 0.60896\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.7096 - loss: 0.8053 - val_accuracy: 0.5784 - val_loss: 1.1690 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7294 - loss: 0.7541\n",
      "Epoch 15: val_accuracy did not improve from 0.60896\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - accuracy: 0.7294 - loss: 0.7541 - val_accuracy: 0.5295 - val_loss: 1.2548 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7172 - loss: 0.7605\n",
      "Epoch 16: val_accuracy did not improve from 0.60896\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.7176 - loss: 0.7599 - val_accuracy: 0.4542 - val_loss: 1.4693 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.7662 - loss: 0.6803 \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.60896\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.7660 - loss: 0.6800 - val_accuracy: 0.4603 - val_loss: 1.6727 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.7803 - loss: 0.6103\n",
      "Epoch 18: val_accuracy improved from 0.60896 to 0.69450, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 132ms/step - accuracy: 0.7802 - loss: 0.6109 - val_accuracy: 0.6945 - val_loss: 0.8652 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.7818 - loss: 0.6221\n",
      "Epoch 19: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.7818 - loss: 0.6221 - val_accuracy: 0.6823 - val_loss: 0.8436 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7860 - loss: 0.6247 \n",
      "Epoch 20: val_accuracy did not improve from 0.69450\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 117ms/step - accuracy: 0.7860 - loss: 0.6242 - val_accuracy: 0.6314 - val_loss: 0.9283 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8074 - loss: 0.5509\n",
      "Epoch 21: val_accuracy improved from 0.69450 to 0.70468, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - accuracy: 0.8071 - loss: 0.5514 - val_accuracy: 0.7047 - val_loss: 0.8122 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8138 - loss: 0.5489 \n",
      "Epoch 22: val_accuracy improved from 0.70468 to 0.72098, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.8138 - loss: 0.5491 - val_accuracy: 0.7210 - val_loss: 0.7725 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8186 - loss: 0.5181\n",
      "Epoch 23: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.8184 - loss: 0.5184 - val_accuracy: 0.7149 - val_loss: 0.8088 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8275 - loss: 0.5042\n",
      "Epoch 24: val_accuracy did not improve from 0.72098\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 116ms/step - accuracy: 0.8274 - loss: 0.5043 - val_accuracy: 0.7210 - val_loss: 0.7881 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8275 - loss: 0.4953 \n",
      "Epoch 25: val_accuracy improved from 0.72098 to 0.72912, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.8273 - loss: 0.4956 - val_accuracy: 0.7291 - val_loss: 0.7277 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8405 - loss: 0.4677\n",
      "Epoch 26: val_accuracy did not improve from 0.72912\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - accuracy: 0.8404 - loss: 0.4679 - val_accuracy: 0.7067 - val_loss: 0.8207 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8432 - loss: 0.4756\n",
      "Epoch 27: val_accuracy did not improve from 0.72912\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 112ms/step - accuracy: 0.8432 - loss: 0.4754 - val_accuracy: 0.7169 - val_loss: 0.7700 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8303 - loss: 0.4627\n",
      "Epoch 28: val_accuracy did not improve from 0.72912\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.8303 - loss: 0.4626 - val_accuracy: 0.7189 - val_loss: 0.7857 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8302 - loss: 0.4684\n",
      "Epoch 29: val_accuracy improved from 0.72912 to 0.74949, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.8305 - loss: 0.4680 - val_accuracy: 0.7495 - val_loss: 0.6984 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8674 - loss: 0.4110\n",
      "Epoch 30: val_accuracy did not improve from 0.74949\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.8670 - loss: 0.4115 - val_accuracy: 0.6904 - val_loss: 0.8424 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8600 - loss: 0.4178\n",
      "Epoch 31: val_accuracy did not improve from 0.74949\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 124ms/step - accuracy: 0.8599 - loss: 0.4180 - val_accuracy: 0.7189 - val_loss: 0.7479 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8427 - loss: 0.4344\n",
      "Epoch 32: val_accuracy did not improve from 0.74949\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.8429 - loss: 0.4340 - val_accuracy: 0.7067 - val_loss: 0.7721 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8714 - loss: 0.3943\n",
      "Epoch 33: val_accuracy did not improve from 0.74949\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.8711 - loss: 0.3947 - val_accuracy: 0.7352 - val_loss: 0.7178 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8737 - loss: 0.3805\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.74949\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.8734 - loss: 0.3809 - val_accuracy: 0.7373 - val_loss: 0.7364 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8678 - loss: 0.3855\n",
      "Epoch 35: val_accuracy improved from 0.74949 to 0.76375, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.8681 - loss: 0.3850 - val_accuracy: 0.7637 - val_loss: 0.6568 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8728 - loss: 0.3540\n",
      "Epoch 36: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 109ms/step - accuracy: 0.8729 - loss: 0.3541 - val_accuracy: 0.7434 - val_loss: 0.7538 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8843 - loss: 0.3456\n",
      "Epoch 37: val_accuracy did not improve from 0.76375\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.8844 - loss: 0.3456 - val_accuracy: 0.7515 - val_loss: 0.6961 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8835 - loss: 0.3230\n",
      "Epoch 38: val_accuracy improved from 0.76375 to 0.76782, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.8835 - loss: 0.3232 - val_accuracy: 0.7678 - val_loss: 0.6701 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8938 - loss: 0.3133\n",
      "Epoch 39: val_accuracy did not improve from 0.76782\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 113ms/step - accuracy: 0.8938 - loss: 0.3134 - val_accuracy: 0.7678 - val_loss: 0.6227 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8789 - loss: 0.3233\n",
      "Epoch 40: val_accuracy improved from 0.76782 to 0.78411, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8788 - loss: 0.3238 - val_accuracy: 0.7841 - val_loss: 0.6522 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8822 - loss: 0.3570\n",
      "Epoch 41: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 113ms/step - accuracy: 0.8824 - loss: 0.3566 - val_accuracy: 0.7413 - val_loss: 0.7165 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8912 - loss: 0.3170 \n",
      "Epoch 42: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.8912 - loss: 0.3168 - val_accuracy: 0.7658 - val_loss: 0.6559 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8928 - loss: 0.3090\n",
      "Epoch 43: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.8928 - loss: 0.3091 - val_accuracy: 0.7576 - val_loss: 0.6465 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8975 - loss: 0.3198\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.8974 - loss: 0.3199 - val_accuracy: 0.7699 - val_loss: 0.6403 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8923 - loss: 0.3023\n",
      "Epoch 45: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.8923 - loss: 0.3024 - val_accuracy: 0.7780 - val_loss: 0.6200 - learning_rate: 6.2500e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9041 - loss: 0.2882\n",
      "Epoch 46: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 112ms/step - accuracy: 0.9041 - loss: 0.2880 - val_accuracy: 0.7739 - val_loss: 0.5995 - learning_rate: 6.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9129 - loss: 0.2821\n",
      "Epoch 47: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9127 - loss: 0.2823 - val_accuracy: 0.7699 - val_loss: 0.6326 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8829 - loss: 0.3278\n",
      "Epoch 48: val_accuracy did not improve from 0.78411\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 144ms/step - accuracy: 0.8834 - loss: 0.3270 - val_accuracy: 0.7821 - val_loss: 0.6093 - learning_rate: 6.2500e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9156 - loss: 0.2815\n",
      "Epoch 49: val_accuracy improved from 0.78411 to 0.78615, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.9155 - loss: 0.2815 - val_accuracy: 0.7862 - val_loss: 0.5841 - learning_rate: 6.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m75/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9173 - loss: 0.2610\n",
      "Epoch 50: val_accuracy did not improve from 0.78615\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.9170 - loss: 0.2615 - val_accuracy: 0.7760 - val_loss: 0.6522 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "==================================================\n",
      "RESULTS FOR emotion_classifier_v6_enhanced:\n",
      "Train Accuracy: 0.9859\n",
      "Validation Accuracy: 0.7862\n",
      "==================================================\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step \n",
      "SUMMARY:\n",
      "Accuracy: 0.7862\n",
      "F1 (weighted): 0.7836\n",
      "F1 (macro): 0.7880\n",
      "F2 (weighted): 0.7846\n",
      "F2 (macro): 0.7907\n",
      "Precision: 0.7862\n",
      "Recall: 0.7862\n",
      "\n",
      "CLASS ACCURACY:\n",
      "angry: 0.8667\n",
      "calm: 0.8533\n",
      "disgust: 0.9231\n",
      "fearful: 0.7600\n",
      "happy: 0.7867\n",
      "neutral: 0.8684\n",
      "sad: 0.6000\n",
      "surprised: 0.6923\n",
      "\n",
      "DETAILED REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.86      0.87      0.86        75\n",
      "        calm       0.80      0.85      0.83        75\n",
      "     disgust       0.78      0.92      0.85        39\n",
      "     fearful       0.74      0.76      0.75        75\n",
      "       happy       0.80      0.79      0.79        75\n",
      "     neutral       0.75      0.87      0.80        38\n",
      "         sad       0.71      0.60      0.65        75\n",
      "   surprised       0.87      0.69      0.77        39\n",
      "\n",
      "    accuracy                           0.79       491\n",
      "   macro avg       0.79      0.79      0.79       491\n",
      "weighted avg       0.79      0.79      0.78       491\n",
      "\n",
      "Results saved to: emotion_classifier_v6_enhanced_results_20250625_013535.txt\n",
      "\n",
      "Output files:\n",
      "  - emotion_classifier_v6_enhanced_best.keras\n",
      "  - emotion_classifier_v6_enhanced_final.keras\n",
      "  - emotion_classifier_v6_enhanced_encoder.pkl\n",
      "  - emotion_classifier_v6_enhanced_results_*.txt\n"
     ]
    }
   ],
   "source": [
    "def run_analysis():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(f\"Starting analysis: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Version: {MODEL_VERSION}\")\n",
    "    print(f\"Type: {MODEL_TYPE}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU setup\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Data paths\n",
    "    speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_audio_data(speech_path, song_path)\n",
    "    if dataset is None:\n",
    "        print(\"Data loading failed. Check paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded {len(dataset)} samples\")\n",
    "    print(f\"Emotions: {dataset['emotion'].unique()}\")\n",
    "    print(f\"Distribution:\\n{dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_set, val_set = train_test_split(dataset, test_size=0.2, \n",
    "                                        random_state=42, stratify=dataset['emotion'])\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature extraction\n",
    "        print(\"\\n=== Processing training data ===\")\n",
    "        X_train, y_train = extract_features(train_set, use_augmentation=True)\n",
    "        \n",
    "        print(\"\\n=== Processing validation data ===\")\n",
    "        X_val, y_val = extract_features(val_set, use_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed.\")\n",
    "            return\n",
    "        \n",
    "        # Encode labels\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        y_val_encoded = encoder.transform(y_val)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train_encoded)\n",
    "        y_val_cat = to_categorical(y_val_encoded)\n",
    "        \n",
    "        print(\"\\nData statistics:\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Build model\n",
    "        model = build_classifier(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weights\n",
    "        weights = compute_class_weights(y_train_encoded)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\n=== Training: {EXPERIMENT_NAME} ===\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_val, y_val_cat),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=configure_callbacks(EXPERIMENT_NAME),\n",
    "            class_weight=weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best model\n",
    "        model = tf.keras.models.load_model(f'{EXPERIMENT_NAME}_best.keras')\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RESULTS FOR {EXPERIMENT_NAME}:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_val_encoded, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val_encoded, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class accuracy\n",
    "        cm = confusion_matrix(y_val_encoded, y_pred_classes)\n",
    "        class_acc = {}\n",
    "        for i, cls in enumerate(encoder.classes_):\n",
    "            class_acc[cls] = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_val_encoded, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display\n",
    "        print(\"SUMMARY:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 (weighted): {weighted_f1:.4f}\")\n",
    "        print(f\"F1 (macro): {macro_f1:.4f}\")\n",
    "        print(f\"F2 (weighted): {weighted_f2:.4f}\")\n",
    "        print(f\"F2 (macro): {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS ACCURACY:\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nDETAILED REPORT:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save\n",
    "        model.save(f'{EXPERIMENT_NAME}_final.keras')\n",
    "        with open(f'{EXPERIMENT_NAME}_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(history, EXPERIMENT_NAME, train_acc, val_acc,\n",
    "                   accuracy, macro_f1, weighted_f1,\n",
    "                   macro_f2, weighted_f2, precision, recall,\n",
    "                   class_acc, report)\n",
    "        \n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_best.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_final.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_encoder.pkl\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_results_*.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07cb57b8-02e3-48b7-a748-a7829417e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis: emotion_classifier_v6_enhanced\n",
      "Version: v6\n",
      "Type: enhanced\n",
      "==================================================\n",
      "\n",
      "Filtered dataset (no 'sad'): 2076 samples\n",
      "Remaining emotions: ['neutral' 'calm' 'happy' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "New distribution:\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 1660\n",
      "Validation set: 416\n",
      "\n",
      "=== Processing training data ===\n",
      "Processing 1660 audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to cache: feature_cache\\4dcba849bd4367d9bcf23e216cdca53a.npz\n",
      "Initial feature count: 1660\n",
      "Applying data augmentation...\n",
      "Adjusting class distribution:\n",
      "Current counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'surprised': 153}\n",
      "Target count: 301.0\n",
      "Augmented dataset size: 2107\n",
      "\n",
      "=== Processing validation data ===\n",
      "Processing 416 audio files...\n",
      "Saved features to cache: feature_cache\\599a1b9564e18f190676c4fe76cf1837.npz\n",
      "Initial feature count: 416\n",
      "\n",
      "Data statistics:\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'surprised']\n",
      "Input shape: (32, 94, 2)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=301, weight=1.00\n",
      "Class 1: count=301, weight=1.00\n",
      "Class 2: count=301, weight=1.00\n",
      "Class 3: count=301, weight=1.00\n",
      "Class 4: count=301, weight=1.00\n",
      "Class 5: count=301, weight=1.00\n",
      "Class 6: count=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,271\n",
      "\n",
      "=== Training: emotion_classifier_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1956 - loss: 2.0050\n",
      "Epoch 1: val_accuracy improved from -inf to 0.18990, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.1963 - loss: 2.0030 - val_accuracy: 0.1899 - val_loss: 1.9759 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3467 - loss: 1.6613\n",
      "Epoch 2: val_accuracy did not improve from 0.18990\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.3468 - loss: 1.6608 - val_accuracy: 0.1803 - val_loss: 2.9727 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4125 - loss: 1.4731\n",
      "Epoch 3: val_accuracy did not improve from 0.18990\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.4128 - loss: 1.4726 - val_accuracy: 0.1803 - val_loss: 4.6174 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4799 - loss: 1.3467\n",
      "Epoch 4: val_accuracy did not improve from 0.18990\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 0.4800 - loss: 1.3463 - val_accuracy: 0.1803 - val_loss: 5.0257 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5146 - loss: 1.2462\n",
      "Epoch 5: val_accuracy did not improve from 0.18990\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.5146 - loss: 1.2463 - val_accuracy: 0.1803 - val_loss: 4.4649 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5375 - loss: 1.1888\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.18990 to 0.21154, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - accuracy: 0.5376 - loss: 1.1887 - val_accuracy: 0.2115 - val_loss: 3.1650 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6003 - loss: 1.0715\n",
      "Epoch 7: val_accuracy did not improve from 0.21154\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.6002 - loss: 1.0715 - val_accuracy: 0.1851 - val_loss: 4.0321 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5933 - loss: 1.0323\n",
      "Epoch 8: val_accuracy improved from 0.21154 to 0.28125, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - accuracy: 0.5936 - loss: 1.0318 - val_accuracy: 0.2812 - val_loss: 2.1568 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6446 - loss: 0.9506\n",
      "Epoch 9: val_accuracy improved from 0.28125 to 0.32212, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.6446 - loss: 0.9505 - val_accuracy: 0.3221 - val_loss: 1.7267 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6569 - loss: 0.9036\n",
      "Epoch 10: val_accuracy improved from 0.32212 to 0.50240, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 0.6570 - loss: 0.9033 - val_accuracy: 0.5024 - val_loss: 1.2906 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6896 - loss: 0.8433\n",
      "Epoch 11: val_accuracy improved from 0.50240 to 0.60337, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.6895 - loss: 0.8436 - val_accuracy: 0.6034 - val_loss: 0.9732 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6999 - loss: 0.7935\n",
      "Epoch 12: val_accuracy did not improve from 0.60337\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.6998 - loss: 0.7938 - val_accuracy: 0.6034 - val_loss: 1.0793 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7050 - loss: 0.7869\n",
      "Epoch 13: val_accuracy did not improve from 0.60337\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.7051 - loss: 0.7867 - val_accuracy: 0.4856 - val_loss: 1.4772 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7302 - loss: 0.7363\n",
      "Epoch 14: val_accuracy improved from 0.60337 to 0.70913, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - accuracy: 0.7301 - loss: 0.7367 - val_accuracy: 0.7091 - val_loss: 0.8048 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7357 - loss: 0.7083\n",
      "Epoch 15: val_accuracy did not improve from 0.70913\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.7356 - loss: 0.7085 - val_accuracy: 0.5481 - val_loss: 1.1377 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7638 - loss: 0.6638\n",
      "Epoch 16: val_accuracy did not improve from 0.70913\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 105ms/step - accuracy: 0.7638 - loss: 0.6639 - val_accuracy: 0.6851 - val_loss: 0.8527 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.7549 - loss: 0.6731\n",
      "Epoch 17: val_accuracy did not improve from 0.70913\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 115ms/step - accuracy: 0.7550 - loss: 0.6728 - val_accuracy: 0.6803 - val_loss: 0.8425 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7820 - loss: 0.6023\n",
      "Epoch 18: val_accuracy did not improve from 0.70913\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - accuracy: 0.7819 - loss: 0.6025 - val_accuracy: 0.6298 - val_loss: 1.0444 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7945 - loss: 0.5943 \n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.70913\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 95ms/step - accuracy: 0.7943 - loss: 0.5944 - val_accuracy: 0.5337 - val_loss: 1.2971 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8121 - loss: 0.5185\n",
      "Epoch 20: val_accuracy improved from 0.70913 to 0.73798, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.8120 - loss: 0.5186 - val_accuracy: 0.7380 - val_loss: 0.7680 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8392 - loss: 0.4905\n",
      "Epoch 21: val_accuracy did not improve from 0.73798\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.8389 - loss: 0.4910 - val_accuracy: 0.6971 - val_loss: 0.7717 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8197 - loss: 0.5122\n",
      "Epoch 22: val_accuracy improved from 0.73798 to 0.74760, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8197 - loss: 0.5122 - val_accuracy: 0.7476 - val_loss: 0.6823 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8320 - loss: 0.4820\n",
      "Epoch 23: val_accuracy improved from 0.74760 to 0.76442, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.8320 - loss: 0.4822 - val_accuracy: 0.7644 - val_loss: 0.6510 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8277 - loss: 0.4806\n",
      "Epoch 24: val_accuracy improved from 0.76442 to 0.76923, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8279 - loss: 0.4804 - val_accuracy: 0.7692 - val_loss: 0.6416 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8278 - loss: 0.4866\n",
      "Epoch 25: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8279 - loss: 0.4862 - val_accuracy: 0.7692 - val_loss: 0.6150 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8415 - loss: 0.4289\n",
      "Epoch 26: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.8417 - loss: 0.4290 - val_accuracy: 0.6659 - val_loss: 0.8571 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8671 - loss: 0.4098\n",
      "Epoch 27: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8670 - loss: 0.4099 - val_accuracy: 0.7308 - val_loss: 0.7181 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8526 - loss: 0.4116\n",
      "Epoch 28: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8526 - loss: 0.4119 - val_accuracy: 0.7404 - val_loss: 0.6396 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8578 - loss: 0.3969 \n",
      "Epoch 29: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 93ms/step - accuracy: 0.8579 - loss: 0.3967 - val_accuracy: 0.6731 - val_loss: 0.9648 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8640 - loss: 0.3858\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.76923\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8638 - loss: 0.3862 - val_accuracy: 0.7644 - val_loss: 0.6335 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8792 - loss: 0.3682\n",
      "Epoch 31: val_accuracy improved from 0.76923 to 0.78365, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.8792 - loss: 0.3681 - val_accuracy: 0.7837 - val_loss: 0.5984 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8685 - loss: 0.3723\n",
      "Epoch 32: val_accuracy did not improve from 0.78365\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8685 - loss: 0.3722 - val_accuracy: 0.7644 - val_loss: 0.6197 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8821 - loss: 0.3464\n",
      "Epoch 33: val_accuracy improved from 0.78365 to 0.81250, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8821 - loss: 0.3465 - val_accuracy: 0.8125 - val_loss: 0.5397 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8713 - loss: 0.3532\n",
      "Epoch 34: val_accuracy did not improve from 0.81250\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 92ms/step - accuracy: 0.8714 - loss: 0.3533 - val_accuracy: 0.7885 - val_loss: 0.5435 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8747 - loss: 0.3532\n",
      "Epoch 35: val_accuracy did not improve from 0.81250\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8749 - loss: 0.3530 - val_accuracy: 0.7957 - val_loss: 0.5331 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8822 - loss: 0.3418\n",
      "Epoch 36: val_accuracy improved from 0.81250 to 0.81490, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8822 - loss: 0.3415 - val_accuracy: 0.8149 - val_loss: 0.5357 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8741 - loss: 0.3529\n",
      "Epoch 37: val_accuracy did not improve from 0.81490\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.8742 - loss: 0.3528 - val_accuracy: 0.7476 - val_loss: 0.6796 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8693 - loss: 0.3532\n",
      "Epoch 38: val_accuracy did not improve from 0.81490\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8695 - loss: 0.3530 - val_accuracy: 0.7284 - val_loss: 0.6964 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8892 - loss: 0.3309\n",
      "Epoch 39: val_accuracy did not improve from 0.81490\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - accuracy: 0.8893 - loss: 0.3305 - val_accuracy: 0.7812 - val_loss: 0.5702 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8844 - loss: 0.3289 \n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.81490\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 94ms/step - accuracy: 0.8846 - loss: 0.3285 - val_accuracy: 0.7861 - val_loss: 0.5432 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8915 - loss: 0.2981\n",
      "Epoch 41: val_accuracy improved from 0.81490 to 0.81971, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8915 - loss: 0.2984 - val_accuracy: 0.8197 - val_loss: 0.5227 - learning_rate: 6.2500e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9164 - loss: 0.2590\n",
      "Epoch 42: val_accuracy improved from 0.81971 to 0.82452, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 95ms/step - accuracy: 0.9163 - loss: 0.2593 - val_accuracy: 0.8245 - val_loss: 0.5002 - learning_rate: 6.2500e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9023 - loss: 0.2936\n",
      "Epoch 43: val_accuracy did not improve from 0.82452\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.9023 - loss: 0.2936 - val_accuracy: 0.8149 - val_loss: 0.5062 - learning_rate: 6.2500e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9037 - loss: 0.2928\n",
      "Epoch 44: val_accuracy did not improve from 0.82452\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.9037 - loss: 0.2927 - val_accuracy: 0.8125 - val_loss: 0.5198 - learning_rate: 6.2500e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8979 - loss: 0.2982\n",
      "Epoch 45: val_accuracy did not improve from 0.82452\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8979 - loss: 0.2983 - val_accuracy: 0.8077 - val_loss: 0.5165 - learning_rate: 6.2500e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9078 - loss: 0.2922\n",
      "Epoch 46: val_accuracy did not improve from 0.82452\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9078 - loss: 0.2919 - val_accuracy: 0.8221 - val_loss: 0.5021 - learning_rate: 6.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9122 - loss: 0.2875\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.82452\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.9122 - loss: 0.2874 - val_accuracy: 0.8101 - val_loss: 0.5353 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9187 - loss: 0.2549\n",
      "Epoch 48: val_accuracy improved from 0.82452 to 0.82692, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.9187 - loss: 0.2549 - val_accuracy: 0.8269 - val_loss: 0.4882 - learning_rate: 3.1250e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8911 - loss: 0.2895\n",
      "Epoch 49: val_accuracy improved from 0.82692 to 0.83413, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8913 - loss: 0.2893 - val_accuracy: 0.8341 - val_loss: 0.4888 - learning_rate: 3.1250e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9150 - loss: 0.2636\n",
      "Epoch 50: val_accuracy did not improve from 0.83413\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.9148 - loss: 0.2637 - val_accuracy: 0.8197 - val_loss: 0.5008 - learning_rate: 3.1250e-05\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "==================================================\n",
      "RESULTS FOR emotion_classifier_v6_enhanced:\n",
      "Train Accuracy: 0.9820\n",
      "Validation Accuracy: 0.8341\n",
      "==================================================\n",
      "\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step \n",
      "SUMMARY:\n",
      "Accuracy: 0.8341\n",
      "F1 (weighted): 0.8326\n",
      "F1 (macro): 0.8316\n",
      "F2 (weighted): 0.8328\n",
      "F2 (macro): 0.8349\n",
      "Precision: 0.8368\n",
      "Recall: 0.8341\n",
      "\n",
      "CLASS ACCURACY:\n",
      "angry: 0.8933\n",
      "calm: 0.8667\n",
      "disgust: 0.8718\n",
      "fearful: 0.8800\n",
      "happy: 0.6667\n",
      "neutral: 0.9211\n",
      "surprised: 0.7692\n",
      "\n",
      "DETAILED REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.84      0.89      0.86        75\n",
      "        calm       0.93      0.87      0.90        75\n",
      "     disgust       0.83      0.87      0.85        39\n",
      "     fearful       0.81      0.88      0.85        75\n",
      "       happy       0.81      0.67      0.73        75\n",
      "     neutral       0.74      0.92      0.82        38\n",
      "   surprised       0.86      0.77      0.81        39\n",
      "\n",
      "    accuracy                           0.83       416\n",
      "   macro avg       0.83      0.84      0.83       416\n",
      "weighted avg       0.84      0.83      0.83       416\n",
      "\n",
      "Results saved to: emotion_classifier_v6_enhanced_results_20250625_015233.txt\n",
      "\n",
      "Output files:\n",
      "  - emotion_classifier_v6_enhanced_best.keras\n",
      "  - emotion_classifier_v6_enhanced_final.keras\n",
      "  - emotion_classifier_v6_enhanced_encoder.pkl\n",
      "  - emotion_classifier_v6_enhanced_results_*.txt\n"
     ]
    }
   ],
   "source": [
    "def run_analysis():\n",
    "    \"\"\"Main execution function (modified to exclude 'sad' class)\"\"\"\n",
    "    print(f\"Starting analysis: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Version: {MODEL_VERSION}\")\n",
    "    print(f\"Type: {MODEL_TYPE}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU setup (unchanged)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Load data (unchanged)\n",
    "    speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    dataset = load_audio_data(speech_path, song_path)\n",
    "    if dataset is None:\n",
    "        print(\"Data loading failed. Check paths.\")\n",
    "        return\n",
    "    \n",
    "    # ==== NEW: Filter out \"sad\" samples ====\n",
    "    dataset = dataset[dataset['emotion'] != 'sad']  # Drop \"sad\" class\n",
    "    print(f\"\\nFiltered dataset (no 'sad'): {len(dataset)} samples\")\n",
    "    print(f\"Remaining emotions: {dataset['emotion'].unique()}\")\n",
    "    print(f\"New distribution:\\n{dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data (stratify on modified emotion distribution)\n",
    "    train_set, val_set = train_test_split(\n",
    "        dataset, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=dataset['emotion']  # Now excludes \"sad\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature extraction (unchanged)\n",
    "        print(\"\\n=== Processing training data ===\")\n",
    "        X_train, y_train = extract_features(train_set, use_augmentation=True)\n",
    "        print(\"\\n=== Processing validation data ===\")\n",
    "        X_val, y_val = extract_features(val_set, use_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed.\")\n",
    "            return\n",
    "        \n",
    "        # Encode labels (now excludes \"sad\")\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        y_val_encoded = encoder.transform(y_val)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train_encoded)\n",
    "        y_val_cat = to_categorical(y_val_encoded)\n",
    "        \n",
    "        print(\"\\nData statistics:\")\n",
    "        print(f\"Classes: {encoder.classes_}\")  # Verify \"sad\" is missing\n",
    "        print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Build model (adjust output layer size automatically)\n",
    "        model = build_classifier(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weights (recomputed without \"sad\")\n",
    "        weights = compute_class_weights(y_train_encoded)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\n=== Training: {EXPERIMENT_NAME} ===\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_val, y_val_cat),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=configure_callbacks(EXPERIMENT_NAME),\n",
    "            class_weight=weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best model\n",
    "        model = tf.keras.models.load_model(f'{EXPERIMENT_NAME}_best.keras')\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RESULTS FOR {EXPERIMENT_NAME}:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_val_encoded, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val_encoded, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class accuracy\n",
    "        cm = confusion_matrix(y_val_encoded, y_pred_classes)\n",
    "        class_acc = {}\n",
    "        for i, cls in enumerate(encoder.classes_):\n",
    "            class_acc[cls] = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_val_encoded, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display\n",
    "        print(\"SUMMARY:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 (weighted): {weighted_f1:.4f}\")\n",
    "        print(f\"F1 (macro): {macro_f1:.4f}\")\n",
    "        print(f\"F2 (weighted): {weighted_f2:.4f}\")\n",
    "        print(f\"F2 (macro): {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS ACCURACY:\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nDETAILED REPORT:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save\n",
    "        model.save(f'{EXPERIMENT_NAME}_final.keras')\n",
    "        with open(f'{EXPERIMENT_NAME}_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(history, EXPERIMENT_NAME, train_acc, val_acc,\n",
    "                   accuracy, macro_f1, weighted_f1,\n",
    "                   macro_f2, weighted_f2, precision, recall,\n",
    "                   class_acc, report)\n",
    "        \n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_best.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_final.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_encoder.pkl\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_results_*.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9251afa6-c0fe-422e-887e-beae10df832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis: emotion_classifier_v6_enhanced\n",
      "Version: v6\n",
      "Type: enhanced\n",
      "==================================================\n",
      "\n",
      "Filtered dataset (no 'surprised'): 2260 samples\n",
      "Remaining emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n",
      "New distribution:\n",
      "emotion\n",
      "calm       376\n",
      "happy      376\n",
      "sad        376\n",
      "angry      376\n",
      "fearful    376\n",
      "disgust    192\n",
      "neutral    188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 1808\n",
      "Validation set: 452\n",
      "\n",
      "=== Processing training data ===\n",
      "Processing 1808 audio files...\n",
      "Saved features to cache: feature_cache\\617f83983e1246b1e4d46a92c0e803ec.npz\n",
      "Initial feature count: 1808\n",
      "Applying data augmentation...\n",
      "Adjusting class distribution:\n",
      "Current counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150, 'sad': 301}\n",
      "Target count: 301.0\n",
      "Augmented dataset size: 2107\n",
      "\n",
      "=== Processing validation data ===\n",
      "Processing 452 audio files...\n",
      "Saved features to cache: feature_cache\\19fce4620f2d5b103a6c02ecf46035ec.npz\n",
      "Initial feature count: 452\n",
      "\n",
      "Data statistics:\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad']\n",
      "Input shape: (32, 94, 2)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=301, weight=1.00\n",
      "Class 1: count=301, weight=1.00\n",
      "Class 2: count=301, weight=1.00\n",
      "Class 3: count=301, weight=1.00\n",
      "Class 4: count=301, weight=1.00\n",
      "Class 5: count=301, weight=1.00\n",
      "Class 6: count=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,271\n",
      "\n",
      "=== Training: emotion_classifier_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.1743 - loss: 2.0894\n",
      "Epoch 1: val_accuracy improved from -inf to 0.08628, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - accuracy: 0.1749 - loss: 2.0870 - val_accuracy: 0.0863 - val_loss: 1.9645 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3095 - loss: 1.6848\n",
      "Epoch 2: val_accuracy improved from 0.08628 to 0.16593, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.3101 - loss: 1.6840 - val_accuracy: 0.1659 - val_loss: 2.0880 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3752 - loss: 1.5440\n",
      "Epoch 3: val_accuracy improved from 0.16593 to 0.16814, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.3755 - loss: 1.5436 - val_accuracy: 0.1681 - val_loss: 2.2810 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.4244 - loss: 1.4210\n",
      "Epoch 4: val_accuracy improved from 0.16814 to 0.22124, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.4246 - loss: 1.4209 - val_accuracy: 0.2212 - val_loss: 2.4778 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.4818 - loss: 1.3130\n",
      "Epoch 5: val_accuracy improved from 0.22124 to 0.22345, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.4820 - loss: 1.3126 - val_accuracy: 0.2235 - val_loss: 2.0653 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5171 - loss: 1.2475\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.22345\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.5174 - loss: 1.2470 - val_accuracy: 0.1991 - val_loss: 3.1955 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5985 - loss: 1.0972\n",
      "Epoch 7: val_accuracy improved from 0.22345 to 0.37168, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.5987 - loss: 1.0970 - val_accuracy: 0.3717 - val_loss: 1.5584 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5997 - loss: 1.0287\n",
      "Epoch 8: val_accuracy did not improve from 0.37168\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.5999 - loss: 1.0286 - val_accuracy: 0.1881 - val_loss: 2.7910 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6263 - loss: 1.0129\n",
      "Epoch 9: val_accuracy improved from 0.37168 to 0.51106, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.6264 - loss: 1.0124 - val_accuracy: 0.5111 - val_loss: 1.3021 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6725 - loss: 0.9337\n",
      "Epoch 10: val_accuracy improved from 0.51106 to 0.58850, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.6723 - loss: 0.9338 - val_accuracy: 0.5885 - val_loss: 1.0935 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6405 - loss: 0.9264\n",
      "Epoch 11: val_accuracy did not improve from 0.58850\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.6407 - loss: 0.9262 - val_accuracy: 0.5509 - val_loss: 1.1287 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6704 - loss: 0.8714\n",
      "Epoch 12: val_accuracy improved from 0.58850 to 0.59956, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.6705 - loss: 0.8713 - val_accuracy: 0.5996 - val_loss: 1.0757 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7048 - loss: 0.8190\n",
      "Epoch 13: val_accuracy improved from 0.59956 to 0.63274, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.7047 - loss: 0.8194 - val_accuracy: 0.6327 - val_loss: 0.9653 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7018 - loss: 0.7984\n",
      "Epoch 14: val_accuracy did not improve from 0.63274\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.7018 - loss: 0.7985 - val_accuracy: 0.6150 - val_loss: 1.0032 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7402 - loss: 0.7361\n",
      "Epoch 15: val_accuracy did not improve from 0.63274\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.7399 - loss: 0.7365 - val_accuracy: 0.4757 - val_loss: 1.4604 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7093 - loss: 0.7703\n",
      "Epoch 16: val_accuracy did not improve from 0.63274\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7096 - loss: 0.7700 - val_accuracy: 0.6040 - val_loss: 1.1015 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7158 - loss: 0.7150\n",
      "Epoch 17: val_accuracy improved from 0.63274 to 0.67920, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7160 - loss: 0.7150 - val_accuracy: 0.6792 - val_loss: 0.8560 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7298 - loss: 0.7270\n",
      "Epoch 18: val_accuracy improved from 0.67920 to 0.68805, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - accuracy: 0.7298 - loss: 0.7268 - val_accuracy: 0.6881 - val_loss: 0.8929 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7532 - loss: 0.6674\n",
      "Epoch 19: val_accuracy did not improve from 0.68805\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.7532 - loss: 0.6674 - val_accuracy: 0.6726 - val_loss: 0.8888 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7731 - loss: 0.6417\n",
      "Epoch 20: val_accuracy did not improve from 0.68805\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - accuracy: 0.7731 - loss: 0.6417 - val_accuracy: 0.6261 - val_loss: 1.0067 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7850 - loss: 0.6087\n",
      "Epoch 21: val_accuracy did not improve from 0.68805\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - accuracy: 0.7849 - loss: 0.6088 - val_accuracy: 0.6482 - val_loss: 0.8572 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7777 - loss: 0.6189\n",
      "Epoch 22: val_accuracy improved from 0.68805 to 0.73009, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.7777 - loss: 0.6186 - val_accuracy: 0.7301 - val_loss: 0.7789 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7883 - loss: 0.5659\n",
      "Epoch 23: val_accuracy did not improve from 0.73009\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.7882 - loss: 0.5661 - val_accuracy: 0.6549 - val_loss: 0.9125 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8046 - loss: 0.5350\n",
      "Epoch 24: val_accuracy did not improve from 0.73009\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.8046 - loss: 0.5351 - val_accuracy: 0.6018 - val_loss: 0.9806 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8050 - loss: 0.5662\n",
      "Epoch 25: val_accuracy did not improve from 0.73009\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.8052 - loss: 0.5654 - val_accuracy: 0.6040 - val_loss: 1.0571 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8309 - loss: 0.4881\n",
      "Epoch 26: val_accuracy improved from 0.73009 to 0.75221, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.8309 - loss: 0.4883 - val_accuracy: 0.7522 - val_loss: 0.6845 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8459 - loss: 0.4403\n",
      "Epoch 27: val_accuracy did not improve from 0.75221\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8456 - loss: 0.4408 - val_accuracy: 0.6305 - val_loss: 0.9695 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8446 - loss: 0.4476\n",
      "Epoch 28: val_accuracy did not improve from 0.75221\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8444 - loss: 0.4480 - val_accuracy: 0.6726 - val_loss: 0.7970 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8558 - loss: 0.4287\n",
      "Epoch 29: val_accuracy did not improve from 0.75221\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.8556 - loss: 0.4291 - val_accuracy: 0.6173 - val_loss: 1.0449 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8368 - loss: 0.4435\n",
      "Epoch 30: val_accuracy improved from 0.75221 to 0.75442, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - accuracy: 0.8368 - loss: 0.4436 - val_accuracy: 0.7544 - val_loss: 0.7070 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8507 - loss: 0.4180\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.75442\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8508 - loss: 0.4179 - val_accuracy: 0.5996 - val_loss: 1.1884 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8587 - loss: 0.3866\n",
      "Epoch 32: val_accuracy improved from 0.75442 to 0.78319, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.8587 - loss: 0.3864 - val_accuracy: 0.7832 - val_loss: 0.6291 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8933 - loss: 0.3250\n",
      "Epoch 33: val_accuracy did not improve from 0.78319\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.8931 - loss: 0.3253 - val_accuracy: 0.7500 - val_loss: 0.6266 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8818 - loss: 0.3320\n",
      "Epoch 34: val_accuracy did not improve from 0.78319\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.8818 - loss: 0.3321 - val_accuracy: 0.7478 - val_loss: 0.6899 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8813 - loss: 0.3380\n",
      "Epoch 35: val_accuracy did not improve from 0.78319\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.8814 - loss: 0.3379 - val_accuracy: 0.7124 - val_loss: 0.7679 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8696 - loss: 0.3562\n",
      "Epoch 36: val_accuracy did not improve from 0.78319\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - accuracy: 0.8698 - loss: 0.3557 - val_accuracy: 0.7389 - val_loss: 0.7101 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8982 - loss: 0.2819\n",
      "Epoch 37: val_accuracy improved from 0.78319 to 0.78540, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8981 - loss: 0.2821 - val_accuracy: 0.7854 - val_loss: 0.5787 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9125 - loss: 0.2862\n",
      "Epoch 38: val_accuracy did not improve from 0.78540\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9124 - loss: 0.2862 - val_accuracy: 0.7655 - val_loss: 0.6417 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8675 - loss: 0.3405\n",
      "Epoch 39: val_accuracy did not improve from 0.78540\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.8678 - loss: 0.3401 - val_accuracy: 0.7146 - val_loss: 0.6996 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9227 - loss: 0.2355\n",
      "Epoch 40: val_accuracy improved from 0.78540 to 0.80088, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 0.9223 - loss: 0.2360 - val_accuracy: 0.8009 - val_loss: 0.5927 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9035 - loss: 0.2749\n",
      "Epoch 41: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9035 - loss: 0.2748 - val_accuracy: 0.7522 - val_loss: 0.6613 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9117 - loss: 0.2479\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9117 - loss: 0.2481 - val_accuracy: 0.7279 - val_loss: 0.6919 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8992 - loss: 0.2727\n",
      "Epoch 43: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.8993 - loss: 0.2726 - val_accuracy: 0.7522 - val_loss: 0.6719 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9115 - loss: 0.2493\n",
      "Epoch 44: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - accuracy: 0.9115 - loss: 0.2493 - val_accuracy: 0.7920 - val_loss: 0.6124 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9215 - loss: 0.2437\n",
      "Epoch 45: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - accuracy: 0.9215 - loss: 0.2437 - val_accuracy: 0.7965 - val_loss: 0.5554 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9219 - loss: 0.2417\n",
      "Epoch 46: val_accuracy did not improve from 0.80088\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.9219 - loss: 0.2416 - val_accuracy: 0.7788 - val_loss: 0.5932 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9240 - loss: 0.2303\n",
      "Epoch 47: val_accuracy improved from 0.80088 to 0.80752, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.9240 - loss: 0.2303 - val_accuracy: 0.8075 - val_loss: 0.5335 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9329 - loss: 0.2146\n",
      "Epoch 48: val_accuracy did not improve from 0.80752\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9328 - loss: 0.2148 - val_accuracy: 0.7765 - val_loss: 0.5726 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9264 - loss: 0.2140\n",
      "Epoch 49: val_accuracy did not improve from 0.80752\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9263 - loss: 0.2143 - val_accuracy: 0.8031 - val_loss: 0.5396 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9201 - loss: 0.2172\n",
      "Epoch 50: val_accuracy did not improve from 0.80752\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.9201 - loss: 0.2171 - val_accuracy: 0.6748 - val_loss: 0.8727 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "==================================================\n",
      "RESULTS FOR emotion_classifier_v6_enhanced:\n",
      "Train Accuracy: 0.9967\n",
      "Validation Accuracy: 0.8075\n",
      "==================================================\n",
      "\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step \n",
      "SUMMARY:\n",
      "Accuracy: 0.8075\n",
      "F1 (weighted): 0.8043\n",
      "F1 (macro): 0.8025\n",
      "F2 (weighted): 0.8054\n",
      "F2 (macro): 0.8013\n",
      "Precision: 0.8083\n",
      "Recall: 0.8075\n",
      "\n",
      "CLASS ACCURACY:\n",
      "angry: 0.9467\n",
      "calm: 0.8400\n",
      "disgust: 0.6923\n",
      "fearful: 0.7867\n",
      "happy: 0.8800\n",
      "neutral: 0.8421\n",
      "sad: 0.6267\n",
      "\n",
      "DETAILED REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.83      0.95      0.88        75\n",
      "        calm       0.83      0.84      0.83        75\n",
      "     disgust       0.90      0.69      0.78        39\n",
      "     fearful       0.80      0.79      0.79        75\n",
      "       happy       0.80      0.88      0.84        75\n",
      "     neutral       0.76      0.84      0.80        38\n",
      "         sad       0.77      0.63      0.69        75\n",
      "\n",
      "    accuracy                           0.81       452\n",
      "   macro avg       0.81      0.80      0.80       452\n",
      "weighted avg       0.81      0.81      0.80       452\n",
      "\n",
      "Results saved to: emotion_classifier_v6_enhanced_results_20250625_020159.txt\n",
      "\n",
      "Output files:\n",
      "  - emotion_classifier_v6_enhanced_best.keras\n",
      "  - emotion_classifier_v6_enhanced_final.keras\n",
      "  - emotion_classifier_v6_enhanced_encoder.pkl\n",
      "  - emotion_classifier_v6_enhanced_results_*.txt\n"
     ]
    }
   ],
   "source": [
    "def run_analysis():\n",
    "    \"\"\"Main execution function (modified to exclude 'surprised' class)\"\"\"\n",
    "    print(f\"Starting analysis: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Version: {MODEL_VERSION}\")\n",
    "    print(f\"Type: {MODEL_TYPE}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # GPU setup (unchanged)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Load data (unchanged)\n",
    "    speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    dataset = load_audio_data(speech_path, song_path)\n",
    "    if dataset is None:\n",
    "        print(\"Data loading failed. Check paths.\")\n",
    "        return\n",
    "    \n",
    "    # ==== KEY CHANGE: Filter out \"surprised\" samples ====\n",
    "    dataset = dataset[dataset['emotion'] != 'surprised']  # Drop \"surprised\" class\n",
    "    print(f\"\\nFiltered dataset (no 'surprised'): {len(dataset)} samples\")\n",
    "    print(f\"Remaining emotions: {dataset['emotion'].unique()}\")\n",
    "    print(f\"New distribution:\\n{dataset['emotion'].value_counts()}\")\n",
    "    \n",
    "    # Split data (stratify on modified emotion distribution)\n",
    "    train_set, val_set = train_test_split(\n",
    "        dataset, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=dataset['emotion']  # Now excludes \"surprised\"\n",
    "    )\n",
    "    \n",
    "    # Rest of your code remains EXACTLY THE SAME from here...\n",
    "    # Only the dataset filtering above changes\n",
    "    print(f\"\\nTraining set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "    \n",
    "    try:\n",
    "        # Feature extraction (unchanged)\n",
    "        print(\"\\n=== Processing training data ===\")\n",
    "        X_train, y_train = extract_features(train_set, use_augmentation=True)\n",
    "        print(\"\\n=== Processing validation data ===\")\n",
    "        X_val, y_val = extract_features(val_set, use_augmentation=False)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed.\")\n",
    "            return\n",
    "        \n",
    "        # Encode labels (now excludes \"surprised\")\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        y_val_encoded = encoder.transform(y_val)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train_encoded)\n",
    "        y_val_cat = to_categorical(y_val_encoded)\n",
    "        \n",
    "        print(\"\\nData statistics:\")\n",
    "        print(f\"Classes: {encoder.classes_}\")  # Verify \"surprised\" is missing\n",
    "        print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "        \n",
    "        # Build model (output layer auto-adjusts to remaining classes)\n",
    "        model = build_classifier(X_train.shape[1:], len(encoder.classes_))\n",
    "        \n",
    "        # Class weights (recomputed without \"surprised\")\n",
    "        weights = compute_class_weights(y_train_encoded)\n",
    "        \n",
    "       # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\n=== Training: {EXPERIMENT_NAME} ===\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_val, y_val_cat),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=configure_callbacks(EXPERIMENT_NAME),\n",
    "            class_weight=weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Load best model\n",
    "        model = tf.keras.models.load_model(f'{EXPERIMENT_NAME}_best.keras')\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RESULTS FOR {EXPERIMENT_NAME}:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_val_encoded, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val_encoded, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class accuracy\n",
    "        cm = confusion_matrix(y_val_encoded, y_pred_classes)\n",
    "        class_acc = {}\n",
    "        for i, cls in enumerate(encoder.classes_):\n",
    "            class_acc[cls] = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_val_encoded, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display\n",
    "        print(\"SUMMARY:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 (weighted): {weighted_f1:.4f}\")\n",
    "        print(f\"F1 (macro): {macro_f1:.4f}\")\n",
    "        print(f\"F2 (weighted): {weighted_f2:.4f}\")\n",
    "        print(f\"F2 (macro): {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS ACCURACY:\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nDETAILED REPORT:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save\n",
    "        model.save(f'{EXPERIMENT_NAME}_final.keras')\n",
    "        with open(f'{EXPERIMENT_NAME}_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder, f)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(history, EXPERIMENT_NAME, train_acc, val_acc,\n",
    "                   accuracy, macro_f1, weighted_f1,\n",
    "                   macro_f2, weighted_f2, precision, recall,\n",
    "                   class_acc, report)\n",
    "        \n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_best.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_final.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_encoder.pkl\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_results_*.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a78fbbb2-b72c-4f1c-80c9-89c50b735462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis: emotion_classifier_v6_enhanced\n",
      "Version: v6\n",
      "Type: enhanced\n",
      "==================================================\n",
      "\n",
      "Filtered dataset (no 'surprised' or 'sad'): 1884 samples\n",
      "Remaining emotions: ['neutral' 'calm' 'happy' 'angry' 'fearful' 'disgust']\n",
      "New distribution:\n",
      "emotion\n",
      "calm       376\n",
      "happy      376\n",
      "angry      376\n",
      "fearful    376\n",
      "disgust    192\n",
      "neutral    188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 1507\n",
      "Validation set: 377\n",
      "\n",
      "=== Processing training data ===\n",
      "Loading features from cache: feature_cache\\f80ddd2e3115295cee0c1082d4d66e6f.npz\n",
      "Initial feature count: 1507\n",
      "Applying data augmentation...\n",
      "Adjusting class distribution:\n",
      "Current counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150}\n",
      "Target count: 301.0\n",
      "Augmented dataset size: 1806\n",
      "\n",
      "=== Processing validation data ===\n",
      "Loading features from cache: feature_cache\\ae3fc77cd47ff840a8f72891c3e3c6d5.npz\n",
      "Initial feature count: 377\n",
      "\n",
      "Data statistics:\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral']\n",
      "Input shape: (32, 94, 2)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=301, weight=1.00\n",
      "Class 1: count=301, weight=1.00\n",
      "Class 2: count=301, weight=1.00\n",
      "Class 3: count=301, weight=1.00\n",
      "Class 4: count=301, weight=1.00\n",
      "Class 5: count=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,142\n",
      "\n",
      "=== Training: emotion_classifier_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2274 - loss: 1.8246\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26525, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2287 - loss: 1.8211 - val_accuracy: 0.2653 - val_loss: 1.7557 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4034 - loss: 1.4895\n",
      "Epoch 2: val_accuracy improved from 0.26525 to 0.28647, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.4038 - loss: 1.4884 - val_accuracy: 0.2865 - val_loss: 1.7572 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4623 - loss: 1.3358\n",
      "Epoch 3: val_accuracy did not improve from 0.28647\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.4630 - loss: 1.3347 - val_accuracy: 0.2467 - val_loss: 1.7786 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4955 - loss: 1.2336\n",
      "Epoch 4: val_accuracy did not improve from 0.28647\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.4961 - loss: 1.2324 - val_accuracy: 0.2387 - val_loss: 1.7183 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5297 - loss: 1.1402\n",
      "Epoch 5: val_accuracy improved from 0.28647 to 0.42706, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.5301 - loss: 1.1396 - val_accuracy: 0.4271 - val_loss: 1.5444 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5742 - loss: 1.0750\n",
      "Epoch 6: val_accuracy did not improve from 0.42706\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.5742 - loss: 1.0754 - val_accuracy: 0.3210 - val_loss: 1.6555 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5907 - loss: 1.0178\n",
      "Epoch 7: val_accuracy improved from 0.42706 to 0.47745, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.5916 - loss: 1.0177 - val_accuracy: 0.4775 - val_loss: 1.2811 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6306 - loss: 0.9456\n",
      "Epoch 8: val_accuracy did not improve from 0.47745\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - accuracy: 0.6312 - loss: 0.9450 - val_accuracy: 0.3634 - val_loss: 1.7802 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6566 - loss: 0.9069\n",
      "Epoch 9: val_accuracy did not improve from 0.47745\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - accuracy: 0.6566 - loss: 0.9063 - val_accuracy: 0.4350 - val_loss: 1.3862 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6888 - loss: 0.8451\n",
      "Epoch 10: val_accuracy improved from 0.47745 to 0.58621, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.6885 - loss: 0.8453 - val_accuracy: 0.5862 - val_loss: 1.0926 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7100 - loss: 0.7846\n",
      "Epoch 11: val_accuracy improved from 0.58621 to 0.60743, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.7102 - loss: 0.7841 - val_accuracy: 0.6074 - val_loss: 1.0010 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6972 - loss: 0.7603\n",
      "Epoch 12: val_accuracy did not improve from 0.60743\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - accuracy: 0.6983 - loss: 0.7587 - val_accuracy: 0.5756 - val_loss: 1.0684 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7345 - loss: 0.6919\n",
      "Epoch 13: val_accuracy did not improve from 0.60743\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - accuracy: 0.7349 - loss: 0.6917 - val_accuracy: 0.4934 - val_loss: 1.2900 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7538 - loss: 0.6667\n",
      "Epoch 14: val_accuracy improved from 0.60743 to 0.68700, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.7541 - loss: 0.6660 - val_accuracy: 0.6870 - val_loss: 0.8513 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7425 - loss: 0.6609\n",
      "Epoch 15: val_accuracy did not improve from 0.68700\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.7433 - loss: 0.6605 - val_accuracy: 0.6419 - val_loss: 0.9257 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8022 - loss: 0.5593\n",
      "Epoch 16: val_accuracy did not improve from 0.68700\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8019 - loss: 0.5595 - val_accuracy: 0.5703 - val_loss: 1.3311 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8068 - loss: 0.5238\n",
      "Epoch 17: val_accuracy improved from 0.68700 to 0.72679, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - accuracy: 0.8068 - loss: 0.5237 - val_accuracy: 0.7268 - val_loss: 0.7451 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8157 - loss: 0.4980\n",
      "Epoch 18: val_accuracy did not improve from 0.72679\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.8158 - loss: 0.4980 - val_accuracy: 0.6419 - val_loss: 1.1006 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8261 - loss: 0.4663\n",
      "Epoch 19: val_accuracy did not improve from 0.72679\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8260 - loss: 0.4665 - val_accuracy: 0.6605 - val_loss: 0.8595 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8235 - loss: 0.4658\n",
      "Epoch 20: val_accuracy did not improve from 0.72679\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8238 - loss: 0.4657 - val_accuracy: 0.4430 - val_loss: 2.0463 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8283 - loss: 0.4477\n",
      "Epoch 21: val_accuracy improved from 0.72679 to 0.77719, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8283 - loss: 0.4477 - val_accuracy: 0.7772 - val_loss: 0.6275 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8599 - loss: 0.3987\n",
      "Epoch 22: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.8596 - loss: 0.3991 - val_accuracy: 0.6605 - val_loss: 0.8716 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8532 - loss: 0.4107\n",
      "Epoch 23: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.8530 - loss: 0.4108 - val_accuracy: 0.7719 - val_loss: 0.6409 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8800 - loss: 0.3547\n",
      "Epoch 24: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8797 - loss: 0.3552 - val_accuracy: 0.6790 - val_loss: 0.7901 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8562 - loss: 0.3965\n",
      "Epoch 25: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.8567 - loss: 0.3958 - val_accuracy: 0.7241 - val_loss: 0.6572 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8778 - loss: 0.3279\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.8775 - loss: 0.3289 - val_accuracy: 0.5889 - val_loss: 1.1875 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8881 - loss: 0.3115\n",
      "Epoch 27: val_accuracy improved from 0.77719 to 0.80637, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.8886 - loss: 0.3107 - val_accuracy: 0.8064 - val_loss: 0.5187 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9184 - loss: 0.2376\n",
      "Epoch 28: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.9184 - loss: 0.2379 - val_accuracy: 0.7798 - val_loss: 0.5726 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9191 - loss: 0.2209\n",
      "Epoch 29: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - accuracy: 0.9192 - loss: 0.2212 - val_accuracy: 0.8064 - val_loss: 0.5450 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9193 - loss: 0.2231\n",
      "Epoch 30: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9191 - loss: 0.2237 - val_accuracy: 0.7454 - val_loss: 0.6703 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9308 - loss: 0.2208\n",
      "Epoch 31: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9309 - loss: 0.2208 - val_accuracy: 0.7745 - val_loss: 0.5747 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9245 - loss: 0.2164\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.9240 - loss: 0.2174 - val_accuracy: 0.7374 - val_loss: 0.6947 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9466 - loss: 0.1672\n",
      "Epoch 33: val_accuracy improved from 0.80637 to 0.82493, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9462 - loss: 0.1677 - val_accuracy: 0.8249 - val_loss: 0.4609 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9485 - loss: 0.1564\n",
      "Epoch 34: val_accuracy improved from 0.82493 to 0.84881, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9483 - loss: 0.1571 - val_accuracy: 0.8488 - val_loss: 0.4300 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9498 - loss: 0.1581\n",
      "Epoch 35: val_accuracy did not improve from 0.84881\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.9497 - loss: 0.1582 - val_accuracy: 0.7560 - val_loss: 0.6637 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9471 - loss: 0.1585\n",
      "Epoch 36: val_accuracy did not improve from 0.84881\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9469 - loss: 0.1588 - val_accuracy: 0.8090 - val_loss: 0.5178 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9478 - loss: 0.1684\n",
      "Epoch 37: val_accuracy did not improve from 0.84881\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9478 - loss: 0.1683 - val_accuracy: 0.8276 - val_loss: 0.4638 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9587 - loss: 0.1268\n",
      "Epoch 38: val_accuracy did not improve from 0.84881\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.9586 - loss: 0.1270 - val_accuracy: 0.8382 - val_loss: 0.4628 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9507 - loss: 0.1492\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.84881\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9507 - loss: 0.1490 - val_accuracy: 0.8276 - val_loss: 0.4808 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9422 - loss: 0.1398\n",
      "Epoch 40: val_accuracy improved from 0.84881 to 0.85411, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9424 - loss: 0.1398 - val_accuracy: 0.8541 - val_loss: 0.4238 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9482 - loss: 0.1347\n",
      "Epoch 41: val_accuracy did not improve from 0.85411\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.9485 - loss: 0.1344 - val_accuracy: 0.8382 - val_loss: 0.4174 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9574 - loss: 0.1221\n",
      "Epoch 42: val_accuracy did not improve from 0.85411\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.9574 - loss: 0.1221 - val_accuracy: 0.8355 - val_loss: 0.3886 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9510 - loss: 0.1353\n",
      "Epoch 43: val_accuracy did not improve from 0.85411\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9514 - loss: 0.1348 - val_accuracy: 0.8435 - val_loss: 0.4104 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9671 - loss: 0.1297\n",
      "Epoch 44: val_accuracy did not improve from 0.85411\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9670 - loss: 0.1295 - val_accuracy: 0.8435 - val_loss: 0.4128 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9593 - loss: 0.1215\n",
      "Epoch 45: val_accuracy improved from 0.85411 to 0.85942, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9592 - loss: 0.1215 - val_accuracy: 0.8594 - val_loss: 0.3845 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9752 - loss: 0.0982\n",
      "Epoch 46: val_accuracy improved from 0.85942 to 0.86207, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9750 - loss: 0.0983 - val_accuracy: 0.8621 - val_loss: 0.3561 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9651 - loss: 0.1135\n",
      "Epoch 47: val_accuracy did not improve from 0.86207\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9650 - loss: 0.1140 - val_accuracy: 0.8355 - val_loss: 0.4430 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9619 - loss: 0.1082\n",
      "Epoch 48: val_accuracy did not improve from 0.86207\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9618 - loss: 0.1084 - val_accuracy: 0.8408 - val_loss: 0.3808 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9559 - loss: 0.1225\n",
      "Epoch 49: val_accuracy did not improve from 0.86207\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 102ms/step - accuracy: 0.9558 - loss: 0.1226 - val_accuracy: 0.8462 - val_loss: 0.3864 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9526 - loss: 0.1363\n",
      "Epoch 50: val_accuracy did not improve from 0.86207\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9527 - loss: 0.1361 - val_accuracy: 0.8249 - val_loss: 0.4682 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "==================================================\n",
      "RESULTS FOR emotion_classifier_v6_enhanced:\n",
      "Train Accuracy: 1.0000\n",
      "Validation Accuracy: 0.8621\n",
      "==================================================\n",
      "\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step \n",
      "SUMMARY:\n",
      "Accuracy: 0.8621\n",
      "F1 (weighted): 0.8622\n",
      "F1 (macro): 0.8565\n",
      "F2 (weighted): 0.8614\n",
      "F2 (macro): 0.8623\n",
      "Precision: 0.8683\n",
      "Recall: 0.8621\n",
      "\n",
      "CLASS ACCURACY:\n",
      "angry: 0.9200\n",
      "calm: 0.8800\n",
      "disgust: 0.8718\n",
      "fearful: 0.8533\n",
      "happy: 0.7600\n",
      "neutral: 0.9211\n",
      "\n",
      "DETAILED REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.87      0.92      0.90        75\n",
      "        calm       0.87      0.88      0.87        75\n",
      "     disgust       0.81      0.87      0.84        39\n",
      "     fearful       0.93      0.85      0.89        75\n",
      "       happy       0.90      0.76      0.83        75\n",
      "     neutral       0.73      0.92      0.81        38\n",
      "\n",
      "    accuracy                           0.86       377\n",
      "   macro avg       0.85      0.87      0.86       377\n",
      "weighted avg       0.87      0.86      0.86       377\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model and artifacts saved successfully!\n",
      "Model: saved_models/emotion_classifier_v6_enhanced_final.keras\n",
      "Encoder: saved_models/emotion_classifier_v6_enhanced_encoder.pkl\n",
      "Class weights: saved_models/emotion_classifier_v6_enhanced_class_weights.pkl\n",
      "Results saved to: emotion_classifier_v6_enhanced_results_20250625_023258.txt\n",
      "\n",
      "Output files:\n",
      "  - emotion_classifier_v6_enhanced_best.keras\n",
      "  - emotion_classifier_v6_enhanced_final.keras\n",
      "  - emotion_classifier_v6_enhanced_encoder.pkl\n",
      "  - emotion_classifier_v6_enhanced_results_*.txt\n"
     ]
    }
   ],
   "source": [
    "def run_analysis():\n",
    "    \"\"\"Main execution function (modified to exclude both 'surprised' and 'sad' classes)\"\"\"\n",
    "    print(f\"Starting analysis: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Version: {MODEL_VERSION}\")\n",
    "    print(f\"Type: {MODEL_TYPE}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # GPU setup (unchanged)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Load data (unchanged)\n",
    "    speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    dataset = load_audio_data(speech_path, song_path)\n",
    "    if dataset is None:\n",
    "        print(\"Data loading failed. Check paths.\")\n",
    "        return\n",
    "\n",
    "    # ==== KEY CHANGE: Filter out BOTH \"surprised\" and \"sad\" samples ====\n",
    "    dataset = dataset[~dataset['emotion'].isin(['surprised', 'sad'])]  # Drop both classes\n",
    "    print(f\"\\nFiltered dataset (no 'surprised' or 'sad'): {len(dataset)} samples\")\n",
    "    print(f\"Remaining emotions: {dataset['emotion'].unique()}\")\n",
    "    print(f\"New distribution:\\n{dataset['emotion'].value_counts()}\")\n",
    "\n",
    "    # Split data (stratify on modified emotion distribution)\n",
    "    train_set, val_set = train_test_split(\n",
    "        dataset,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset['emotion']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "\n",
    "    try:\n",
    "        # Feature extraction (unchanged)\n",
    "        print(\"\\n=== Processing training data ===\")\n",
    "        X_train, y_train = extract_features(train_set, use_augmentation=True)\n",
    "        print(\"\\n=== Processing validation data ===\")\n",
    "        X_val, y_val = extract_features(val_set, use_augmentation=False)\n",
    "\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed.\")\n",
    "            return\n",
    "\n",
    "        # Encode labels (now excludes both classes)\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        y_val_encoded = encoder.transform(y_val)\n",
    "\n",
    "        y_train_cat = to_categorical(y_train_encoded)\n",
    "        y_val_cat = to_categorical(y_val_encoded)\n",
    "\n",
    "        print(\"\\nData statistics:\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "\n",
    "        # Build model\n",
    "        model = build_classifier(X_train.shape[1:], len(encoder.classes_))\n",
    "\n",
    "        # Class weights\n",
    "        weights = compute_class_weights(y_train_encoded)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "\n",
    "        # Train model\n",
    "        print(f\"\\n=== Training: {EXPERIMENT_NAME} ===\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_val, y_val_cat),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=configure_callbacks(EXPERIMENT_NAME),\n",
    "            class_weight=weights,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Load best model\n",
    "        model = tf.keras.models.load_model(f'{EXPERIMENT_NAME}_best.keras')\n",
    "\n",
    "        # Evaluate\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"RESULTS FOR {EXPERIMENT_NAME}:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_val_encoded, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val_encoded, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class accuracy\n",
    "        cm = confusion_matrix(y_val_encoded, y_pred_classes)\n",
    "        class_acc = {}\n",
    "        for i, cls in enumerate(encoder.classes_):\n",
    "            class_acc[cls] = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_val_encoded, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display\n",
    "        print(\"SUMMARY:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 (weighted): {weighted_f1:.4f}\")\n",
    "        print(f\"F1 (macro): {macro_f1:.4f}\")\n",
    "        print(f\"F2 (weighted): {weighted_f2:.4f}\")\n",
    "        print(f\"F2 (macro): {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS ACCURACY:\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nDETAILED REPORT:\")\n",
    "        print(report)\n",
    "\n",
    "        try:\n",
    "            # Save model and artifacts\n",
    "            os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "            model_path = f'saved_models/{EXPERIMENT_NAME}_final.keras'\n",
    "            model.save(model_path)\n",
    "\n",
    "            encoder_path = f'saved_models/{EXPERIMENT_NAME}_encoder.pkl'\n",
    "            with open(encoder_path, 'wb') as f:\n",
    "                pickle.dump(encoder, f)\n",
    "\n",
    "            class_weights_path = f'saved_models/{EXPERIMENT_NAME}_class_weights.pkl'\n",
    "            with open(class_weights_path, 'wb') as f:\n",
    "                pickle.dump(weights, f)\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"Model and artifacts saved successfully!\")\n",
    "            print(f\"Model: {model_path}\")\n",
    "            print(f\"Encoder: {encoder_path}\")\n",
    "            print(f\"Class weights: {class_weights_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving model artifacts: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Save results\n",
    "        save_results(\n",
    "            history, EXPERIMENT_NAME, train_acc, val_acc,\n",
    "            accuracy, macro_f1, weighted_f1,\n",
    "            macro_f2, weighted_f2, precision, recall,\n",
    "            class_acc, report\n",
    "        )\n",
    "\n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_best.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_final.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_encoder.pkl\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_results_*.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ca1b33a-5437-433a-a997-6cc312198fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis: emotion_classifier_v6_enhanced\n",
      "Version: v6\n",
      "Type: enhanced\n",
      "==================================================\n",
      "\n",
      "Filtered dataset (no 'surprised' or 'sad'): 1884 samples\n",
      "Remaining emotions: ['neutral' 'calm' 'happy' 'angry' 'fearful' 'disgust']\n",
      "New distribution:\n",
      "emotion\n",
      "calm       376\n",
      "happy      376\n",
      "angry      376\n",
      "fearful    376\n",
      "disgust    192\n",
      "neutral    188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 1507\n",
      "Validation set: 377\n",
      "\n",
      "=== Processing training data ===\n",
      "Loading features from cache: feature_cache\\f80ddd2e3115295cee0c1082d4d66e6f.npz\n",
      "Initial feature count: 1507\n",
      "Applying data augmentation...\n",
      "Adjusting class distribution:\n",
      "Current counts: {'angry': 301, 'calm': 301, 'disgust': 153, 'fearful': 301, 'happy': 301, 'neutral': 150}\n",
      "Target count: 301.0\n",
      "Augmented dataset size: 1806\n",
      "\n",
      "=== Processing validation data ===\n",
      "Loading features from cache: feature_cache\\ae3fc77cd47ff840a8f72891c3e3c6d5.npz\n",
      "Initial feature count: 377\n",
      "\n",
      "Data statistics:\n",
      "Classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral']\n",
      "Input shape: (32, 94, 2)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=301, weight=1.00\n",
      "Class 1: count=301, weight=1.00\n",
      "Class 2: count=301, weight=1.00\n",
      "Class 3: count=301, weight=1.00\n",
      "Class 4: count=301, weight=1.00\n",
      "Class 5: count=301, weight=1.00\n",
      "\n",
      "Model parameters: 111,142\n",
      "\n",
      "=== Training: emotion_classifier_v6_enhanced ===\n",
      "Epoch 1/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2307 - loss: 1.8159\n",
      "Epoch 1: val_accuracy improved from -inf to 0.19894, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - accuracy: 0.2325 - loss: 1.8106 - val_accuracy: 0.1989 - val_loss: 1.7768 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4338 - loss: 1.3972\n",
      "Epoch 2: val_accuracy did not improve from 0.19894\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.4341 - loss: 1.3959 - val_accuracy: 0.1989 - val_loss: 2.0715 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4672 - loss: 1.2614\n",
      "Epoch 3: val_accuracy did not improve from 0.19894\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.4675 - loss: 1.2610 - val_accuracy: 0.1989 - val_loss: 2.2218 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5580 - loss: 1.1314\n",
      "Epoch 4: val_accuracy did not improve from 0.19894\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.5577 - loss: 1.1314 - val_accuracy: 0.1989 - val_loss: 2.6867 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5960 - loss: 1.0370\n",
      "Epoch 5: val_accuracy did not improve from 0.19894\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.5964 - loss: 1.0364 - val_accuracy: 0.1989 - val_loss: 2.7953 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6255 - loss: 0.9866\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.19894 to 0.20690, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.6257 - loss: 0.9864 - val_accuracy: 0.2069 - val_loss: 2.6598 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6882 - loss: 0.8668\n",
      "Epoch 7: val_accuracy improved from 0.20690 to 0.26260, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.6887 - loss: 0.8654 - val_accuracy: 0.2626 - val_loss: 2.2826 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6939 - loss: 0.7874\n",
      "Epoch 8: val_accuracy improved from 0.26260 to 0.31300, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.6944 - loss: 0.7867 - val_accuracy: 0.3130 - val_loss: 2.2237 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7210 - loss: 0.7466\n",
      "Epoch 9: val_accuracy improved from 0.31300 to 0.57029, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.7210 - loss: 0.7465 - val_accuracy: 0.5703 - val_loss: 1.1298 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7473 - loss: 0.6806\n",
      "Epoch 10: val_accuracy did not improve from 0.57029\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.7469 - loss: 0.6817 - val_accuracy: 0.3263 - val_loss: 2.7878 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7751 - loss: 0.6279\n",
      "Epoch 11: val_accuracy did not improve from 0.57029\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - accuracy: 0.7743 - loss: 0.6294 - val_accuracy: 0.2812 - val_loss: 3.6118 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7567 - loss: 0.6639\n",
      "Epoch 12: val_accuracy improved from 0.57029 to 0.61008, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.7567 - loss: 0.6636 - val_accuracy: 0.6101 - val_loss: 1.0112 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7772 - loss: 0.6016\n",
      "Epoch 13: val_accuracy improved from 0.61008 to 0.66578, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.7771 - loss: 0.6016 - val_accuracy: 0.6658 - val_loss: 0.8914 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7840 - loss: 0.5774\n",
      "Epoch 14: val_accuracy did not improve from 0.66578\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.7842 - loss: 0.5767 - val_accuracy: 0.6101 - val_loss: 0.9842 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7918 - loss: 0.5451\n",
      "Epoch 15: val_accuracy did not improve from 0.66578\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.7921 - loss: 0.5450 - val_accuracy: 0.5517 - val_loss: 1.2728 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7956 - loss: 0.5616\n",
      "Epoch 16: val_accuracy did not improve from 0.66578\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.7959 - loss: 0.5608 - val_accuracy: 0.6048 - val_loss: 0.9385 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8057 - loss: 0.5586\n",
      "Epoch 17: val_accuracy improved from 0.66578 to 0.71353, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.8058 - loss: 0.5579 - val_accuracy: 0.7135 - val_loss: 0.7378 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8440 - loss: 0.4648\n",
      "Epoch 18: val_accuracy did not improve from 0.71353\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.8437 - loss: 0.4651 - val_accuracy: 0.6658 - val_loss: 0.9621 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8322 - loss: 0.4715\n",
      "Epoch 19: val_accuracy improved from 0.71353 to 0.74005, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - accuracy: 0.8322 - loss: 0.4712 - val_accuracy: 0.7401 - val_loss: 0.6867 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8741 - loss: 0.3879\n",
      "Epoch 20: val_accuracy did not improve from 0.74005\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.8735 - loss: 0.3890 - val_accuracy: 0.7082 - val_loss: 0.7069 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8598 - loss: 0.3860\n",
      "Epoch 21: val_accuracy did not improve from 0.74005\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.8593 - loss: 0.3868 - val_accuracy: 0.5225 - val_loss: 1.5345 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8440 - loss: 0.4282\n",
      "Epoch 22: val_accuracy did not improve from 0.74005\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.8441 - loss: 0.4276 - val_accuracy: 0.7082 - val_loss: 0.6962 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8593 - loss: 0.4077\n",
      "Epoch 23: val_accuracy did not improve from 0.74005\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.8593 - loss: 0.4079 - val_accuracy: 0.7401 - val_loss: 0.6977 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8565 - loss: 0.3971\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.74005\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.8566 - loss: 0.3969 - val_accuracy: 0.6790 - val_loss: 0.8643 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8859 - loss: 0.3275\n",
      "Epoch 25: val_accuracy improved from 0.74005 to 0.76393, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.8860 - loss: 0.3275 - val_accuracy: 0.7639 - val_loss: 0.6504 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8936 - loss: 0.2885\n",
      "Epoch 26: val_accuracy improved from 0.76393 to 0.80637, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.8938 - loss: 0.2884 - val_accuracy: 0.8064 - val_loss: 0.5287 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9159 - loss: 0.2417\n",
      "Epoch 27: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.9155 - loss: 0.2427 - val_accuracy: 0.7454 - val_loss: 0.6312 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9154 - loss: 0.2674\n",
      "Epoch 28: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9151 - loss: 0.2676 - val_accuracy: 0.7401 - val_loss: 0.6734 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9130 - loss: 0.2499\n",
      "Epoch 29: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.9129 - loss: 0.2505 - val_accuracy: 0.6737 - val_loss: 0.7633 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9161 - loss: 0.2593\n",
      "Epoch 30: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9160 - loss: 0.2591 - val_accuracy: 0.8011 - val_loss: 0.5329 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9023 - loss: 0.2700\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9023 - loss: 0.2700 - val_accuracy: 0.7851 - val_loss: 0.5864 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9366 - loss: 0.2021\n",
      "Epoch 32: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9364 - loss: 0.2024 - val_accuracy: 0.7958 - val_loss: 0.5295 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9323 - loss: 0.2039\n",
      "Epoch 33: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.9323 - loss: 0.2040 - val_accuracy: 0.7321 - val_loss: 0.7018 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9214 - loss: 0.2407\n",
      "Epoch 34: val_accuracy did not improve from 0.80637\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9218 - loss: 0.2399 - val_accuracy: 0.8011 - val_loss: 0.5213 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9370 - loss: 0.1799\n",
      "Epoch 35: val_accuracy improved from 0.80637 to 0.81698, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9370 - loss: 0.1800 - val_accuracy: 0.8170 - val_loss: 0.4998 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9246 - loss: 0.1929\n",
      "Epoch 36: val_accuracy improved from 0.81698 to 0.82228, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.9247 - loss: 0.1930 - val_accuracy: 0.8223 - val_loss: 0.5033 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9312 - loss: 0.2038\n",
      "Epoch 37: val_accuracy did not improve from 0.82228\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9312 - loss: 0.2036 - val_accuracy: 0.7825 - val_loss: 0.5954 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9343 - loss: 0.1791 \n",
      "Epoch 38: val_accuracy did not improve from 0.82228\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9343 - loss: 0.1794 - val_accuracy: 0.8117 - val_loss: 0.4700 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9352 - loss: 0.1891\n",
      "Epoch 39: val_accuracy improved from 0.82228 to 0.84085, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9353 - loss: 0.1889 - val_accuracy: 0.8408 - val_loss: 0.4465 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9453 - loss: 0.1836\n",
      "Epoch 40: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9453 - loss: 0.1831 - val_accuracy: 0.8170 - val_loss: 0.4642 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9403 - loss: 0.1792\n",
      "Epoch 41: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9402 - loss: 0.1797 - val_accuracy: 0.8196 - val_loss: 0.4703 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9499 - loss: 0.1668\n",
      "Epoch 42: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9497 - loss: 0.1668 - val_accuracy: 0.8037 - val_loss: 0.5457 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9383 - loss: 0.1821\n",
      "Epoch 43: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9384 - loss: 0.1819 - val_accuracy: 0.8249 - val_loss: 0.4723 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9318 - loss: 0.1961\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9318 - loss: 0.1960 - val_accuracy: 0.8196 - val_loss: 0.4765 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9498 - loss: 0.1718\n",
      "Epoch 45: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.9498 - loss: 0.1718 - val_accuracy: 0.8143 - val_loss: 0.4464 - learning_rate: 6.2500e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9392 - loss: 0.1807\n",
      "Epoch 46: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9392 - loss: 0.1807 - val_accuracy: 0.8382 - val_loss: 0.4644 - learning_rate: 6.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9509 - loss: 0.1683\n",
      "Epoch 47: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9508 - loss: 0.1682 - val_accuracy: 0.8276 - val_loss: 0.4296 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9587 - loss: 0.1674 \n",
      "Epoch 48: val_accuracy did not improve from 0.84085\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9585 - loss: 0.1675 - val_accuracy: 0.8302 - val_loss: 0.4483 - learning_rate: 6.2500e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9552 - loss: 0.1421\n",
      "Epoch 49: val_accuracy improved from 0.84085 to 0.85146, saving model to emotion_classifier_v6_enhanced_best.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9550 - loss: 0.1426 - val_accuracy: 0.8515 - val_loss: 0.3980 - learning_rate: 6.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m56/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9453 - loss: 0.1486\n",
      "Epoch 50: val_accuracy did not improve from 0.85146\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - accuracy: 0.9454 - loss: 0.1487 - val_accuracy: 0.8408 - val_loss: 0.4291 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "==================================================\n",
      "RESULTS FOR emotion_classifier_v6_enhanced:\n",
      "Train Accuracy: 0.9989\n",
      "Validation Accuracy: 0.8515\n",
      "==================================================\n",
      "\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (Table Format):\n",
      "               Pred: angry  Pred: calm  Pred: disgust  Pred: fearful  \\\n",
      "True: angry             66           0              3              3   \n",
      "True: calm               0          70              0              0   \n",
      "True: disgust            4           2             31              0   \n",
      "True: fearful            1           2              1             67   \n",
      "True: happy              5           5              1              9   \n",
      "True: neutral            0           2              0              1   \n",
      "\n",
      "               Pred: happy  Pred: neutral  \n",
      "True: angry              3              0  \n",
      "True: calm               1              4  \n",
      "True: disgust            2              0  \n",
      "True: fearful            3              1  \n",
      "True: happy             52              3  \n",
      "True: neutral            0             35  \n",
      "Accuracy: 0.8515\n",
      "F1 (weighted): 0.8492\n",
      "F1 (macro): 0.8486\n",
      "F2 (weighted): 0.8500\n",
      "F2 (macro): 0.8505\n",
      "Precision: 0.8520\n",
      "Recall: 0.8515\n",
      "\n",
      "CLASS ACCURACY:\n",
      "angry: 0.8800\n",
      "calm: 0.9333\n",
      "disgust: 0.7949\n",
      "fearful: 0.8933\n",
      "happy: 0.6933\n",
      "neutral: 0.9211\n",
      "\n",
      "DETAILED REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.87      0.88      0.87        75\n",
      "        calm       0.86      0.93      0.90        75\n",
      "     disgust       0.86      0.79      0.83        39\n",
      "     fearful       0.84      0.89      0.86        75\n",
      "       happy       0.85      0.69      0.76        75\n",
      "     neutral       0.81      0.92      0.86        38\n",
      "\n",
      "    accuracy                           0.85       377\n",
      "   macro avg       0.85      0.85      0.85       377\n",
      "weighted avg       0.85      0.85      0.85       377\n",
      "\n",
      "\n",
      "==================================================\n",
      "Model and artifacts saved successfully!\n",
      "Model: saved_models/emotion_classifier_v6_enhanced_final.keras\n",
      "Encoder: saved_models/emotion_classifier_v6_enhanced_encoder.pkl\n",
      "Class weights: saved_models/emotion_classifier_v6_enhanced_class_weights.pkl\n",
      "Results saved to: emotion_classifier_v6_enhanced_results_20250625_070632.txt\n",
      "\n",
      "Output files:\n",
      "  - emotion_classifier_v6_enhanced_best.keras\n",
      "  - emotion_classifier_v6_enhanced_final.keras\n",
      "  - emotion_classifier_v6_enhanced_encoder.pkl\n",
      "  - emotion_classifier_v6_enhanced_results_*.txt\n"
     ]
    }
   ],
   "source": [
    "def run_analysis():\n",
    "    \"\"\"Main execution function (modified to exclude both 'surprised' and 'sad' classes)\"\"\"\n",
    "    print(f\"Starting analysis: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Version: {MODEL_VERSION}\")\n",
    "    print(f\"Type: {MODEL_TYPE}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # GPU setup (unchanged)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Load data (unchanged)\n",
    "    speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "    song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "    dataset = load_audio_data(speech_path, song_path)\n",
    "    if dataset is None:\n",
    "        print(\"Data loading failed. Check paths.\")\n",
    "        return\n",
    "\n",
    "    # ==== Filter out BOTH \"surprised\" and \"sad\" samples ====\n",
    "    dataset = dataset[~dataset['emotion'].isin(['surprised', 'sad'])]\n",
    "    print(f\"\\nFiltered dataset (no 'surprised' or 'sad'): {len(dataset)} samples\")\n",
    "    print(f\"Remaining emotions: {dataset['emotion'].unique()}\")\n",
    "    print(f\"New distribution:\\n{dataset['emotion'].value_counts()}\")\n",
    "\n",
    "    # Split data (stratified)\n",
    "    train_set, val_set = train_test_split(\n",
    "        dataset,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset['emotion']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "\n",
    "    try:\n",
    "        # Feature extraction\n",
    "        print(\"\\n=== Processing training data ===\")\n",
    "        X_train, y_train = extract_features(train_set, use_augmentation=True)\n",
    "\n",
    "        print(\"\\n=== Processing validation data ===\")\n",
    "        X_val, y_val = extract_features(val_set, use_augmentation=False)\n",
    "\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            print(\"Feature extraction failed.\")\n",
    "            return\n",
    "\n",
    "        # Encode labels\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        y_val_encoded = encoder.transform(y_val)\n",
    "\n",
    "        y_train_cat = to_categorical(y_train_encoded)\n",
    "        y_val_cat = to_categorical(y_val_encoded)\n",
    "\n",
    "        print(\"\\nData statistics:\")\n",
    "        print(f\"Classes: {encoder.classes_}\")\n",
    "        print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "\n",
    "        # Build model\n",
    "        model = build_classifier(X_train.shape[1:], len(encoder.classes_))\n",
    "\n",
    "        # Compute class weights\n",
    "        weights = compute_class_weights(y_train_encoded)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "\n",
    "        # Train model\n",
    "        print(f\"\\n=== Training: {EXPERIMENT_NAME} ===\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_val, y_val_cat),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=configure_callbacks(EXPERIMENT_NAME),\n",
    "            class_weight=weights,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Load best model\n",
    "        model = tf.keras.models.load_model(f'{EXPERIMENT_NAME}_best.keras')\n",
    "\n",
    "        # Evaluate\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"RESULTS FOR {EXPERIMENT_NAME}:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_val_encoded, y_pred_classes)\n",
    "        macro_f1 = f1_score(y_val_encoded, y_pred_classes, average='macro')\n",
    "        weighted_f1 = f1_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        macro_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='macro')\n",
    "        weighted_f2 = fbeta_score(y_val_encoded, y_pred_classes, beta=2, average='weighted')\n",
    "        precision = precision_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val_encoded, y_pred_classes, average='weighted')\n",
    "\n",
    "        # Class accuracy\n",
    "        cm = confusion_matrix(y_val_encoded, y_pred_classes)\n",
    "        class_acc = {}\n",
    "        for i, cls in enumerate(encoder.classes_):\n",
    "            class_acc[cls] = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_val_encoded, y_pred_classes, target_names=encoder.classes_)\n",
    "\n",
    "        # Display Confusion Matrix\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import pandas as pd\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=encoder.classes_,\n",
    "                    yticklabels=encoder.classes_)\n",
    "        plt.title(\"Confusion Matrix (Validation Set)\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{EXPERIMENT_NAME}_confusion_matrix.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Display Confusion Matrix as Table\n",
    "        conf_matrix_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[f\"True: {label}\" for label in encoder.classes_],\n",
    "            columns=[f\"Pred: {label}\" for label in encoder.classes_]\n",
    "        )\n",
    "        print(\"\\nConfusion Matrix (Table Format):\")\n",
    "        print(conf_matrix_df)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 (weighted): {weighted_f1:.4f}\")\n",
    "        print(f\"F1 (macro): {macro_f1:.4f}\")\n",
    "        print(f\"F2 (weighted): {weighted_f2:.4f}\")\n",
    "        print(f\"F2 (macro): {macro_f2:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "        print(\"\\nCLASS ACCURACY:\")\n",
    "        for cls, acc in class_acc.items():\n",
    "            print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nDETAILED REPORT:\")\n",
    "        print(report)\n",
    "\n",
    "        try:\n",
    "            # Save model and artifacts\n",
    "            os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "            model_path = f'saved_models/{EXPERIMENT_NAME}_final.keras'\n",
    "            model.save(model_path)\n",
    "\n",
    "            encoder_path = f'saved_models/{EXPERIMENT_NAME}_encoder.pkl'\n",
    "            with open(encoder_path, 'wb') as f:\n",
    "                pickle.dump(encoder, f)\n",
    "\n",
    "            class_weights_path = f'saved_models/{EXPERIMENT_NAME}_class_weights.pkl'\n",
    "            with open(class_weights_path, 'wb') as f:\n",
    "                pickle.dump(weights, f)\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"Model and artifacts saved successfully!\")\n",
    "            print(f\"Model: {model_path}\")\n",
    "            print(f\"Encoder: {encoder_path}\")\n",
    "            print(f\"Class weights: {class_weights_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving model artifacts: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Save results\n",
    "        save_results(\n",
    "            history, EXPERIMENT_NAME, train_acc, val_acc,\n",
    "            accuracy, macro_f1, weighted_f1,\n",
    "            macro_f2, weighted_f2, precision, recall,\n",
    "            class_acc, report\n",
    "        )\n",
    "\n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_best.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_final.keras\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_encoder.pkl\")\n",
    "        print(f\"  - {EXPERIMENT_NAME}_results_*.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58d8d2e3-d791-4f5e-96cb-a0779621b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saved_models/emotion_classifier_v6_enhanced_final.keras\"\n",
    "ENCODER_PATH = \"saved_models/emotion_classifier_v6_enhanced_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e71189a-099a-45c7-81f2-b6f5c36f542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them before analysis\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "with open(ENCODER_PATH, 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ea31ee-3225-46f0-b389-ecd288972129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total files found: 2452\n",
      "Sample files:\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-01-01.wav -> neutral\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-02-01.wav -> neutral\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-02-01-01.wav -> neutral\n",
      "\n",
      "Total files found: 2452\n",
      "Sample files:\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-01-01.wav -> neutral\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-02-01.wav -> neutral\n",
      "  C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-02-01-01.wav -> neutral\n",
      "\n",
      "First 3 files:\n",
      "Path: C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-01-01.wav | Emotion: neutral\n",
      "Path: C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-01-02-01.wav | Emotion: neutral\n",
      "Path: C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01\\03-01-01-01-02-01-01.wav | Emotion: neutral\n",
      "File: 03-01-01-01-01-01-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.79\n",
      "File: 03-01-01-01-01-02-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-01-01-02-01-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.92\n",
      "File: 03-01-01-01-02-02-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-02-01-01-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.90\n",
      "File: 03-01-02-01-01-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-01-02-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-01-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-01-02-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-02-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-03-02-01-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-01-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-02-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-04-01-01-01-01.wav\n",
      "True: sad, Predicted: calm, Conf: 0.60\n",
      "File: 03-01-04-01-01-02-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.50\n",
      "File: 03-01-04-01-02-01-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-04-01-02-02-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.88\n",
      "File: 03-01-04-02-01-01-01.wav\n",
      "True: sad, Predicted: calm, Conf: 0.41\n",
      "File: 03-01-04-02-01-02-01.wav\n",
      "True: sad, Predicted: happy, Conf: 0.65\n",
      "File: 03-01-04-02-02-01-01.wav\n",
      "True: sad, Predicted: happy, Conf: 0.81\n",
      "File: 03-01-04-02-02-02-01.wav\n",
      "True: sad, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-05-01-01-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.86\n",
      "File: 03-01-05-01-02-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-02-01-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-01.wav\n",
      "True: fearful, Predicted: happy, Conf: 0.43\n",
      "File: 03-01-06-01-01-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.71\n",
      "File: 03-01-06-01-02-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-01-02-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-06-02-01-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-07-01-01-01-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.68\n",
      "File: 03-01-07-01-02-02-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-01.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.78\n",
      "File: 03-01-08-01-01-02-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.48\n",
      "File: 03-01-08-01-02-01-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.53\n",
      "File: 03-01-08-01-02-02-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-08-02-01-01-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.54\n",
      "File: 03-01-08-02-01-02-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.83\n",
      "File: 03-01-08-02-02-01-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.80\n",
      "File: 03-01-08-02-02-02-01.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.49\n",
      "File: 03-01-01-01-01-01-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-01-02-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-02-01-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-01-01-02-02-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.89\n",
      "File: 03-01-02-01-01-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-01-03-01-01-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-01-02-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-03-01-02-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-03-02-01-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-01-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-02-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-02-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-04-01-01-01-02.wav\n",
      "True: sad, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-04-01-01-02-02.wav\n",
      "True: sad, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-04-01-02-01-02.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-04-01-02-02-02.wav\n",
      "True: sad, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-04-02-01-01-02.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.91\n",
      "File: 03-01-04-02-01-02-02.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.67\n",
      "File: 03-01-04-02-02-01-02.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.48\n",
      "File: 03-01-04-02-02-02-02.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.71\n",
      "File: 03-01-05-01-01-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-01-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-01-01-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-01-06-01-02-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-01-02-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-02-02-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-01-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-02-02-02.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-08-01-01-01-02.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.62\n",
      "File: 03-01-08-01-01-02-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.45\n",
      "File: 03-01-08-01-02-01-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.88\n",
      "File: 03-01-08-01-02-02-02.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.53\n",
      "File: 03-01-08-02-01-01-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.49\n",
      "File: 03-01-08-02-01-02-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.93\n",
      "File: 03-01-08-02-02-01-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-08-02-02-02-02.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.87\n",
      "File: 03-01-01-01-01-01-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-01-02-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.90\n",
      "File: 03-01-01-01-02-01-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-01-01-02-02-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.57\n",
      "File: 03-01-02-01-01-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-02-01-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-01-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-03-01-01-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.37\n",
      "File: 03-01-03-01-01-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-01-02-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-02-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-01-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-01-03-02-01-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-02-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.79\n",
      "File: 03-01-03-02-02-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-04-01-01-01-03.wav\n",
      "True: sad, Predicted: calm, Conf: 0.86\n",
      "File: 03-01-04-01-01-02-03.wav\n",
      "True: sad, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-04-01-02-01-03.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-04-01-02-02-03.wav\n",
      "True: sad, Predicted: calm, Conf: 0.83\n",
      "File: 03-01-04-02-01-01-03.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-01-04-02-01-02-03.wav\n",
      "True: sad, Predicted: happy, Conf: 0.57\n",
      "File: 03-01-04-02-02-01-03.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-04-02-02-02-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.31\n",
      "File: 03-01-05-01-01-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 0.78\n",
      "File: 03-01-05-01-01-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-02-01-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-01-01-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.72\n",
      "File: 03-01-06-01-02-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.85\n",
      "File: 03-01-06-01-02-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-02-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-02-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-03.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-08-01-01-02-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-08-01-02-01-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.83\n",
      "File: 03-01-08-01-02-02-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.65\n",
      "File: 03-01-08-02-01-01-03.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.54\n",
      "File: 03-01-08-02-01-02-03.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.42\n",
      "File: 03-01-08-02-02-01-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.80\n",
      "File: 03-01-08-02-02-02-03.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-01-01-01-01-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-01-02-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-01-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.88\n",
      "File: 03-01-01-01-02-02-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-02-01-01-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-01-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.77\n",
      "File: 03-01-03-01-01-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-01-03-01-02-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-01-02-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-01-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-01-03-02-01-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-04-01-01-01-04.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.58\n",
      "File: 03-01-04-01-01-02-04.wav\n",
      "True: sad, Predicted: happy, Conf: 0.43\n",
      "File: 03-01-04-01-02-01-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.49\n",
      "File: 03-01-04-01-02-02-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.67\n",
      "File: 03-01-04-02-01-01-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.62\n",
      "File: 03-01-04-02-01-02-04.wav\n",
      "True: sad, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-04-02-02-01-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.62\n",
      "File: 03-01-04-02-02-02-04.wav\n",
      "True: sad, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-05-01-01-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-02-01-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-01-01-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-01-02-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-01-02-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-01-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-01-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.82\n",
      "File: 03-01-07-01-01-01-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-04.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.86\n",
      "File: 03-01-08-01-01-02-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-08-01-02-01-04.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.41\n",
      "File: 03-01-08-01-02-02-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.90\n",
      "File: 03-01-08-02-01-01-04.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-08-02-01-02-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.64\n",
      "File: 03-01-08-02-02-01-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.78\n",
      "File: 03-01-08-02-02-02-04.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.71\n",
      "File: 03-01-01-01-01-01-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.58\n",
      "File: 03-01-01-01-01-02-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.90\n",
      "File: 03-01-01-01-02-01-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.89\n",
      "File: 03-01-01-01-02-02-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-02-01-01-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-01-02-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-02-02-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-03-01-01-01-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.86\n",
      "File: 03-01-03-01-01-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-01-02-01-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.77\n",
      "File: 03-01-03-01-02-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-02-01-01-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.68\n",
      "File: 03-01-03-02-01-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-02-01-05.wav\n",
      "True: happy, Predicted: angry, Conf: 0.88\n",
      "File: 03-01-03-02-02-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-04-01-01-01-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.63\n",
      "File: 03-01-04-01-01-02-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.71\n",
      "File: 03-01-04-01-02-01-05.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.84\n",
      "File: 03-01-04-01-02-02-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.86\n",
      "File: 03-01-04-02-01-01-05.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.74\n",
      "File: 03-01-04-02-01-02-05.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.71\n",
      "File: 03-01-04-02-02-01-05.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-04-02-02-02-05.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.89\n",
      "File: 03-01-05-01-01-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-05.wav\n",
      "True: angry, Predicted: disgust, Conf: 0.84\n",
      "File: 03-01-05-01-02-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-01-05-02-01-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-02-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.79\n",
      "File: 03-01-06-01-01-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.45\n",
      "File: 03-01-06-01-02-01-05.wav\n",
      "True: fearful, Predicted: neutral, Conf: 0.62\n",
      "File: 03-01-06-01-02-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-02-01-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-05.wav\n",
      "True: fearful, Predicted: angry, Conf: 0.45\n",
      "File: 03-01-06-02-02-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-02-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-01-01-02-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.95\n",
      "File: 03-01-07-02-01-01-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-02-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-02-01-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-05.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-05.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.51\n",
      "File: 03-01-08-01-01-02-05.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.48\n",
      "File: 03-01-08-01-02-01-05.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.75\n",
      "File: 03-01-08-01-02-02-05.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.96\n",
      "File: 03-01-08-02-01-01-05.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.56\n",
      "File: 03-01-08-02-01-02-05.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.97\n",
      "File: 03-01-08-02-02-01-05.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.43\n",
      "File: 03-01-08-02-02-02-05.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.73\n",
      "File: 03-01-01-01-01-01-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-01-02-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-02-01-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-02-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-02-01-01-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-01-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-02-01-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-01-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-01-02-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-01-02-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-01-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-02-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-02-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-04-01-01-01-06.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-04-01-01-02-06.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-04-01-02-01-06.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.71\n",
      "File: 03-01-04-01-02-02-06.wav\n",
      "True: sad, Predicted: calm, Conf: 0.82\n",
      "File: 03-01-04-02-01-01-06.wav\n",
      "True: sad, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-04-02-01-02-06.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.77\n",
      "File: 03-01-04-02-02-01-06.wav\n",
      "True: sad, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-04-02-02-02-06.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.63\n",
      "File: 03-01-05-01-01-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-02-06.wav\n",
      "True: angry, Predicted: happy, Conf: 0.38\n",
      "File: 03-01-05-02-01-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.90\n",
      "File: 03-01-06-01-01-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-01-02-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-01-02-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-02-01-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-02-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-02-02-06.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-06.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.77\n",
      "File: 03-01-08-01-01-02-06.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-08-01-02-01-06.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-08-01-02-02-06.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.86\n",
      "File: 03-01-08-02-01-01-06.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.66\n",
      "File: 03-01-08-02-01-02-06.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.64\n",
      "File: 03-01-08-02-02-01-06.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.53\n",
      "File: 03-01-08-02-02-02-06.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.92\n",
      "File: 03-01-01-01-01-01-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.40\n",
      "File: 03-01-01-01-01-02-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.62\n",
      "File: 03-01-01-01-02-01-07.wav\n",
      "True: neutral, Predicted: calm, Conf: 0.61\n",
      "File: 03-01-01-01-02-02-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.80\n",
      "File: 03-01-02-01-01-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-01-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-01-02-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-02-01-02-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-02-02-01-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-02-01-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 0.49\n",
      "File: 03-01-02-02-02-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-01-01-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-01-02-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-02-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-02-01-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-02-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-04-01-01-01-07.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.41\n",
      "File: 03-01-04-01-01-02-07.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.71\n",
      "File: 03-01-04-01-02-01-07.wav\n",
      "True: sad, Predicted: calm, Conf: 0.78\n",
      "File: 03-01-04-01-02-02-07.wav\n",
      "True: sad, Predicted: happy, Conf: 0.52\n",
      "File: 03-01-04-02-01-01-07.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-04-02-01-02-07.wav\n",
      "True: sad, Predicted: happy, Conf: 0.63\n",
      "File: 03-01-04-02-02-01-07.wav\n",
      "True: sad, Predicted: happy, Conf: 0.44\n",
      "File: 03-01-04-02-02-02-07.wav\n",
      "True: sad, Predicted: happy, Conf: 0.47\n",
      "File: 03-01-05-01-01-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-01-07.wav\n",
      "True: angry, Predicted: disgust, Conf: 0.76\n",
      "File: 03-01-05-01-02-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-01-01-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-06-01-02-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.89\n",
      "File: 03-01-06-02-01-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-02-01-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-02-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-07-01-01-01-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-01-02-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.95\n",
      "File: 03-01-07-01-02-02-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-02-01-02-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.97\n",
      "File: 03-01-07-02-02-01-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-07.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-07.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.54\n",
      "File: 03-01-08-01-01-02-07.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.58\n",
      "File: 03-01-08-01-02-01-07.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.75\n",
      "File: 03-01-08-01-02-02-07.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-08-02-01-01-07.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.88\n",
      "File: 03-01-08-02-01-02-07.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.81\n",
      "File: 03-01-08-02-02-01-07.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.83\n",
      "File: 03-01-08-02-02-02-07.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.81\n",
      "File: 03-01-01-01-01-01-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-01-02-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-01-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-02-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-02-01-01-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-02-08.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.76\n",
      "File: 03-01-02-02-01-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.64\n",
      "File: 03-01-03-01-01-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-01-02-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.82\n",
      "File: 03-01-03-01-02-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-02-01-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-01-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-02-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-04-01-01-01-08.wav\n",
      "True: sad, Predicted: calm, Conf: 0.55\n",
      "File: 03-01-04-01-01-02-08.wav\n",
      "True: sad, Predicted: calm, Conf: 0.86\n",
      "File: 03-01-04-01-02-01-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.52\n",
      "File: 03-01-04-01-02-02-08.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.76\n",
      "File: 03-01-04-02-01-01-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-04-02-01-02-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-01-04-02-02-01-08.wav\n",
      "True: sad, Predicted: happy, Conf: 0.86\n",
      "File: 03-01-04-02-02-02-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-05-01-01-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-01-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-01-01-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-01-02-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-01-02-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.66\n",
      "File: 03-01-06-02-01-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-08.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.82\n",
      "File: 03-01-08-01-01-02-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.70\n",
      "File: 03-01-08-01-02-01-08.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.56\n",
      "File: 03-01-08-01-02-02-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.55\n",
      "File: 03-01-08-02-01-01-08.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.40\n",
      "File: 03-01-08-02-01-02-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.68\n",
      "File: 03-01-08-02-02-01-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-08-02-02-02-08.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.88\n",
      "File: 03-01-01-01-01-01-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-01-02-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-01-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.90\n",
      "File: 03-01-01-01-02-02-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.93\n",
      "File: 03-01-02-01-01-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-01-01-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-02-02-01-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.61\n",
      "File: 03-01-03-01-01-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-01-02-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-03-01-02-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.86\n",
      "File: 03-01-03-02-01-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-02-02-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-04-01-01-01-09.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.62\n",
      "File: 03-01-04-01-01-02-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.55\n",
      "File: 03-01-04-01-02-01-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.52\n",
      "File: 03-01-04-01-02-02-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-04-02-01-01-09.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.49\n",
      "File: 03-01-04-02-01-02-09.wav\n",
      "True: sad, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-04-02-02-01-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.47\n",
      "File: 03-01-04-02-02-02-09.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.73\n",
      "File: 03-01-05-01-01-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.51\n",
      "File: 03-01-05-01-01-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.74\n",
      "File: 03-01-05-01-02-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.92\n",
      "File: 03-01-05-01-02-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-02-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-06-01-01-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-06-01-02-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-02-01-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-01-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-02-02-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-07-01-01-01-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.96\n",
      "File: 03-01-07-02-01-02-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-09.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-09.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.94\n",
      "File: 03-01-08-01-01-02-09.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.73\n",
      "File: 03-01-08-01-02-01-09.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.81\n",
      "File: 03-01-08-01-02-02-09.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.64\n",
      "File: 03-01-08-02-01-01-09.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.55\n",
      "File: 03-01-08-02-01-02-09.wav\n",
      "True: surprised, Predicted: neutral, Conf: 0.24\n",
      "File: 03-01-08-02-02-01-09.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.46\n",
      "File: 03-01-08-02-02-02-09.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.77\n",
      "File: 03-01-01-01-01-01-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-01-02-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.80\n",
      "File: 03-01-01-01-02-01-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-02-02-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-02-01-01-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-01-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-02-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-02-02-01-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-03-01-01-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-01-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-01-02-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.87\n",
      "File: 03-01-03-01-02-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-01-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-01-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-02-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-02-02-10.wav\n",
      "True: happy, Predicted: angry, Conf: 0.55\n",
      "File: 03-01-04-01-01-01-10.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.34\n",
      "File: 03-01-04-01-01-02-10.wav\n",
      "True: sad, Predicted: calm, Conf: 0.71\n",
      "File: 03-01-04-01-02-01-10.wav\n",
      "True: sad, Predicted: calm, Conf: 0.78\n",
      "File: 03-01-04-01-02-02-10.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.48\n",
      "File: 03-01-04-02-01-01-10.wav\n",
      "True: sad, Predicted: happy, Conf: 0.55\n",
      "File: 03-01-04-02-01-02-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-04-02-02-01-10.wav\n",
      "True: sad, Predicted: calm, Conf: 0.56\n",
      "File: 03-01-04-02-02-02-10.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.70\n",
      "File: 03-01-05-01-01-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-01-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-01-01-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-01-06-01-02-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-01-02-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.45\n",
      "File: 03-01-06-02-01-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-02-01-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-02-02-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-02-02-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-07-01-01-01-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-01-02-02-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-02-01-01-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-10.wav\n",
      "True: disgust, Predicted: happy, Conf: 0.39\n",
      "File: 03-01-07-02-02-02-10.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-10.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-08-01-01-02-10.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.46\n",
      "File: 03-01-08-01-02-01-10.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-08-01-02-02-10.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-08-02-01-01-10.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.36\n",
      "File: 03-01-08-02-01-02-10.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.74\n",
      "File: 03-01-08-02-02-01-10.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.53\n",
      "File: 03-01-08-02-02-02-10.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.92\n",
      "File: 03-01-01-01-01-01-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.93\n",
      "File: 03-01-01-01-01-02-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-01-01-02-01-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.88\n",
      "File: 03-01-01-01-02-02-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-01-02-01-01-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-01-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-02-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-11.wav\n",
      "True: happy, Predicted: neutral, Conf: 0.85\n",
      "File: 03-01-03-01-01-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-03-01-02-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.47\n",
      "File: 03-01-03-01-02-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.57\n",
      "File: 03-01-03-02-01-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.79\n",
      "File: 03-01-03-02-01-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-01-03-02-02-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-02-02-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.80\n",
      "File: 03-01-04-01-01-01-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.51\n",
      "File: 03-01-04-01-01-02-11.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.42\n",
      "File: 03-01-04-01-02-01-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-04-01-02-02-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-04-02-01-01-11.wav\n",
      "True: sad, Predicted: angry, Conf: 0.71\n",
      "File: 03-01-04-02-01-02-11.wav\n",
      "True: sad, Predicted: happy, Conf: 0.33\n",
      "File: 03-01-04-02-02-01-11.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.66\n",
      "File: 03-01-04-02-02-02-11.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.93\n",
      "File: 03-01-05-01-01-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.94\n",
      "File: 03-01-05-01-01-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-01-05-01-02-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-01-05-01-02-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-02-01-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.90\n",
      "File: 03-01-06-01-01-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.71\n",
      "File: 03-01-06-01-02-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-01-02-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-02-01-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-07-01-01-01-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-01-02-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.95\n",
      "File: 03-01-07-02-01-01-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.97\n",
      "File: 03-01-07-02-01-02-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-02-02-11.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.58\n",
      "File: 03-01-08-01-01-02-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.49\n",
      "File: 03-01-08-01-02-01-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.45\n",
      "File: 03-01-08-01-02-02-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-08-02-01-01-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-08-02-01-02-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.70\n",
      "File: 03-01-08-02-02-01-11.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.79\n",
      "File: 03-01-08-02-02-02-11.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.48\n",
      "File: 03-01-01-01-01-01-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-01-01-01-01-02-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-01-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-02-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-02-01-01-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-01-01-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-12.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.70\n",
      "File: 03-01-02-02-01-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-01-01-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-02-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-01-02-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-02-01-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-02-02-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-04-01-01-01-12.wav\n",
      "True: sad, Predicted: happy, Conf: 0.74\n",
      "File: 03-01-04-01-01-02-12.wav\n",
      "True: sad, Predicted: calm, Conf: 0.83\n",
      "File: 03-01-04-01-02-01-12.wav\n",
      "True: sad, Predicted: happy, Conf: 0.36\n",
      "File: 03-01-04-01-02-02-12.wav\n",
      "True: sad, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-04-02-01-01-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-04-02-01-02-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-04-02-02-01-12.wav\n",
      "True: sad, Predicted: happy, Conf: 0.56\n",
      "File: 03-01-04-02-02-02-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-05-01-01-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.64\n",
      "File: 03-01-05-01-01-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.79\n",
      "File: 03-01-05-01-02-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.90\n",
      "File: 03-01-05-02-01-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.90\n",
      "File: 03-01-06-01-01-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-06-01-02-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-01-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.74\n",
      "File: 03-01-06-02-02-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.96\n",
      "File: 03-01-07-01-02-02-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-01-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-02-02-12.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-08-01-01-01-12.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.75\n",
      "File: 03-01-08-01-01-02-12.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.83\n",
      "File: 03-01-08-01-02-01-12.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-08-01-02-02-12.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-08-02-01-01-12.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.37\n",
      "File: 03-01-08-02-01-02-12.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-08-02-02-01-12.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.87\n",
      "File: 03-01-08-02-02-02-12.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.62\n",
      "File: 03-01-01-01-01-01-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-01-01-01-01-02-13.wav\n",
      "True: neutral, Predicted: calm, Conf: 0.48\n",
      "File: 03-01-01-01-02-01-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-01-01-02-02-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-02-01-01-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-02-01-02-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.84\n",
      "File: 03-01-02-02-01-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.85\n",
      "File: 03-01-02-02-02-01-13.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.77\n",
      "File: 03-01-02-02-02-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.82\n",
      "File: 03-01-03-01-01-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.72\n",
      "File: 03-01-03-01-01-02-13.wav\n",
      "True: happy, Predicted: neutral, Conf: 0.40\n",
      "File: 03-01-03-01-02-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.79\n",
      "File: 03-01-03-01-02-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-02-01-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-02-13.wav\n",
      "True: happy, Predicted: fearful, Conf: 0.42\n",
      "File: 03-01-03-02-02-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.87\n",
      "File: 03-01-03-02-02-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-04-01-01-01-13.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.67\n",
      "File: 03-01-04-01-01-02-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.53\n",
      "File: 03-01-04-01-02-01-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.73\n",
      "File: 03-01-04-01-02-02-13.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.89\n",
      "File: 03-01-04-02-01-01-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.67\n",
      "File: 03-01-04-02-01-02-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.46\n",
      "File: 03-01-04-02-02-01-13.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.59\n",
      "File: 03-01-04-02-02-02-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.58\n",
      "File: 03-01-05-01-01-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-01-01-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-02-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-02-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.72\n",
      "File: 03-01-06-01-01-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-06-01-02-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.55\n",
      "File: 03-01-06-01-02-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.73\n",
      "File: 03-01-06-02-01-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.73\n",
      "File: 03-01-06-02-01-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-02-02-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-02-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.65\n",
      "File: 03-01-07-01-01-01-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.58\n",
      "File: 03-01-07-01-01-02-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.66\n",
      "File: 03-01-07-01-02-01-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-01-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.96\n",
      "File: 03-01-07-02-02-02-13.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-13.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.53\n",
      "File: 03-01-08-01-01-02-13.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.49\n",
      "File: 03-01-08-01-02-01-13.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.47\n",
      "File: 03-01-08-01-02-02-13.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-08-02-01-01-13.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.69\n",
      "File: 03-01-08-02-01-02-13.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.40\n",
      "File: 03-01-08-02-02-01-13.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.69\n",
      "File: 03-01-08-02-02-02-13.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.47\n",
      "File: 03-01-01-01-01-01-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-01-02-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-01-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-02-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-02-01-01-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-01-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 0.85\n",
      "File: 03-01-02-01-02-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 0.88\n",
      "File: 03-01-02-01-02-02-14.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.68\n",
      "File: 03-01-02-02-01-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-01-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-01-02-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.80\n",
      "File: 03-01-03-01-02-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-03-02-01-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-14.wav\n",
      "True: happy, Predicted: angry, Conf: 0.52\n",
      "File: 03-01-03-02-02-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-04-01-01-01-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.36\n",
      "File: 03-01-04-01-01-02-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.58\n",
      "File: 03-01-04-01-02-01-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.97\n",
      "File: 03-01-04-01-02-02-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.90\n",
      "File: 03-01-04-02-01-01-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-04-02-01-02-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.76\n",
      "File: 03-01-04-02-02-01-14.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.95\n",
      "File: 03-01-04-02-02-02-14.wav\n",
      "True: sad, Predicted: angry, Conf: 0.68\n",
      "File: 03-01-05-01-01-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 0.78\n",
      "File: 03-01-05-01-02-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-01-01-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-01-02-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-01-06-01-02-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-02-01-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-07-01-01-01-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.91\n",
      "File: 03-01-07-01-02-01-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-02-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-14.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.54\n",
      "File: 03-01-08-01-01-01-14.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.54\n",
      "File: 03-01-08-01-01-02-14.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.27\n",
      "File: 03-01-08-01-02-01-14.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.63\n",
      "File: 03-01-08-01-02-02-14.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.66\n",
      "File: 03-01-08-02-01-01-14.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.97\n",
      "File: 03-01-08-02-01-02-14.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.95\n",
      "File: 03-01-08-02-02-01-14.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.94\n",
      "File: 03-01-08-02-02-02-14.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-01-01-01-01-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-01-01-01-02-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.75\n",
      "File: 03-01-01-01-02-01-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-02-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-02-01-01-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-01-01-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-01-02-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-02-02-01-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-02-01-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.73\n",
      "File: 03-01-02-02-02-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-02-02-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-01-01-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.70\n",
      "File: 03-01-03-01-02-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-02-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.70\n",
      "File: 03-01-03-02-01-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-04-01-01-01-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.65\n",
      "File: 03-01-04-01-01-02-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.35\n",
      "File: 03-01-04-01-02-01-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.90\n",
      "File: 03-01-04-01-02-02-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.75\n",
      "File: 03-01-04-02-01-01-15.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.68\n",
      "File: 03-01-04-02-01-02-15.wav\n",
      "True: sad, Predicted: calm, Conf: 0.60\n",
      "File: 03-01-04-02-02-01-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-04-02-02-02-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.49\n",
      "File: 03-01-05-01-01-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.86\n",
      "File: 03-01-05-01-02-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-02-02-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.48\n",
      "File: 03-01-06-01-01-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-01-06-01-02-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-01-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.74\n",
      "File: 03-01-06-02-02-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-07-01-01-01-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-01-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-02-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-15.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-15.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.54\n",
      "File: 03-01-08-01-01-02-15.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.52\n",
      "File: 03-01-08-01-02-01-15.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.59\n",
      "File: 03-01-08-01-02-02-15.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.58\n",
      "File: 03-01-08-02-01-01-15.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.63\n",
      "File: 03-01-08-02-01-02-15.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.61\n",
      "File: 03-01-08-02-02-01-15.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.49\n",
      "File: 03-01-08-02-02-02-15.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.57\n",
      "File: 03-01-01-01-01-01-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-01-02-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-01-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-02-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-01-02-01-01-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.90\n",
      "File: 03-01-02-01-01-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.74\n",
      "File: 03-01-02-01-02-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-01-02-02-16.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.54\n",
      "File: 03-01-02-02-01-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.85\n",
      "File: 03-01-02-02-01-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.69\n",
      "File: 03-01-02-02-02-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-03-01-01-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-01-01-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-02-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.84\n",
      "File: 03-01-03-01-02-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-01-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-03-02-02-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.82\n",
      "File: 03-01-04-01-01-01-16.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.75\n",
      "File: 03-01-04-01-01-02-16.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-04-01-02-01-16.wav\n",
      "True: sad, Predicted: happy, Conf: 0.41\n",
      "File: 03-01-04-01-02-02-16.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.80\n",
      "File: 03-01-04-02-01-01-16.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.88\n",
      "File: 03-01-04-02-01-02-16.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-04-02-02-01-16.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.95\n",
      "File: 03-01-04-02-02-02-16.wav\n",
      "True: sad, Predicted: happy, Conf: 0.33\n",
      "File: 03-01-05-01-01-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-01-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.90\n",
      "File: 03-01-06-01-01-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-01-02-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-01-02-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-01-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-01-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-16.wav\n",
      "True: disgust, Predicted: angry, Conf: 0.41\n",
      "File: 03-01-07-01-02-02-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-16.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-16.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.88\n",
      "File: 03-01-08-01-01-02-16.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.67\n",
      "File: 03-01-08-01-02-01-16.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.54\n",
      "File: 03-01-08-01-02-02-16.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.57\n",
      "File: 03-01-08-02-01-01-16.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-08-02-01-02-16.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-08-02-02-01-16.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.76\n",
      "File: 03-01-08-02-02-02-16.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.87\n",
      "File: 03-01-01-01-01-01-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.90\n",
      "File: 03-01-01-01-01-02-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-01-01-01-02-01-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-01-01-02-02-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-02-01-01-01-17.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.66\n",
      "File: 03-01-02-01-01-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-02-01-02-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-01-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-02-02-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-03-01-01-01-17.wav\n",
      "True: happy, Predicted: calm, Conf: 0.80\n",
      "File: 03-01-03-01-01-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-01-02-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.40\n",
      "File: 03-01-03-01-02-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.76\n",
      "File: 03-01-03-02-01-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-02-01-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-02-02-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-01-03-02-02-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-01-04-01-01-01-17.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-01-04-01-01-02-17.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.50\n",
      "File: 03-01-04-01-02-01-17.wav\n",
      "True: sad, Predicted: calm, Conf: 0.36\n",
      "File: 03-01-04-01-02-02-17.wav\n",
      "True: sad, Predicted: angry, Conf: 0.47\n",
      "File: 03-01-04-02-01-01-17.wav\n",
      "True: sad, Predicted: happy, Conf: 0.47\n",
      "File: 03-01-04-02-01-02-17.wav\n",
      "True: sad, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-04-02-02-01-17.wav\n",
      "True: sad, Predicted: happy, Conf: 0.45\n",
      "File: 03-01-04-02-02-02-17.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.55\n",
      "File: 03-01-05-01-01-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-01-01-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.94\n",
      "File: 03-01-05-01-02-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.89\n",
      "File: 03-01-05-02-01-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-01-01-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-01-02-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-02-01-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-01-06-02-02-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.70\n",
      "File: 03-01-06-02-02-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-07-01-01-01-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-17.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-08-01-01-01-17.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.74\n",
      "File: 03-01-08-01-01-02-17.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-08-01-02-01-17.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.62\n",
      "File: 03-01-08-01-02-02-17.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-08-02-01-01-17.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.52\n",
      "File: 03-01-08-02-01-02-17.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-08-02-02-01-17.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.69\n",
      "File: 03-01-08-02-02-02-17.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.71\n",
      "File: 03-01-01-01-01-01-18.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-01-01-01-02-18.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-01-18.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.86\n",
      "File: 03-01-01-01-02-02-18.wav\n",
      "True: neutral, Predicted: calm, Conf: 0.55\n",
      "File: 03-01-02-01-01-01-18.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-01-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-18.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-01-01-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-18.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-01-01-02-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-01-02-01-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-01-02-02-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-01-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-01-02-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-02-01-18.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-02-02-02-18.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-01-04-01-01-01-18.wav\n",
      "True: sad, Predicted: happy, Conf: 0.37\n",
      "File: 03-01-04-01-01-02-18.wav\n",
      "True: sad, Predicted: calm, Conf: 0.54\n",
      "File: 03-01-04-01-02-01-18.wav\n",
      "True: sad, Predicted: calm, Conf: 0.93\n",
      "File: 03-01-04-01-02-02-18.wav\n",
      "True: sad, Predicted: calm, Conf: 0.68\n",
      "File: 03-01-04-02-01-01-18.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-04-02-01-02-18.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-04-02-02-01-18.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-04-02-02-02-18.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-05-01-01-01-18.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-01-02-18.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-01-18.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-02-18.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-01-18.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-18.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-18.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-18.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-01-06-01-01-02-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-01-02-01-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.54\n",
      "File: 03-01-06-01-02-02-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-02-01-01-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-02-02-01-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-18.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-07-01-01-01-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-01-01-02-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.71\n",
      "File: 03-01-07-01-02-01-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.94\n",
      "File: 03-01-07-02-02-01-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-18.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.61\n",
      "File: 03-01-08-01-01-02-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.51\n",
      "File: 03-01-08-01-02-01-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.64\n",
      "File: 03-01-08-01-02-02-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.30\n",
      "File: 03-01-08-02-01-01-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.92\n",
      "File: 03-01-08-02-01-02-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.73\n",
      "File: 03-01-08-02-02-01-18.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.72\n",
      "File: 03-01-08-02-02-02-18.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.53\n",
      "File: 03-01-01-01-01-01-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.71\n",
      "File: 03-01-01-01-01-02-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-01-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-02-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-02-01-01-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-02-01-19.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.55\n",
      "File: 03-01-02-01-02-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-01-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-01-03-01-01-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.82\n",
      "File: 03-01-03-01-02-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.71\n",
      "File: 03-01-03-01-02-02-19.wav\n",
      "True: happy, Predicted: neutral, Conf: 0.47\n",
      "File: 03-01-03-02-01-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.75\n",
      "File: 03-01-03-02-01-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-02-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.68\n",
      "File: 03-01-03-02-02-02-19.wav\n",
      "True: happy, Predicted: disgust, Conf: 0.35\n",
      "File: 03-01-04-01-01-01-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.54\n",
      "File: 03-01-04-01-01-02-19.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.60\n",
      "File: 03-01-04-01-02-01-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.82\n",
      "File: 03-01-04-01-02-02-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-04-02-01-01-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.58\n",
      "File: 03-01-04-02-01-02-19.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.86\n",
      "File: 03-01-04-02-02-01-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.78\n",
      "File: 03-01-04-02-02-02-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.31\n",
      "File: 03-01-05-01-01-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-01-02-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.75\n",
      "File: 03-01-05-01-02-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.65\n",
      "File: 03-01-06-01-01-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-01-02-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-01-06-02-01-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-02-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-02-02-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-02-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-19.wav\n",
      "True: disgust, Predicted: angry, Conf: 0.69\n",
      "File: 03-01-07-02-02-02-19.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-19.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-08-01-01-02-19.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-08-01-02-01-19.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.68\n",
      "File: 03-01-08-01-02-02-19.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.74\n",
      "File: 03-01-08-02-01-01-19.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.83\n",
      "File: 03-01-08-02-01-02-19.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.79\n",
      "File: 03-01-08-02-02-01-19.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.40\n",
      "File: 03-01-08-02-02-02-19.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.58\n",
      "File: 03-01-01-01-01-01-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-01-02-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-01-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.82\n",
      "File: 03-01-01-01-02-02-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-02-01-01-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-01-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 0.87\n",
      "File: 03-01-02-01-02-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-02-01-02-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-01-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-01-03-01-01-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-01-03-01-02-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-03-01-02-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-02-01-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-03-02-01-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-01-04-01-01-01-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.85\n",
      "File: 03-01-04-01-01-02-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.63\n",
      "File: 03-01-04-01-02-01-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.41\n",
      "File: 03-01-04-01-02-02-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.90\n",
      "File: 03-01-04-02-01-01-20.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-04-02-01-02-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.55\n",
      "File: 03-01-04-02-02-01-20.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.48\n",
      "File: 03-01-04-02-02-02-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-05-01-01-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.84\n",
      "File: 03-01-05-01-01-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.56\n",
      "File: 03-01-05-01-02-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.60\n",
      "File: 03-01-05-01-02-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-02-01-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-01-01-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-06-01-02-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.79\n",
      "File: 03-01-06-01-02-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-02-01-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-01-06-02-01-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-02-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-07-01-01-01-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-02-01-01-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-20.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-20.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.68\n",
      "File: 03-01-08-01-01-02-20.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.35\n",
      "File: 03-01-08-01-02-01-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-08-01-02-02-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.62\n",
      "File: 03-01-08-02-01-01-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-08-02-01-02-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.51\n",
      "File: 03-01-08-02-02-01-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-08-02-02-02-20.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-01-01-01-01-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-01-01-01-01-02-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-01-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-02-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-02-01-01-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-01-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.94\n",
      "File: 03-01-02-01-02-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-01-02-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-01-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-02-02-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-02-02-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-03-01-01-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-01-01-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-01-02-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-01-02-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.62\n",
      "File: 03-01-03-02-01-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-01-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-01-03-02-02-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-02-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-01-04-01-01-01-21.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.54\n",
      "File: 03-01-04-01-01-02-21.wav\n",
      "True: sad, Predicted: calm, Conf: 0.83\n",
      "File: 03-01-04-01-02-01-21.wav\n",
      "True: sad, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-04-01-02-02-21.wav\n",
      "True: sad, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-04-02-01-01-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-04-02-01-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-01-04-02-02-01-21.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.64\n",
      "File: 03-01-04-02-02-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.90\n",
      "File: 03-01-05-01-01-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.60\n",
      "File: 03-01-05-01-01-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.91\n",
      "File: 03-01-05-01-02-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-02-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-02-02-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-21.wav\n",
      "True: fearful, Predicted: disgust, Conf: 0.94\n",
      "File: 03-01-06-01-01-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-01-02-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.85\n",
      "File: 03-01-06-01-02-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.56\n",
      "File: 03-01-06-02-01-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-06-02-01-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.80\n",
      "File: 03-01-06-02-02-02-21.wav\n",
      "True: fearful, Predicted: disgust, Conf: 0.85\n",
      "File: 03-01-07-01-01-01-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-01-02-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-01-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-02-01-01-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-21.wav\n",
      "True: disgust, Predicted: angry, Conf: 0.71\n",
      "File: 03-01-07-02-02-01-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-21.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.87\n",
      "File: 03-01-08-01-01-02-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.60\n",
      "File: 03-01-08-01-02-01-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.81\n",
      "File: 03-01-08-01-02-02-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.59\n",
      "File: 03-01-08-02-01-01-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.95\n",
      "File: 03-01-08-02-01-02-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.82\n",
      "File: 03-01-08-02-02-01-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.89\n",
      "File: 03-01-08-02-02-02-21.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.49\n",
      "File: 03-01-01-01-01-01-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-01-01-01-01-02-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-01-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-02-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-02-01-01-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-01-01-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.82\n",
      "File: 03-01-02-01-02-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-02-01-02-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-02-02-01-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-03-01-01-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-01-03-01-01-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-03-01-02-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.87\n",
      "File: 03-01-03-01-02-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.46\n",
      "File: 03-01-03-02-01-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-02-02-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-02-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-04-01-01-01-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.56\n",
      "File: 03-01-04-01-01-02-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.92\n",
      "File: 03-01-04-01-02-01-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-04-01-02-02-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-01-04-02-01-01-22.wav\n",
      "True: sad, Predicted: happy, Conf: 0.75\n",
      "File: 03-01-04-02-01-02-22.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.56\n",
      "File: 03-01-04-02-02-01-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.64\n",
      "File: 03-01-04-02-02-02-22.wav\n",
      "True: sad, Predicted: calm, Conf: 0.51\n",
      "File: 03-01-05-01-01-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 0.93\n",
      "File: 03-01-05-01-01-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 0.75\n",
      "File: 03-01-05-01-02-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 0.94\n",
      "File: 03-01-05-02-01-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-06-01-01-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.72\n",
      "File: 03-01-06-01-02-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-01-02-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-01-06-02-01-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-01-06-02-01-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-01-06-02-02-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.84\n",
      "File: 03-01-06-02-02-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-07-01-01-01-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.98\n",
      "File: 03-01-07-01-01-02-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.93\n",
      "File: 03-01-07-01-02-01-22.wav\n",
      "True: disgust, Predicted: angry, Conf: 0.46\n",
      "File: 03-01-07-01-02-02-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-22.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-08-01-01-01-22.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.72\n",
      "File: 03-01-08-01-01-02-22.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.72\n",
      "File: 03-01-08-01-02-01-22.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.66\n",
      "File: 03-01-08-01-02-02-22.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.89\n",
      "File: 03-01-08-02-01-01-22.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.41\n",
      "File: 03-01-08-02-01-02-22.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.77\n",
      "File: 03-01-08-02-02-01-22.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-08-02-02-02-22.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.81\n",
      "File: 03-01-01-01-01-01-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-01-02-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-01-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-01-01-02-02-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-02-01-01-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-01-01-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-02-01-02-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.89\n",
      "File: 03-01-02-01-02-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.84\n",
      "File: 03-01-02-02-01-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-01-02-02-01-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-02-02-02-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.78\n",
      "File: 03-01-02-02-02-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-03-01-01-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-01-03-01-01-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-01-03-01-02-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.83\n",
      "File: 03-01-03-01-02-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.80\n",
      "File: 03-01-03-02-01-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.66\n",
      "File: 03-01-03-02-01-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.65\n",
      "File: 03-01-03-02-02-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.87\n",
      "File: 03-01-03-02-02-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-04-01-01-01-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.84\n",
      "File: 03-01-04-01-01-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.96\n",
      "File: 03-01-04-01-02-01-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.70\n",
      "File: 03-01-04-01-02-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.67\n",
      "File: 03-01-04-02-01-01-23.wav\n",
      "True: sad, Predicted: happy, Conf: 0.32\n",
      "File: 03-01-04-02-01-02-23.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.64\n",
      "File: 03-01-04-02-02-01-23.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.46\n",
      "File: 03-01-04-02-02-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.54\n",
      "File: 03-01-05-01-01-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-01-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-01-05-01-02-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-01-05-02-01-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-02-01-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-02-02-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-06-01-01-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.48\n",
      "File: 03-01-06-01-01-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.58\n",
      "File: 03-01-06-01-02-01-23.wav\n",
      "True: fearful, Predicted: happy, Conf: 0.70\n",
      "File: 03-01-06-01-02-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.45\n",
      "File: 03-01-06-02-01-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-01-06-02-01-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-02-02-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-01-06-02-02-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-01-07-01-01-01-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-01-02-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-02-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-23.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-08-01-01-01-23.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.70\n",
      "File: 03-01-08-01-01-02-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.93\n",
      "File: 03-01-08-01-02-01-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.84\n",
      "File: 03-01-08-01-02-02-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.72\n",
      "File: 03-01-08-02-01-01-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.73\n",
      "File: 03-01-08-02-01-02-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.67\n",
      "File: 03-01-08-02-02-01-23.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.90\n",
      "File: 03-01-08-02-02-02-23.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.60\n",
      "File: 03-01-01-01-01-01-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.87\n",
      "File: 03-01-01-01-01-02-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-01-01-01-02-01-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-01-01-01-02-02-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-01-02-01-01-01-24.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.86\n",
      "File: 03-01-02-01-01-02-24.wav\n",
      "True: calm, Predicted: neutral, Conf: 0.73\n",
      "File: 03-01-02-01-02-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-01-02-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-01-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 0.60\n",
      "File: 03-01-02-02-01-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-02-02-02-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-01-02-02-02-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-01-03-01-01-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.79\n",
      "File: 03-01-03-01-01-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-01-02-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.77\n",
      "File: 03-01-03-01-02-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-01-03-02-01-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-01-03-02-02-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-01-03-02-02-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-01-04-01-01-01-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.95\n",
      "File: 03-01-04-01-01-02-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-04-01-02-01-24.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.63\n",
      "File: 03-01-04-01-02-02-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-01-04-02-01-01-24.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.60\n",
      "File: 03-01-04-02-01-02-24.wav\n",
      "True: sad, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-04-02-02-01-24.wav\n",
      "True: sad, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-04-02-02-02-24.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.93\n",
      "File: 03-01-05-01-01-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-01-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-01-05-01-02-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-01-02-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 0.82\n",
      "File: 03-01-05-02-01-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-01-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-01-05-02-02-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-01-05-02-02-02-24.wav\n",
      "True: angry, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-06-01-01-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-01-06-01-01-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-01-06-01-02-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-01-02-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-06-02-01-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-01-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-01-06-02-02-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-06-02-02-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-01-07-01-01-01-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-01-02-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-01-02-01-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-07-01-02-02-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-01-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-01-02-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-01-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 1.00\n",
      "File: 03-01-07-02-02-02-24.wav\n",
      "True: disgust, Predicted: disgust, Conf: 0.99\n",
      "File: 03-01-08-01-01-01-24.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.78\n",
      "File: 03-01-08-01-01-02-24.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.91\n",
      "File: 03-01-08-01-02-01-24.wav\n",
      "True: surprised, Predicted: happy, Conf: 0.72\n",
      "File: 03-01-08-01-02-02-24.wav\n",
      "True: surprised, Predicted: fearful, Conf: 0.83\n",
      "File: 03-01-08-02-01-01-24.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.71\n",
      "File: 03-01-08-02-01-02-24.wav\n",
      "True: surprised, Predicted: angry, Conf: 0.73\n",
      "File: 03-01-08-02-02-01-24.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.79\n",
      "File: 03-01-08-02-02-02-24.wav\n",
      "True: surprised, Predicted: disgust, Conf: 0.79\n",
      "File: 03-02-01-01-01-01-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-02-01.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-01.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-03-01-01-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-02-03-02-01-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-02-02-01-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-02-01.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-04-01-01-01-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.81\n",
      "File: 03-02-04-01-01-02-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-04-01-02-01-01.wav\n",
      "True: sad, Predicted: calm, Conf: 0.82\n",
      "File: 03-02-04-01-02-02-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.77\n",
      "File: 03-02-04-02-01-01-01.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.83\n",
      "File: 03-02-04-02-01-02-01.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.84\n",
      "File: 03-02-04-02-02-01-01.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-04-02-02-02-01.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.77\n",
      "File: 03-02-05-01-01-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-01-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-02-01-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-01-01.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-02-01.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-01-01-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-01.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-01-01-01-01-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-02.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-02.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-01-01-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-02-03-01-02-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-02-01-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-02-02.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-04-01-01-01-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.35\n",
      "File: 03-02-04-01-01-02-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.55\n",
      "File: 03-02-04-01-02-01-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.44\n",
      "File: 03-02-04-01-02-02-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.31\n",
      "File: 03-02-04-02-01-01-02.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.57\n",
      "File: 03-02-04-02-01-02-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.86\n",
      "File: 03-02-04-02-02-01-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.72\n",
      "File: 03-02-04-02-02-02-02.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-02-05-01-01-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-02.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.85\n",
      "File: 03-02-06-01-01-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-06-01-02-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.80\n",
      "File: 03-02-06-01-02-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-01-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-01-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-02.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-01-01-01-01-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-03.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-02-02-01-02-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-03.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-03.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-01-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-02-03-01-02-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-01-02-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-01-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-02-01-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-02-01-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.83\n",
      "File: 03-02-03-02-02-02-03.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-02-04-01-01-01-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.33\n",
      "File: 03-02-04-01-01-02-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.92\n",
      "File: 03-02-04-01-02-01-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.62\n",
      "File: 03-02-04-01-02-02-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-04-02-01-01-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.96\n",
      "File: 03-02-04-02-01-02-03.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.94\n",
      "File: 03-02-04-02-02-01-03.wav\n",
      "True: sad, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-04-02-02-02-03.wav\n",
      "True: sad, Predicted: happy, Conf: 0.37\n",
      "File: 03-02-05-01-01-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-01-02-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-03.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-01-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-02-01-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-01-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-03.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-04.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-04.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-01-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-02-02-04.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-04-01-01-01-04.wav\n",
      "True: sad, Predicted: calm, Conf: 0.59\n",
      "File: 03-02-04-01-01-02-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.51\n",
      "File: 03-02-04-01-02-01-04.wav\n",
      "True: sad, Predicted: calm, Conf: 0.44\n",
      "File: 03-02-04-01-02-02-04.wav\n",
      "True: sad, Predicted: calm, Conf: 0.86\n",
      "File: 03-02-04-02-01-01-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-04-02-01-02-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-04-02-02-01-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.93\n",
      "File: 03-02-04-02-02-02-04.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-05-01-01-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-04.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-02-04.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-06-01-01-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-01-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-01-02-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.80\n",
      "File: 03-02-06-02-01-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-06-02-02-01-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-04.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-01-01-01-01-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-01-02-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-05.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.90\n",
      "File: 03-02-02-02-01-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-05.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-02-03-01-01-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.67\n",
      "File: 03-02-03-01-02-01-05.wav\n",
      "True: happy, Predicted: calm, Conf: 0.84\n",
      "File: 03-02-03-01-02-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.72\n",
      "File: 03-02-03-02-01-01-05.wav\n",
      "True: happy, Predicted: fearful, Conf: 0.58\n",
      "File: 03-02-03-02-01-02-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-02-01-05.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-02-02-02-05.wav\n",
      "True: happy, Predicted: fearful, Conf: 0.90\n",
      "File: 03-02-04-01-01-01-05.wav\n",
      "True: sad, Predicted: disgust, Conf: 0.34\n",
      "File: 03-02-04-01-01-02-05.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.25\n",
      "File: 03-02-04-01-02-01-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.53\n",
      "File: 03-02-04-01-02-02-05.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.63\n",
      "File: 03-02-04-02-01-01-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.82\n",
      "File: 03-02-04-02-01-02-05.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.74\n",
      "File: 03-02-04-02-02-01-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-04-02-02-02-05.wav\n",
      "True: sad, Predicted: calm, Conf: 0.65\n",
      "File: 03-02-05-01-01-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-05-01-02-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 0.84\n",
      "File: 03-02-05-01-02-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-05.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-05.wav\n",
      "True: fearful, Predicted: calm, Conf: 0.79\n",
      "File: 03-02-06-01-01-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.70\n",
      "File: 03-02-06-01-02-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-01-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-05.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-06.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-06.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-01-01-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-02-06.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-04-01-01-01-06.wav\n",
      "True: sad, Predicted: calm, Conf: 0.82\n",
      "File: 03-02-04-01-01-02-06.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.78\n",
      "File: 03-02-04-01-02-01-06.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.93\n",
      "File: 03-02-04-01-02-02-06.wav\n",
      "True: sad, Predicted: calm, Conf: 0.84\n",
      "File: 03-02-04-02-01-01-06.wav\n",
      "True: sad, Predicted: angry, Conf: 0.56\n",
      "File: 03-02-04-02-01-02-06.wav\n",
      "True: sad, Predicted: angry, Conf: 0.66\n",
      "File: 03-02-04-02-02-01-06.wav\n",
      "True: sad, Predicted: angry, Conf: 0.94\n",
      "File: 03-02-04-02-02-02-06.wav\n",
      "True: sad, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-01-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-06.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-01-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-06-02-01-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.58\n",
      "File: 03-02-06-02-01-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-02-06.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-02-01-01-01-02-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.92\n",
      "File: 03-02-01-01-02-01-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-02-07.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.68\n",
      "File: 03-02-02-01-01-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-07.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-01-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-01-02-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-02-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-02-01-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-07.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-02-07.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-04-01-01-01-07.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-04-01-01-02-07.wav\n",
      "True: sad, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-04-01-02-01-07.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-04-01-02-02-07.wav\n",
      "True: sad, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-04-02-01-01-07.wav\n",
      "True: sad, Predicted: calm, Conf: 0.68\n",
      "File: 03-02-04-02-01-02-07.wav\n",
      "True: sad, Predicted: calm, Conf: 0.90\n",
      "File: 03-02-04-02-02-01-07.wav\n",
      "True: sad, Predicted: happy, Conf: 0.35\n",
      "File: 03-02-04-02-02-02-07.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.71\n",
      "File: 03-02-05-01-01-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-01-01-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-01-02-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-05-02-01-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-07.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-01-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-01-02-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-02-06-01-02-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.67\n",
      "File: 03-02-06-02-01-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-02-01-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.88\n",
      "File: 03-02-06-02-02-01-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-07.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-01-01-01-01-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-01-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-08.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-08.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-01-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-01-02-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-01-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-02-02-01-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-02-08.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-04-01-01-01-08.wav\n",
      "True: sad, Predicted: calm, Conf: 0.79\n",
      "File: 03-02-04-01-01-02-08.wav\n",
      "True: sad, Predicted: calm, Conf: 0.94\n",
      "File: 03-02-04-01-02-01-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-04-01-02-02-08.wav\n",
      "True: sad, Predicted: calm, Conf: 0.93\n",
      "File: 03-02-04-02-01-01-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.39\n",
      "File: 03-02-04-02-01-02-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.83\n",
      "File: 03-02-04-02-02-01-08.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.93\n",
      "File: 03-02-04-02-02-02-08.wav\n",
      "True: sad, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-05-01-01-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-08.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-01-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-06-02-02-01-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-08.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.73\n",
      "File: 03-02-01-01-02-02-09.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-02-01-01-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-01-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-09.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-09.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-01-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-02-02-09.wav\n",
      "True: happy, Predicted: happy, Conf: 0.51\n",
      "File: 03-02-04-01-01-01-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.67\n",
      "File: 03-02-04-01-01-02-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.58\n",
      "File: 03-02-04-01-02-01-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.63\n",
      "File: 03-02-04-01-02-02-09.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-02-04-02-01-01-09.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-04-02-01-02-09.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-04-02-02-01-09.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.90\n",
      "File: 03-02-04-02-02-02-09.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-05-01-01-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-01-01-02-09.wav\n",
      "True: angry, Predicted: happy, Conf: 0.78\n",
      "File: 03-02-05-01-02-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-01-02-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-02-05-02-01-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-09.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-01-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.78\n",
      "File: 03-02-06-01-02-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-09.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-10.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-01-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-10.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-03-01-01-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-02-03-01-02-01-10.wav\n",
      "True: happy, Predicted: calm, Conf: 0.66\n",
      "File: 03-02-03-01-02-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-01-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-02-10.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-04-01-01-01-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.51\n",
      "File: 03-02-04-01-01-02-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-02-04-01-02-01-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.71\n",
      "File: 03-02-04-01-02-02-10.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-04-02-01-01-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-04-02-01-02-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-04-02-02-01-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.51\n",
      "File: 03-02-04-02-02-02-10.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-05-01-01-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-10.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-10.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-06-01-01-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-01-01-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-02-01-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-02-02-02-10.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-01-01-01-01-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-01-02-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-02-01-01-02-01-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.98\n",
      "File: 03-02-01-01-02-02-11.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-01-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-11.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-01-01-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-02-03-01-02-01-11.wav\n",
      "True: happy, Predicted: calm, Conf: 0.67\n",
      "File: 03-02-03-01-02-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.85\n",
      "File: 03-02-03-02-01-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-01-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-02-02-11.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-04-01-01-01-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-04-01-01-02-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-04-01-02-01-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-02-04-01-02-02-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.96\n",
      "File: 03-02-04-02-01-01-11.wav\n",
      "True: sad, Predicted: angry, Conf: 0.28\n",
      "File: 03-02-04-02-01-02-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.78\n",
      "File: 03-02-04-02-02-01-11.wav\n",
      "True: sad, Predicted: happy, Conf: 0.39\n",
      "File: 03-02-04-02-02-02-11.wav\n",
      "True: sad, Predicted: calm, Conf: 0.91\n",
      "File: 03-02-05-01-01-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.93\n",
      "File: 03-02-05-01-01-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.45\n",
      "File: 03-02-05-01-02-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-05-01-02-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-02-01-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-01-11.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-11.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-06-01-01-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-06-01-01-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-06-02-01-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-02-02-01-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-11.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-12.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-02-02-02-01-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-12.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-12.wav\n",
      "True: calm, Predicted: calm, Conf: 0.89\n",
      "File: 03-02-03-01-01-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-01-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-02-03-01-02-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-12.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-02-12.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-04-01-01-01-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.56\n",
      "File: 03-02-04-01-01-02-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-04-01-02-01-12.wav\n",
      "True: sad, Predicted: calm, Conf: 0.49\n",
      "File: 03-02-04-01-02-02-12.wav\n",
      "True: sad, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-04-02-01-01-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.93\n",
      "File: 03-02-04-02-01-02-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-04-02-02-01-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-04-02-02-02-12.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-05-01-01-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-12.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-02-12.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-01-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-01-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-01-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-12.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-01-01-01-01-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.93\n",
      "File: 03-02-01-01-01-02-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-02-01-01-02-02-13.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-02-02-01-01-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.93\n",
      "File: 03-02-02-01-01-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-13.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-13.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.67\n",
      "File: 03-02-03-01-01-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-01-02-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.73\n",
      "File: 03-02-03-01-02-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-02-03-02-01-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.88\n",
      "File: 03-02-03-02-01-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-02-02-01-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.50\n",
      "File: 03-02-03-02-02-02-13.wav\n",
      "True: happy, Predicted: happy, Conf: 0.86\n",
      "File: 03-02-04-01-01-01-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.53\n",
      "File: 03-02-04-01-01-02-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.93\n",
      "File: 03-02-04-01-02-01-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.75\n",
      "File: 03-02-04-01-02-02-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.64\n",
      "File: 03-02-04-02-01-01-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.44\n",
      "File: 03-02-04-02-01-02-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.56\n",
      "File: 03-02-04-02-02-01-13.wav\n",
      "True: sad, Predicted: calm, Conf: 0.62\n",
      "File: 03-02-04-02-02-02-13.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.54\n",
      "File: 03-02-05-01-01-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-01-01-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.94\n",
      "File: 03-02-05-02-01-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-02-01-13.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-13.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-06-01-01-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.85\n",
      "File: 03-02-06-01-01-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-01-02-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-02-06-01-02-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-01-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-02-01-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-02-06-02-02-02-13.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-01-01-01-01-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-14.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-14.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.59\n",
      "File: 03-02-03-01-01-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-02-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-02-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-02-01-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-14.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-02-14.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-04-01-01-01-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.84\n",
      "File: 03-02-04-01-01-02-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-04-01-02-01-14.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.44\n",
      "File: 03-02-04-01-02-02-14.wav\n",
      "True: sad, Predicted: angry, Conf: 0.80\n",
      "File: 03-02-04-02-01-01-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-04-02-01-02-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.56\n",
      "File: 03-02-04-02-02-01-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-04-02-02-02-14.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.75\n",
      "File: 03-02-05-01-01-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-14.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-01-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-01-02-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-01-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-02-01-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-01-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-02-02-14.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-01-01-01-01-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-02-01-01-01-02-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.97\n",
      "File: 03-02-01-01-02-02-15.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-01-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-02-02-01-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-02-15.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-15.wav\n",
      "True: happy, Predicted: angry, Conf: 0.81\n",
      "File: 03-02-03-01-01-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-01-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.74\n",
      "File: 03-02-03-02-02-01-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.80\n",
      "File: 03-02-03-02-02-02-15.wav\n",
      "True: happy, Predicted: happy, Conf: 0.67\n",
      "File: 03-02-04-01-01-01-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.61\n",
      "File: 03-02-04-01-01-02-15.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.55\n",
      "File: 03-02-04-01-02-01-15.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.56\n",
      "File: 03-02-04-01-02-02-15.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.74\n",
      "File: 03-02-04-02-01-01-15.wav\n",
      "True: sad, Predicted: angry, Conf: 0.57\n",
      "File: 03-02-04-02-01-02-15.wav\n",
      "True: sad, Predicted: angry, Conf: 0.54\n",
      "File: 03-02-04-02-02-01-15.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.52\n",
      "File: 03-02-04-02-02-02-15.wav\n",
      "True: sad, Predicted: happy, Conf: 0.34\n",
      "File: 03-02-05-01-01-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.93\n",
      "File: 03-02-05-01-01-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.89\n",
      "File: 03-02-05-01-02-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-02-01-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-02-02-01-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-15.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.50\n",
      "File: 03-02-06-01-01-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.75\n",
      "File: 03-02-06-01-02-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-01-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-02-01-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-15.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-01-01-01-01-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-16.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.64\n",
      "File: 03-02-02-01-02-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-01-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-01-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.96\n",
      "File: 03-02-02-02-02-01-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-02-02-02-02-02-16.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-03-01-01-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-02-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-01-02-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-02-02-16.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-04-01-01-01-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.78\n",
      "File: 03-02-04-01-01-02-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.69\n",
      "File: 03-02-04-01-02-01-16.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.67\n",
      "File: 03-02-04-01-02-02-16.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.60\n",
      "File: 03-02-04-02-01-01-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-04-02-01-02-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-04-02-02-01-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-04-02-02-02-16.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-05-01-01-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 0.96\n",
      "File: 03-02-05-02-01-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-16.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-01-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.64\n",
      "File: 03-02-06-02-01-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-02-01-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-16.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-01-01-01-01-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.95\n",
      "File: 03-02-01-01-01-02-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-01-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-02-17.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-02-01-01-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.97\n",
      "File: 03-02-02-01-01-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-01-02-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-17.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-01-01-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-01-17.wav\n",
      "True: happy, Predicted: calm, Conf: 0.92\n",
      "File: 03-02-03-01-02-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-01-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-02-17.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-02-04-01-01-01-17.wav\n",
      "True: sad, Predicted: calm, Conf: 0.93\n",
      "File: 03-02-04-01-01-02-17.wav\n",
      "True: sad, Predicted: calm, Conf: 0.95\n",
      "File: 03-02-04-01-02-01-17.wav\n",
      "True: sad, Predicted: calm, Conf: 0.75\n",
      "File: 03-02-04-01-02-02-17.wav\n",
      "True: sad, Predicted: calm, Conf: 0.84\n",
      "File: 03-02-04-02-01-01-17.wav\n",
      "True: sad, Predicted: angry, Conf: 0.94\n",
      "File: 03-02-04-02-01-02-17.wav\n",
      "True: sad, Predicted: angry, Conf: 0.70\n",
      "File: 03-02-04-02-02-01-17.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-04-02-02-02-17.wav\n",
      "True: sad, Predicted: angry, Conf: 0.75\n",
      "File: 03-02-05-01-01-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-01-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-17.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-06-01-01-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-01-02-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.90\n",
      "File: 03-02-06-01-02-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-02-01-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-01-02-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-01-17.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-02-17.wav\n",
      "True: fearful, Predicted: angry, Conf: 0.74\n",
      "File: 03-02-01-01-01-01-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.88\n",
      "File: 03-02-01-01-01-02-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-01-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-02-19.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-19.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-19.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.91\n",
      "File: 03-02-03-01-01-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.84\n",
      "File: 03-02-03-01-02-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-02-03-02-01-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-02-02-02-19.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-04-01-01-01-19.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.71\n",
      "File: 03-02-04-01-01-02-19.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.66\n",
      "File: 03-02-04-01-02-01-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.63\n",
      "File: 03-02-04-01-02-02-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.68\n",
      "File: 03-02-04-02-01-01-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-02-04-02-01-02-19.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-04-02-02-01-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.97\n",
      "File: 03-02-04-02-02-02-19.wav\n",
      "True: sad, Predicted: calm, Conf: 0.72\n",
      "File: 03-02-05-01-01-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-02-05-01-01-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.86\n",
      "File: 03-02-05-01-02-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-19.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-19.wav\n",
      "True: fearful, Predicted: happy, Conf: 0.51\n",
      "File: 03-02-06-01-01-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-01-02-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-06-02-01-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.81\n",
      "File: 03-02-06-02-01-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.61\n",
      "File: 03-02-06-02-02-01-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-02-19.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.93\n",
      "File: 03-02-01-01-01-01-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-20.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-20.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-01-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-01-02-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-02-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.73\n",
      "File: 03-02-03-02-01-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-02-01-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.63\n",
      "File: 03-02-03-02-02-01-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-02-02-20.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-04-01-01-01-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.61\n",
      "File: 03-02-04-01-01-02-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.48\n",
      "File: 03-02-04-01-02-01-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.88\n",
      "File: 03-02-04-01-02-02-20.wav\n",
      "True: sad, Predicted: calm, Conf: 0.60\n",
      "File: 03-02-04-02-01-01-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-04-02-01-02-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-04-02-02-01-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-04-02-02-02-20.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-05-01-01-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-02-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-02-01-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-20.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-01-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-01-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-01-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-02-01-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-02-02-20.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-01-01-01-01-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.89\n",
      "File: 03-02-01-01-01-02-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.92\n",
      "File: 03-02-01-01-02-01-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-21.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-02-02-02-01-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-21.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-01-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-01-02-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.92\n",
      "File: 03-02-03-01-02-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.89\n",
      "File: 03-02-03-02-01-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-02-02-02-21.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-04-01-01-01-21.wav\n",
      "True: sad, Predicted: calm, Conf: 0.84\n",
      "File: 03-02-04-01-01-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.59\n",
      "File: 03-02-04-01-02-01-21.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-04-01-02-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-04-02-01-01-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.96\n",
      "File: 03-02-04-02-01-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-04-02-02-01-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-04-02-02-02-21.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.85\n",
      "File: 03-02-05-01-01-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.85\n",
      "File: 03-02-05-01-01-02-21.wav\n",
      "True: angry, Predicted: fearful, Conf: 0.53\n",
      "File: 03-02-05-01-02-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 0.94\n",
      "File: 03-02-05-02-01-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-21.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-01-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.94\n",
      "File: 03-02-06-01-02-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-01-02-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.92\n",
      "File: 03-02-06-02-01-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-02-01-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-01-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-02-21.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-01-01-01-01-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-01-02-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-22.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 0.51\n",
      "File: 03-02-02-01-02-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-22.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.84\n",
      "File: 03-02-03-01-01-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-02-03-01-02-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.93\n",
      "File: 03-02-03-01-02-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.94\n",
      "File: 03-02-03-02-01-01-22.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-01-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-02-01-22.wav\n",
      "True: happy, Predicted: fearful, Conf: 0.53\n",
      "File: 03-02-03-02-02-02-22.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-04-01-01-01-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.84\n",
      "File: 03-02-04-01-01-02-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.75\n",
      "File: 03-02-04-01-02-01-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.86\n",
      "File: 03-02-04-01-02-02-22.wav\n",
      "True: sad, Predicted: angry, Conf: 0.71\n",
      "File: 03-02-04-02-01-01-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-04-02-01-02-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-04-02-02-01-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.86\n",
      "File: 03-02-04-02-02-02-22.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.69\n",
      "File: 03-02-05-01-01-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-22.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-06-01-01-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-02-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.89\n",
      "File: 03-02-06-02-02-01-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-02-02-22.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.87\n",
      "File: 03-02-01-01-01-01-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-01-02-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.94\n",
      "File: 03-02-01-01-02-01-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-02-23.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-02-01-01-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.91\n",
      "File: 03-02-02-01-02-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-02-01-02-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-23.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-23.wav\n",
      "True: calm, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-03-01-01-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-01-01-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.97\n",
      "File: 03-02-03-01-02-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-02-03-01-02-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.99\n",
      "File: 03-02-03-02-01-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 1.00\n",
      "File: 03-02-03-02-02-01-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-03-02-02-02-23.wav\n",
      "True: happy, Predicted: happy, Conf: 0.98\n",
      "File: 03-02-04-01-01-01-23.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.50\n",
      "File: 03-02-04-01-01-02-23.wav\n",
      "True: sad, Predicted: neutral, Conf: 0.36\n",
      "File: 03-02-04-01-02-01-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.93\n",
      "File: 03-02-04-01-02-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-04-02-01-01-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.96\n",
      "File: 03-02-04-02-01-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.94\n",
      "File: 03-02-04-02-02-01-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.97\n",
      "File: 03-02-04-02-02-02-23.wav\n",
      "True: sad, Predicted: calm, Conf: 0.98\n",
      "File: 03-02-05-01-01-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-01-01-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-01-02-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.95\n",
      "File: 03-02-05-01-02-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 0.97\n",
      "File: 03-02-05-02-01-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-23.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-23.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.85\n",
      "File: 03-02-06-01-01-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.95\n",
      "File: 03-02-06-01-02-01-23.wav\n",
      "True: fearful, Predicted: disgust, Conf: 0.79\n",
      "File: 03-02-06-01-02-02-23.wav\n",
      "True: fearful, Predicted: disgust, Conf: 0.85\n",
      "File: 03-02-06-02-01-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-06-02-01-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.91\n",
      "File: 03-02-06-02-02-01-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.77\n",
      "File: 03-02-06-02-02-02-23.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "File: 03-02-01-01-01-01-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.96\n",
      "File: 03-02-01-01-01-02-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-01-01-02-01-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 0.99\n",
      "File: 03-02-01-01-02-02-24.wav\n",
      "True: neutral, Predicted: neutral, Conf: 1.00\n",
      "File: 03-02-02-01-01-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-01-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-01-02-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-01-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-01-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-02-02-02-02-24.wav\n",
      "True: calm, Predicted: calm, Conf: 1.00\n",
      "File: 03-02-03-01-01-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-01-01-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.95\n",
      "File: 03-02-03-01-02-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.90\n",
      "File: 03-02-03-01-02-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-02-01-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.84\n",
      "File: 03-02-03-02-01-02-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.81\n",
      "File: 03-02-03-02-02-01-24.wav\n",
      "True: happy, Predicted: happy, Conf: 0.96\n",
      "File: 03-02-03-02-02-02-24.wav\n",
      "True: happy, Predicted: calm, Conf: 0.52\n",
      "File: 03-02-04-01-01-01-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.96\n",
      "File: 03-02-04-01-01-02-24.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.58\n",
      "File: 03-02-04-01-02-01-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.74\n",
      "File: 03-02-04-01-02-02-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-04-02-01-01-24.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.65\n",
      "File: 03-02-04-02-01-02-24.wav\n",
      "True: sad, Predicted: fearful, Conf: 0.78\n",
      "File: 03-02-04-02-02-01-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.99\n",
      "File: 03-02-04-02-02-02-24.wav\n",
      "True: sad, Predicted: calm, Conf: 0.86\n",
      "File: 03-02-05-01-01-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-01-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 0.98\n",
      "File: 03-02-05-01-02-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-01-02-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 0.99\n",
      "File: 03-02-05-02-01-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-01-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-01-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-05-02-02-02-24.wav\n",
      "True: angry, Predicted: angry, Conf: 1.00\n",
      "File: 03-02-06-01-01-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 1.00\n",
      "File: 03-02-06-01-01-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-01-02-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-01-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.86\n",
      "File: 03-02-06-02-01-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.98\n",
      "File: 03-02-06-02-02-01-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.99\n",
      "File: 03-02-06-02-02-02-24.wav\n",
      "True: fearful, Predicted: fearful, Conf: 0.97\n",
      "\n",
      "=== Confidence Summary ===\n",
      "Included emotions (6 classes):\n",
      "  Count: 1884\n",
      "  Avg confidence: 0.95\n",
      "  Min confidence: 0.35\n",
      "\n",
      "Excluded emotions:\n",
      "  Count: 568\n",
      "  Max confidence: 1.00\n",
      "  95th percentile: 0.99\n",
      "\n",
      "Recommended threshold: 0.85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_training_files(speech_path, song_path):\n",
    "    \"\"\"Correctly handle RAVDESS's nested actor folders\"\"\"\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy',\n",
    "        '04': 'sad', '05': 'angry', '06': 'fearful',\n",
    "        '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    for root in [speech_path, song_path]:\n",
    "        # Scan through all Actor_* subdirectories\n",
    "        for actor_dir in Path(root).glob('Actor_*'):\n",
    "            if actor_dir.is_dir():\n",
    "                for wav_file in actor_dir.glob('*.wav'):\n",
    "                    try:\n",
    "                        parts = wav_file.stem.split('-')  # Split filename without extension\n",
    "                        if len(parts) >= 3:  # Ensure valid RAVDESS format\n",
    "                            emotion_code = parts[2]\n",
    "                            emotion = emotion_map.get(emotion_code, 'unknown')\n",
    "                            \n",
    "                            files.append({\n",
    "                                'path': str(wav_file),\n",
    "                                'true_emotion': emotion\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping {wav_file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nTotal files found: {len(files)}\")\n",
    "    if files:\n",
    "        print(\"Sample files:\")\n",
    "        for f in files[:3]:\n",
    "            print(f\"  {f['path']} -> {f['true_emotion']}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Usage\n",
    "speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "training_files = get_training_files(speech_path, song_path)\n",
    "\n",
    "# 2. Calculate Thresholds (using only training data)\n",
    "def calculate_training_thresholds(model, encoder, training_files):\n",
    "    confidences = {'included': [], 'excluded': []}\n",
    "    \n",
    "    for item in training_files:\n",
    "        try:\n",
    "            features = get_audio_features(item['path'])\n",
    "            preds = model.predict(np.expand_dims(features, axis=0), verbose=0)[0]\n",
    "            confidence = np.max(preds)\n",
    "            predicted_emotion = encoder.inverse_transform([np.argmax(preds)])[0]\n",
    "            \n",
    "            # Debug print for each file\n",
    "            print(f\"File: {os.path.basename(item['path'])}\")\n",
    "            print(f\"True: {item['true_emotion']}, Predicted: {predicted_emotion}, Conf: {confidence:.2f}\")\n",
    "            \n",
    "            if item['true_emotion'] in encoder.classes_:\n",
    "                confidences['included'].append(confidence)\n",
    "            else:\n",
    "                confidences['excluded'].append(confidence)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {item['path']}: {str(e)}\")\n",
    "    \n",
    "    # Add safety checks for empty arrays\n",
    "    if not confidences['included']:\n",
    "        raise ValueError(\"No included emotions found - check if encoder.classes_ matches your training files\")\n",
    "    \n",
    "    # Calculate threshold with fallbacks\n",
    "    threshold = 0.7  # Default fallback\n",
    "    \n",
    "    if confidences['excluded']:\n",
    "        threshold = np.percentile(confidences['excluded'], 95) + 0.05\n",
    "        threshold = min(0.85, max(0.6, threshold))\n",
    "    \n",
    "    print(\"\\n=== Confidence Summary ===\")\n",
    "    print(f\"Included emotions ({len(encoder.classes_)} classes):\")\n",
    "    print(f\"  Count: {len(confidences['included'])}\")\n",
    "    if confidences['included']:\n",
    "        print(f\"  Avg confidence: {np.mean(confidences['included']):.2f}\")\n",
    "        print(f\"  Min confidence: {np.min(confidences['included']):.2f}\")\n",
    "    \n",
    "    if confidences['excluded']:\n",
    "        print(f\"\\nExcluded emotions:\")\n",
    "        print(f\"  Count: {len(confidences['excluded'])}\")\n",
    "        print(f\"  Max confidence: {np.max(confidences['excluded']):.2f}\")\n",
    "        print(f\"  95th percentile: {np.percentile(confidences['excluded'], 95):.2f}\")\n",
    "    \n",
    "    print(f\"\\nRecommended threshold: {threshold:.2f}\")\n",
    "    return threshold\n",
    "# 3. Run Analysis\n",
    "training_files = get_training_files(speech_path, song_path)\n",
    "\n",
    "if len(training_files) > 0:\n",
    "    print(\"\\nFirst 3 files:\")\n",
    "    for file in training_files[:3]:\n",
    "        print(f\"Path: {file['path']} | Emotion: {file['true_emotion']}\")\n",
    "else:\n",
    "    print(\"\\nStill no files found. Check:\")\n",
    "    print(\"1. Folder permissions\")\n",
    "    print(\"2. File extensions (.wav vs .WAV)\")\n",
    "    print(\"3. Hidden files/folders\")\n",
    "\n",
    "optimal_threshold = calculate_training_thresholds(model, encoder, training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbecbb8-269c-4d8d-aaf2-ecbf5fac07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal threshold i am getting :-0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb874898-a6cf-4eab-83fe-1de524db2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder classes: ['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder classes:\", encoder.classes_)\n",
    "# Should show ONLY your 6 trained emotions (no sad/surprised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce40dace-73c5-4269-aaae-e48ae8bb94ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training files:\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training files:\")\n",
    "for file in training_files[:5]:  # First 5 files\n",
    "    print(f\"Path: {file['path']} | Emotion: {file['true_emotion']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e220642-eca2-4921-91ed-4d8b6113a86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total files found: 0\n",
      "Total files found: 0\n"
     ]
    }
   ],
   "source": [
    "training_files = get_training_files(\n",
    "    speech_path=r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\",\n",
    "    song_path=r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    ")\n",
    "print(f\"Total files found: {len(training_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0767bc34-e54b-4fd2-9024-1e7231f1a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech path exists: True\n",
      "Song path exists: True\n"
     ]
    }
   ],
   "source": [
    "speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "print(\"Speech path exists:\", os.path.exists(speech_path))\n",
    "print(\"Song path exists:\", os.path.exists(song_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ddc3204-2288-464c-8114-cb24db764c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: C:\\Users\\user\\Audio_Speech_Actors_01-24\n",
      "\n",
      "Attempt 1: os.listdir()\n",
      "Found 24 items (first 5): ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05']\n",
      "\n",
      "Attempt 2: Path.glob('*')\n",
      "Found 24 items (first 5): [WindowsPath('C:/Users/user/Audio_Speech_Actors_01-24/Actor_01'), WindowsPath('C:/Users/user/Audio_Speech_Actors_01-24/Actor_02'), WindowsPath('C:/Users/user/Audio_Speech_Actors_01-24/Actor_03'), WindowsPath('C:/Users/user/Audio_Speech_Actors_01-24/Actor_04'), WindowsPath('C:/Users/user/Audio_Speech_Actors_01-24/Actor_05')]\n",
      "\n",
      "Attempt 3: os.scandir()\n",
      "Found 24 items (first 5): ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05']\n",
      "\n",
      "Visibility Test:\n",
      "Can access 'C:\\Users\\user\\Audio_Speech_Actors_01-24\\Actor_01': False\n",
      "File size: 28672 bytes\n",
      "\n",
      "Scanning: C:\\Users\\user\\Audio_Song_Actors_01-24\n",
      "\n",
      "Attempt 1: os.listdir()\n",
      "Found 24 items (first 5): ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05']\n",
      "\n",
      "Attempt 2: Path.glob('*')\n",
      "Found 24 items (first 5): [WindowsPath('C:/Users/user/Audio_Song_Actors_01-24/Actor_01'), WindowsPath('C:/Users/user/Audio_Song_Actors_01-24/Actor_02'), WindowsPath('C:/Users/user/Audio_Song_Actors_01-24/Actor_03'), WindowsPath('C:/Users/user/Audio_Song_Actors_01-24/Actor_04'), WindowsPath('C:/Users/user/Audio_Song_Actors_01-24/Actor_05')]\n",
      "\n",
      "Attempt 3: os.scandir()\n",
      "Found 24 items (first 5): ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05']\n",
      "\n",
      "Visibility Test:\n",
      "Can access 'C:\\Users\\user\\Audio_Song_Actors_01-24\\Actor_01': False\n",
      "File size: 24576 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_folder(folder_path):\n",
    "    \"\"\"Deep scan a folder for audio files\"\"\"\n",
    "    print(f\"\\nScanning: {folder_path}\")\n",
    "    \n",
    "    # Check if truly empty\n",
    "    if not os.listdir(folder_path):\n",
    "        print(\">> Folder is EMPTY in Python (but exists)\")\n",
    "        return\n",
    "    \n",
    "    # Try different methods to find files\n",
    "    print(\"\\nAttempt 1: os.listdir()\")\n",
    "    files = os.listdir(folder_path)\n",
    "    print(f\"Found {len(files)} items (first 5): {files[:5]}\")\n",
    "    \n",
    "    print(\"\\nAttempt 2: Path.glob('*')\")\n",
    "    path_files = list(Path(folder_path).glob('*'))\n",
    "    print(f\"Found {len(path_files)} items (first 5): {path_files[:5]}\")\n",
    "    \n",
    "    print(\"\\nAttempt 3: os.scandir()\")\n",
    "    with os.scandir(folder_path) as entries:\n",
    "        scandir_files = [entry.name for entry in entries]\n",
    "    print(f\"Found {len(scandir_files)} items (first 5): {scandir_files[:5]}\")\n",
    "    \n",
    "    # Check file visibility\n",
    "    print(\"\\nVisibility Test:\")\n",
    "    test_file = os.path.join(folder_path, files[0]) if files else None\n",
    "    if test_file:\n",
    "        print(f\"Can access '{test_file}': {os.path.isfile(test_file)}\")\n",
    "        print(f\"File size: {os.path.getsize(test_file)} bytes\")\n",
    "\n",
    "# Run diagnostics\n",
    "speech_path = r\"C:\\Users\\user\\Audio_Speech_Actors_01-24\"\n",
    "song_path = r\"C:\\Users\\user\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "scan_folder(speech_path)\n",
    "scan_folder(song_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f226fd-e1cf-4858-aac8-05aa50a5169d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
